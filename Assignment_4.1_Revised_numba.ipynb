{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resampling 1min data to wanted timeframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import config as c\n",
    "\n",
    "def prepare_price_data(\n",
    "    csv_path: str,  # Ëº∏ÂÖ• CSV Ê™îÊ°àÂÆåÊï¥Ë∑ØÂæë\n",
    "    datasource: str = 'bybit_btcusdt',\n",
    "    factor: str = 'price',\n",
    "    timeframe: str = '1D', \n",
    "    delay_minutes: int = 0\n",
    "):\n",
    "    \"\"\"\n",
    "    ËÆÄÂèñ 1m Ë≥áÊñôÔºåËΩâÊàêÊåáÂÆöÁöÑÊôÇÈñìÈÄ±Êúü (timeframe)Ôºå\n",
    "    ÂèØÈÅ∏ÊìáÂª∂ÈÅ≤(Ê≠£ÂÄº)ÊàñÊèêÂâç(Ë≤†ÂÄº)ÊôÇÈñìÁ¥¢ÂºïÔºå‰∏¶Ëá™ÂãïÂ≠òÊ™îÂà∞Áï∂ÂâçÂ∑•‰ΩúÁõÆÈåÑ„ÄÇ\n",
    "\n",
    "    ÂèÉÊï∏Ôºö\n",
    "      csv_path      : „ÄêÂÆåÊï¥Ë∑ØÂæë„ÄëËº∏ÂÖ•ÁöÑ 1m Á¥öÂà• CSV Ê™îÊ°à\n",
    "      datasource    : Ë≥áÊñô‰æÜÊ∫êÂêçÁ®± (Â¶Ç bybit_btcusdt)\n",
    "      factor        : ÂΩ±ÈüøÂõ†Â≠êÂêçÁ®± (Â¶Ç price)\n",
    "      timeframe     : ËΩâÊèõÂæåÁöÑÊôÇÈñìÈÄ±ÊúüÔºåÂ¶Ç '1H'„ÄÅ'1D' Á≠â (È†êË®≠ '1D')\n",
    "      delay_minutes : ÊôÇÈñìÂπ≥ÁßªÁöÑÂàÜÈêòÊï∏ (Ê≠£ÂÄº = Âª∂ÂæåÔºõË≤†ÂÄº = ÊèêÂâç)\n",
    "\n",
    "    ÂõûÂÇ≥Ôºö\n",
    "      pandas DataFrame (resampled ÂæåÁöÑÁµêÊûú)Ôºå\n",
    "      ‰∏¶Â∞áÁµêÊûúËº∏Âá∫ÁÇ∫ CSVÔºåÂëΩÂêçÊ†ºÂºèÔºö\n",
    "      {datasource}_{factor}_{timeframe}_{start_time}_{end_time}.csv\n",
    "    \"\"\"\n",
    "    # 1. ËÆÄÂèñ CSVÔºåËß£ÊûêÊôÇÈñì\n",
    "    df = pd.read_csv(\n",
    "        csv_path, \n",
    "        parse_dates=['Time']  # pandas ÊúÉËá™ÂãïËß£ÊûêÊôÇÈñìÊ†ºÂºè\n",
    "    )\n",
    "\n",
    "    # 2. Â∞á 'Time' Ê¨ÑË®≠ÁÇ∫Á¥¢Âºï\n",
    "    df.set_index('Time', inplace=True)\n",
    "\n",
    "    # 3. ÊôÇÈñìÂπ≥Áßª (Âª∂ÈÅ≤ / ÊèêÂâç)\n",
    "    if delay_minutes != 0:\n",
    "        df.index = df.index + pd.Timedelta(minutes=delay_minutes)\n",
    "\n",
    "    # 4. ÂÆöÁæ© resample ËÅöÂêàÊñπÂºè\n",
    "    ohlc_dict = {\n",
    "        'Open': 'first',\n",
    "        'High': 'max',\n",
    "        'Low': 'min',\n",
    "        'Close': 'last',\n",
    "        'Volume': 'sum',\n",
    "        'Turnover': 'sum'\n",
    "    }\n",
    "    \n",
    "    # 5. ÈÄ≤Ë°å resample\n",
    "    df_resampled = df.resample(timeframe).agg(ohlc_dict).dropna(how='any')\n",
    "\n",
    "    # Use Time to create one more column named 'start_time' that is in unix timestamp\n",
    "    df_resampled['start_time'] = df_resampled.index.astype('int64') // 10**6\n",
    "    # df_resampled['start_time'] = df_resampled['start_time'].astype('float64')\n",
    "\n",
    "    # 6. Áç≤ÂèñÈñãÂßãËàáÁµêÊùüÊôÇÈñì (Ê†ºÂºè YYYY-MM-DD)\n",
    "    if not df_resampled.empty:\n",
    "        start_time = df_resampled.index[0].strftime('%Y-%m-%d')\n",
    "        end_time = df_resampled.index[-1].strftime('%Y-%m-%d')\n",
    "\n",
    "        # 7. ÊßãÂª∫Ëº∏Âá∫Ê™îÊ°àÂêçÁ®±\n",
    "        output_filename = f\"./data/resample_{datasource}_{timeframe}.csv\"\n",
    "        output_path = os.path.join(os.getcwd(), output_filename)  # Áï∂ÂâçÂ∑•‰ΩúÁõÆÈåÑ\n",
    "\n",
    "        # 8. Ëº∏Âá∫ CSV\n",
    "        df_resampled.to_csv(output_path)\n",
    "        print(f\"‚úÖ Ê™îÊ°àÂ∑≤ÂÑ≤Â≠òÔºö{output_path}\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Resampled DataFrame ÁÇ∫Á©∫ÔºåÊú™Áî¢ÁîüËº∏Âá∫Ê™îÊ°àÔºÅ\")\n",
    "\n",
    "    return df_resampled\n",
    "\n",
    "df_r = prepare_price_data(\n",
    "    csv_path=\"./data/bybit_btcusdt_price_1m_2020-01-01.csv\",\n",
    "    datasource='bybit_btc',\n",
    "    factor='price',\n",
    "    timeframe=c.candle_timeframe,\n",
    "    delay_minutes=c.candle_delay\n",
    ")\n",
    "\n",
    "print(df_r.head()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilsnumpy import load_data, data_processing\n",
    "import config as c\n",
    "\n",
    "unselected_df = load_data(c.candle_file, c.factor_file)\n",
    "df = unselected_df[[\"Time\",\"start_time\", \"Close\", c.factor]].copy()\n",
    "\n",
    "df[[\"Time\",\"start_time\", \"Close\", c.factor]].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Visualization of Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilsnumpy import load_data, data_processing\n",
    "import matplotlib.pyplot as plt\n",
    "import config as c\n",
    "\n",
    "unselected_df = load_data(c.candle_file, c.factor_file)\n",
    "df = unselected_df[[\"start_time\", \"Close\", c.factor]].copy()\n",
    "df.columns = [\"start_time\", \"close\", c.factor]\n",
    "df = data_processing(df, \"diff\", c.factor)\n",
    "# df = data_processing(df, \"cbrt\", factor)\n",
    "\n",
    "# Visualize the raw data of factor do not need close price\n",
    "fig, ax1 = plt.subplots(figsize=(15, 8))\n",
    "ax1.plot(df['start_time'], df[c.factor], label=c.factor, color='green', linewidth=2)\n",
    "ax1.set_xlabel(\"Date\", fontsize=12)\n",
    "ax1.set_ylabel(c.factor, fontsize=12, color='green')\n",
    "ax1.tick_params(axis='y', labelcolor='green')\n",
    "# Add title and grid\n",
    "plt.title(f\"Raw Data of {c.factor}\", fontsize=16)\n",
    "fig.tight_layout()  # Adjust layout to prevent overlap\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Train Backtest + show heatmap of Split backtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from itertools import product\n",
    "from utilsnumpy import backtest , load_data, load_single_data, combine_factors\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import config as c\n",
    "\n",
    "\n",
    "annualizer_dict = {\n",
    "    '1m': 525600,  # 1-minute intervals in a year\n",
    "    '5m': 105120,  # 5-minute intervals in a year\n",
    "    '15m': 35040,  # 15-minute intervals in a year\n",
    "    '30m': 17520,  # 30-minute intervals in a year\n",
    "    '1h': 8760,    # 1-hour intervals in a year\n",
    "    '4h': 2190,    # 4-hour intervals in a year\n",
    "    '1d': 365,     # 1-day intervals in a year\n",
    "    '1w': 52,      # 1-week intervals in a year\n",
    "    '1M': 12       # 1-month intervals in a year\n",
    "}\n",
    "\n",
    "all_backtest_results = []\n",
    "plot_data = []\n",
    "\n",
    "# ‚úÖ Á¨¨‰∏âÈöéÊÆµÔºàÁ¥∞Ë∑ëÔºâÔºöÂæûÊâãÂãïË®òÈåÑËÆÄÂèñÁâπÂÆö models & entrys\n",
    "# ‚úÖ Ëá™ÂãïÂàÜÈ°û models & entrys\n",
    "def parse_manual_selection(filepath, all_models):\n",
    "    \"\"\"\n",
    "    Ëß£ÊûêÊâãÂãïË®òÈåÑÁöÑÊñá‰ª∂ÔºåÊ†πÊìö ALL_MODELS Ëá™ÂãïÂàÜÈ°û model & entryÔºà‰∏¶Á∂≠ÊåÅÂ∞çÊáâÈóú‰øÇÔºâ„ÄÇ\n",
    "    :param filepath: ÊâãÂãïË®òÈåÑÊ™îÊ°àÁöÑË∑ØÂæë\n",
    "    :param all_models: ÊâÄÊúâÂèØÁî® modelsÔºàÁî®‰æÜËæ®Ë≠òÂì™‰∫õÊòØ modelÔºâ\n",
    "    :return: Â≠óÂÖ∏ { model: [entry1, entry2, ...] }\n",
    "    \"\"\"\n",
    "    with open(filepath, \"r\") as file:\n",
    "        lines = [line.strip() for line in file.readlines() if line.strip()]  # ÂéªÈô§Á©∫Ë°å\n",
    "\n",
    "    models_entrys = {}\n",
    "    current_model = None\n",
    "\n",
    "    for line in lines:\n",
    "        if line in all_models:  # Â¶ÇÊûúÊòØÂ∑≤Áü• modelÔºåÂâáÈñãÂïüÊñ∞ÁöÑ entry Ê∏ÖÂñÆ\n",
    "            current_model = line\n",
    "            models_entrys[current_model] = []\n",
    "        elif current_model:  # Â¶ÇÊûúÊòØ entryÔºåÂâáÂä†Âà∞Áï∂ÂâçÁöÑ model\n",
    "            models_entrys[current_model].append(line)\n",
    "\n",
    "    return models_entrys\n",
    "\n",
    "def main(data1, data2, data3, factor, factor2, interval, operation,preprocess, model, entry, window_end, window_step, threshold_end, threshold_step):\n",
    "    # Load data\n",
    "    unselected_df = load_data(data1, data2)\n",
    "    # Select wanted data column\n",
    "    df = unselected_df[[\"Time\", \"start_time\", \"Close\", factor]].copy()\n",
    "    # rename data column\n",
    "    df.columns = [\"Time\", \"start_time\", \"close\", factor]\n",
    "\n",
    "    # Load data3 if operation sign provided\n",
    "    if operation != 'none':\n",
    "        df1 = load_single_data(data3, factor2)\n",
    "        # Merge df and df1\n",
    "        df = pd.merge_asof(df, df1.sort_values('start_time'), on=\"start_time\", direction=\"nearest\")\n",
    "        df, new_column_name = combine_factors(df, factor, factor2, operation)\n",
    "        factor = new_column_name\n",
    "\n",
    "    # Check NaNs\n",
    "    num_nans = df[factor].isna().sum().sum()\n",
    "    total_rows = len(df)\n",
    "\n",
    "    if num_nans > total_rows * 0.03:\n",
    "        print(f\"Skipping factor {factor}: {num_nans} NaNs exceed 3% Threshold.\")\n",
    "        return\n",
    "    else:\n",
    "        df.dropna(inplace=True)\n",
    "\n",
    "    # metrics setting\n",
    "    window_start= 5\n",
    "    threshold_start = 0.0\n",
    "    annualizer = annualizer_dict.get(interval, None) # Day data, so 365\n",
    "    train_split = annualizer * 3\n",
    "    \n",
    "    backtest_report = []\n",
    "\n",
    "    # Split data into train and test sets (Train set: 3Year, Test set use remaining data)\n",
    "    df_train = df[:train_split].reset_index(drop=True).copy()\n",
    "    # df_test = df[train_split:].reset_index(drop=True).copy()\n",
    "\n",
    "    # backtest\n",
    "    for rolling_window in range(window_start, window_end, window_step):\n",
    "        for threshold in np.arange(threshold_start, threshold_end, threshold_step):\n",
    "            backtest_report.append(backtest(df_train, rolling_window, threshold, preprocess, entry, annualizer, model, factor, interval, \"sr\"))\n",
    "    all_backtest_results.append(backtest_report)\n",
    "    \n",
    "    # Extract pivot table for SR to plot heatmap\n",
    "    backtest_df = pd.DataFrame(backtest_report)\n",
    "    plot_data.append((model, entry, backtest_df))\n",
    "\n",
    "def plot_heatmaps(sr_threshold=1.5):\n",
    "    for model, entry, backtest_df in plot_data:\n",
    "        # ‚úÖ Dynamically set SR threshold based on entry name\n",
    "        if 'short' in entry:\n",
    "            srthreshold = 1.2\n",
    "        elif 'long' in entry:\n",
    "            srthreshold = sr_threshold\n",
    "        else: \n",
    "            srthreshold = sr_threshold\n",
    "\n",
    "        # ‚úÖ Optimized pivot using groupby instead of pivot\n",
    "        sr_pivot_data = backtest_df.groupby(['rolling_window', 'threshold'])['SR'].mean().unstack()\n",
    "\n",
    "        # ‚úÖ Check if the entire heatmap is NaN\n",
    "        if sr_pivot_data.isna().all().all():\n",
    "            print(f\"‚ö†Ô∏è Skipping {model}_{entry} heatmap: All SR values are NaN.\")\n",
    "            continue  # Skip plotting\n",
    "\n",
    "        # ‚úÖ Check if there is at least one SR > threshold\n",
    "        if not np.any(sr_pivot_data.to_numpy() > srthreshold):\n",
    "            print(f\"‚ö†Ô∏è Skipping {model}_{entry} heatmap: No SR value exceeds {srthreshold}.\")\n",
    "            continue  # Skip plotting\n",
    "\n",
    "        plt.figure(figsize=(18, 14))  # ‚úÖ Reduced figure size for faster rendering\n",
    "        sns.heatmap(sr_pivot_data, annot=True, fmt=\".2f\", cmap=\"RdYlGn\", linewidths=0.3, cbar_kws={'label': 'Sharpe Ratio'})\n",
    "        plt.xticks(ticks=range(len(sr_pivot_data.columns)), labels=[f\"{col:.2f}\" for col in sr_pivot_data.columns], rotation=45)\n",
    "        plt.yticks(ticks=range(len(sr_pivot_data.index)), labels=[f\"{row:.2f}\" for row in sr_pivot_data.index], rotation=0)\n",
    "        plt.title(f\"{model}_{c.preprocess}_{entry} Train Period BackTest SR Heatmap\", fontsize=14)\n",
    "        plt.show()  # ‚úÖ Display the plot\n",
    "        # ÂÑ≤Â≠ò SR Heatmap\n",
    "        if c.save_plot == True:\n",
    "            plt.savefig(f\"{model}_{entry}_heatmap\", dpi=300, bbox_inches='tight')\n",
    "            print(f\"Â∑≤ÂÑ≤Â≠ò {model}_{entry}_heatmap.png\")\n",
    "        plt.close()  # ‚úÖ Free memory after each plot\n",
    "\n",
    "# ‚úÖ ÈÅ∏ÊìáÊ®°ÂºèÔºöÁ¨¨‰∏ÄÊ≠• or Á¨¨‰∏âÊ≠•\n",
    "USE_ALL_MODELS = True  # ‚úÖ Ë®≠ÂÆöÁÇ∫ True ‚Üí Á¨¨‰∏ÄÊ≠•ÔºàÂÆåÊï¥Ë∑ëÔºâÔºåË®≠ÂÆöÁÇ∫ False ‚Üí Á¨¨‰∏âÊ≠•ÔºàÂÉÖË∑ëÈÅ∏ÊìáÁöÑÁµÑÂêàÔºâ\n",
    "\n",
    "if USE_ALL_MODELS:\n",
    "    models = c.ALL_MODELS\n",
    "    entry_map = {model: c.ALL_ENTRYS for model in c.ALL_MODELS}  # ÊØèÂÄã model Ë∑ëÊâÄÊúâ entries\n",
    "    window_step = 20  # Á¨¨‰∏ÄÈöéÊÆµÁî®ËºÉÂ§ßÊ≠•Èï∑\n",
    "    threshold_step = 0.2\n",
    "else:\n",
    "    entry_map = parse_manual_selection(\"manual_selected.txt\", c.ALL_MODELS)  # ËÆÄÂèñÊâãÂãïÁØ©ÈÅ∏\n",
    "    models = list(entry_map.keys())  # ÂèñÂæóÊâÄÊúâÊ®°Âûã\n",
    "    window_step = 10  # Á¨¨‰∏âÈöéÊÆµÁî®ËºÉÂ∞èÊ≠•Èï∑\n",
    "    threshold_step = 0.1\n",
    "\n",
    "total_combinations = sum(len(entry_map[model]) for model in models)\n",
    "\n",
    "# ‚úÖ Âä†ÂÖ• `tqdm` ÈÄ≤Â∫¶Ê¢ùÔºàÈ°ØÁ§∫ÊØèÂÄã `model` Âíå `entry`Ôºâ\n",
    "with tqdm(total=total_combinations, desc=\"üîç Backtesting Strategies\", unit=\"strategy\", leave=True) as pbar:\n",
    "    for model in models:\n",
    "        for entry in entry_map[model]:  # ‚úÖ Âè™ÈÅãË°åË©≤ model Áõ∏ÈóúÁöÑ entries\n",
    "            pbar.set_postfix({\"Model\": model, \"Entry\": entry})  # ‚úÖ ÂãïÊÖãÈ°ØÁ§∫Áï∂Ââç `model` & `entry`\n",
    "            main(\n",
    "                c.candle_file,\n",
    "                c.factor_file,\n",
    "                c.factor2_file,\n",
    "                c.factor,\n",
    "                c.factor2,\n",
    "                c.interval,\n",
    "                c.operation,\n",
    "                c.preprocess,\n",
    "                model,\n",
    "                entry,\n",
    "                window_end=301,\n",
    "                window_step=window_step,\n",
    "                threshold_end=3.51,\n",
    "                threshold_step=threshold_step\n",
    "            )\n",
    "            pbar.update(1)  # ‚úÖ ÊØèÂÆåÊàê‰∏ÄÂÄãÁµÑÂêàÔºåÊõ¥Êñ∞ÈÄ≤Â∫¶Ê¢ù\n",
    "\n",
    "# ‚úÖ Plot all heatmaps that SR > 1.5after backtesting\n",
    "plot_heatmaps(1.5)\n",
    "   \n",
    "# ÁîüÊàê‰∏ÄÂÄãjson file ÂÑ≤Â≠òÂÖ®ÈÉ® models Âíå entry ÁöÑ metrics\n",
    "# output_filename = f\"{factor}_{interval}_split_backtest.json\" \n",
    "# output_backtest_data = {\"backtests\": all_backtest_results}\n",
    "# with open(output_filename, \"w\") as json_file:\n",
    "#     json.dump(output_backtest_data, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Forward Testing Period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilsnumpy import backtest , load_data, load_single_data, combine_factors\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import talib\n",
    "import math\n",
    "import json\n",
    "import config as c\n",
    "\n",
    "annualizer_dict = {\n",
    "    '1m': 525600,  # 1-minute intervals in a year\n",
    "    '5m': 105120,  # 5-minute intervals in a year\n",
    "    '15m': 35040,  # 15-minute intervals in a year\n",
    "    '30m': 17520,  # 30-minute intervals in a year\n",
    "    '1h': 8760,    # 1-hour intervals in a year\n",
    "    '4h': 2190,    # 4-hour intervals in a year\n",
    "    '1d': 365,     # 1-day intervals in a year\n",
    "    '1w': 52,      # 1-week intervals in a year\n",
    "    '1M': 12       # 1-month intervals in a year\n",
    "}\n",
    "\n",
    "def main(data1, data2, data3, factor, factor2, interval, operation, preprocess, model, entry, window, threshold):\n",
    "    # Merge Data\n",
    "    unselected_df = load_data(data1, data2)\n",
    "    # Select wanted data column\n",
    "    df = unselected_df[[\"Time\", \"start_time\", \"Close\", factor]].copy()\n",
    "    # rename data column\n",
    "    df.columns = [\"Time\", \"start_time\", \"close\", factor]\n",
    "    \n",
    "    # Load data3 if operation sign provided\n",
    "    if operation != 'none':\n",
    "        df1 = load_single_data(data3, factor2)\n",
    "        # Merge df and df1\n",
    "        df = pd.merge_asof(df, df1.sort_values('start_time'), on=\"start_time\", direction=\"nearest\")\n",
    "        df, new_column_name = combine_factors(df, factor, factor2, operation)\n",
    "        factor = new_column_name\n",
    "    \n",
    "    # metrics setting\n",
    "    rolling_window = window\n",
    "    threshold = threshold\n",
    "    annualizer = annualizer_dict.get(interval, None) # Day data, so 365\n",
    "    train_split = annualizer * 3\n",
    "\n",
    "    # Split data into train and test sets (Train set: 3Year, Test set use remaining data)\n",
    "    # df_train = df[:train_split].reset_index(drop=True).copy()\n",
    "    df_test = df[train_split:].reset_index(drop=True).copy()\n",
    "\n",
    "    forwardtest_report = []\n",
    "    forwardtest_report.append(backtest(df_test, rolling_window, threshold, preprocess, entry, annualizer, model, factor, interval))\n",
    "\n",
    "    print(json.dumps(forwardtest_report, indent=4))\n",
    "\n",
    "    # ÂÑ≤Â≠ò forward test csv\n",
    "    if c.save_plot == True:\n",
    "        df.to_csv(f\"{c.alpha_id}_forward_test_df.csv\", index=False)\n",
    "        print(f\"Â∑≤ÂÑ≤Â≠ò {c.alpha_id}_forward_test_df.csv\")\n",
    "    \n",
    "    df_test['start_time'] = pd.to_datetime(df_test['start_time'], unit='ms')\n",
    "    start_date = df_test['start_time'].min().strftime('%Y-%m-%d')\n",
    "    end_date = df_test['start_time'].max().strftime('%Y-%m-%d')\n",
    "    # Plot close price on the left y-axis\n",
    "    fig, ax1 = plt.subplots(figsize=(15, 8))\n",
    "    ax1.plot(df_test['start_time'], df_test['close'], label='Close Price', color='green', linewidth=2)\n",
    "    ax1.set_xlabel(\"Date\", fontsize=12)\n",
    "    ax1.set_ylabel(\"Close Price\", fontsize=12, color='green')\n",
    "    ax1.tick_params(axis='y', labelcolor='green')\n",
    "    # Plot cumulative PnL on the right y-axis\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(df_test['start_time'], df_test['cumu_pnl'], label='Cumulative PnL', color='blue', linewidth=2)\n",
    "    ax2.set_ylabel(\"Cumulative PnL\", fontsize=12, color='blue')\n",
    "    ax2.tick_params(axis='y', labelcolor='blue')\n",
    "    # Add title and grid\n",
    "    plt.title(f\"Close Price and Cumulative PnL Plot (Forward Test Period)-({start_date} ~ {end_date})\", fontsize=16)\n",
    "    fig.tight_layout()  # Adjust layout to prevent overlap\n",
    "    plt.grid(True)\n",
    "\n",
    "    # ÂÑ≤Â≠ò Forwardtest CumuPnL Âíå json file\n",
    "    if c.save_plot == True:\n",
    "        plt.savefig(f\"Forwardtest_Equity_Curve_{start_date}_{end_date}\", dpi=300, bbox_inches='tight')\n",
    "        print(f\"Â∑≤ÂÑ≤Â≠ò Forwardtest_Equity_Curve_{start_date}_{end_date}.png\")\n",
    "        output_forwardtest_data = {\"forward_test\": forwardtest_report}\n",
    "        with open(f\"{c.alpha_id}_final_{factor}_{interval}_forward_test.json\", \"w\") as json_file:\n",
    "            json.dump(output_forwardtest_data, json_file, indent=4)\n",
    "    plt.show()\n",
    "\n",
    "main(\n",
    "    c.candle_file,\n",
    "    c.factor_file,\n",
    "    c.factor2_file,\n",
    "    c.factor,\n",
    "    c.factor2,\n",
    "    c.interval,\n",
    "    c.operation,\n",
    "    c.preprocess,\n",
    "    c.model,\n",
    "    c.entry,\n",
    "    c.window,\n",
    "    c.threshold\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Train backtest (For cumuPNL Graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilsnumpy import backtest , load_data, load_single_data, combine_factors\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import talib\n",
    "import math\n",
    "import json\n",
    "import config as c\n",
    "\n",
    "annualizer_dict = {\n",
    "    '1m': 525600,  # 1-minute intervals in a year\n",
    "    '5m': 105120,  # 5-minute intervals in a year\n",
    "    '15m': 35040,  # 15-minute intervals in a year\n",
    "    '30m': 17520,  # 30-minute intervals in a year\n",
    "    '1h': 8760,    # 1-hour intervals in a year\n",
    "    '4h': 2190,    # 4-hour intervals in a year\n",
    "    '1d': 365,     # 1-day intervals in a year\n",
    "    '1w': 52,      # 1-week intervals in a year\n",
    "    '1M': 12       # 1-month intervals in a year\n",
    "}\n",
    "\n",
    "def main(data1, data2, data3, factor, factor2, interval, operation, preprocess, model, entry, window, threshold):\n",
    "    # Merge Data\n",
    "    unselected_df = load_data(data1, data2)\n",
    "    # Select wanted data column\n",
    "    df = unselected_df[[\"Time\", \"start_time\", \"Close\", factor]].copy()\n",
    "    # rename data column\n",
    "    df.columns = [\"Time\", \"start_time\", \"close\", factor]\n",
    "    \n",
    "    # Load data3 if operation sign provided\n",
    "    if operation != 'none':\n",
    "        df1 = load_single_data(data3, factor2)\n",
    "        # Merge df and df1\n",
    "        df = pd.merge_asof(df, df1.sort_values('start_time'), on=\"start_time\", direction=\"nearest\")\n",
    "        df, new_column_name = combine_factors(df, factor, factor2, operation)\n",
    "        factor = new_column_name    \n",
    "    # metrics setting\n",
    "    rolling_window = window\n",
    "    threshold = threshold\n",
    "    annualizer = annualizer_dict.get(interval, None)\n",
    "    train_split = annualizer * 3\n",
    "\n",
    "    # Split data into train and test sets (Train set: 3Year, Test set use remaining data)\n",
    "    df_train = df[:train_split].reset_index(drop=True).copy()\n",
    "    # df_test = df[train_split:].reset_index(drop=True).copy()\n",
    "\n",
    "    backtest_report = []\n",
    "    backtest_report.append(backtest(df_train, rolling_window, threshold, preprocess, entry, annualizer, model, factor, interval))\n",
    "\n",
    "    print(json.dumps(backtest_report, indent=4))\n",
    "\n",
    "    # ÂÑ≤Â≠ò backtest csv\n",
    "    if c.save_plot == True:\n",
    "        df.to_csv(f\"{c.alpha_id}_backtest_df.csv\", index=False)\n",
    "        print(f\"Â∑≤ÂÑ≤Â≠ò {c.alpha_id}_backtest_df.csv\")\n",
    "    \n",
    "    df_train['start_time'] = pd.to_datetime(df_train['start_time'], unit='ms')\n",
    "    start_date = df_train['start_time'].min().strftime('%Y-%m-%d')\n",
    "    end_date = df_train['start_time'].max().strftime('%Y-%m-%d')\n",
    "    # Plot close price on the left y-axis\n",
    "    fig, ax1 = plt.subplots(figsize=(15, 8))\n",
    "    ax1.plot(df_train['start_time'], df_train['close'], label='Close Price', color='green', linewidth=2)\n",
    "    ax1.set_xlabel(\"Date\", fontsize=12)\n",
    "    ax1.set_ylabel(\"Close Price\", fontsize=12, color='green')\n",
    "    ax1.tick_params(axis='y', labelcolor='green')\n",
    "    # Plot cumulative PnL on the right y-axis\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(df_train['start_time'], df_train['cumu_pnl'], label='Cumulative PnL', color='blue', linewidth=2)\n",
    "    ax2.set_ylabel(\"Cumulative PnL\", fontsize=12, color='blue')\n",
    "    ax2.tick_params(axis='y', labelcolor='blue')\n",
    "    # Add title and grid\n",
    "    plt.title(f\"Close Price and Cumulative PnL Plot (Split Backtest Period)-({start_date} ~ {end_date})\", fontsize=16)\n",
    "    fig.tight_layout()  # Adjust layout to prevent overlap\n",
    "    plt.grid(True)\n",
    "    # ÂÑ≤Â≠ò Backtest CumuPnL\n",
    "    if c.save_plot == True:\n",
    "        plt.savefig(f\"Backtest_Equity_Curve_{start_date}_{end_date}\", dpi=300, bbox_inches='tight')\n",
    "        print(f\"Â∑≤ÂÑ≤Â≠ò Backtest_Equity_Curve_{start_date}_{end_date}.png\")\n",
    "        output_backtest_data = {\"back_test\": backtest_report}\n",
    "        with open(f\"{c.alpha_id}_final_{factor}_{interval}_back_test.json\", \"w\") as json_file:\n",
    "            json.dump(output_backtest_data, json_file, indent=4)\n",
    "    plt.show()\n",
    "\n",
    "main(\n",
    "    c.candle_file,\n",
    "    c.factor_file,\n",
    "    c.factor2_file,\n",
    "    c.factor,\n",
    "    c.factor2,\n",
    "    c.interval,\n",
    "    c.operation,\n",
    "    c.preprocess,\n",
    "    c.model,\n",
    "    c.entry,\n",
    "    c.window,\n",
    "    c.threshold\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backtest(No Permutation)(HandTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilsnumpy import backtest , load_data, load_single_data, combine_factors\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import talib\n",
    "import math\n",
    "import json\n",
    "import modin\n",
    "import config as c\n",
    "\n",
    "annualizer_dict = {\n",
    "    '1m': 525600,  # 1-minute intervals in a year\n",
    "    '5m': 105120,  # 5-minute intervals in a year\n",
    "    '15m': 35040,  # 15-minute intervals in a year\n",
    "    '30m': 17520,  # 30-minute intervals in a year\n",
    "    '1h': 8760,    # 1-hour intervals in a year\n",
    "    '4h': 2190,    # 4-hour intervals in a year\n",
    "    '1d': 365,     # 1-day intervals in a year\n",
    "    '1w': 52,      # 1-week intervals in a year\n",
    "    '1M': 12       # 1-month intervals in a year\n",
    "}\n",
    "\n",
    "def main(data1, data2, data3, factor, factor2, interval, operation, preprocess, model, entry, window, threshold):\n",
    "    # Merge Data\n",
    "    unselected_df = load_data(data1, data2)\n",
    "    # Select wanted data column\n",
    "    df = unselected_df[[\"Time\", \"start_time\", \"Close\", factor]].copy()\n",
    "    # rename data column\n",
    "    df.columns = [\"Time\", \"start_time\", \"close\", factor]\n",
    "    \n",
    "    # Load data3 if operation sign provided\n",
    "    if operation != 'none':\n",
    "        df1 = load_single_data(data3, factor2)\n",
    "        # Merge df and df1\n",
    "        df = pd.merge_asof(df, df1.sort_values('start_time'), on=\"start_time\", direction=\"nearest\")\n",
    "        df, new_column_name = combine_factors(df, factor, factor2, operation)\n",
    "        factor = new_column_name\n",
    "        \n",
    "    # metrics setting\n",
    "    rolling_window = window\n",
    "    threshold = threshold\n",
    "    annualizer = annualizer_dict.get(interval, None) # Day data, so 365\n",
    "\n",
    "    backtest_report = []\n",
    "    backtest_report.append(backtest(df, rolling_window, threshold, preprocess, entry, annualizer, model, factor, interval))\n",
    "\n",
    "    print(json.dumps(backtest_report, indent=4))\n",
    "\n",
    "\n",
    "    # ÂÑ≤Â≠ò full time test csv\n",
    "    if c.save_plot == True:\n",
    "        df.to_csv(f\"{c.alpha_id}_full_time_backtest_df.csv\", index=False)\n",
    "        print(f\"Â∑≤ÂÑ≤Â≠ò {c.alpha_id}_full_time_backtest_df.csv\")\n",
    "    # df.to_csv(f\"./liveRunning/excel_for_each_backtest/{factor}_{preprocess}_{interval}_{model}_{entry}_{window}_{threshold}.csv\", index=False)\n",
    "\n",
    "    df['start_time'] = pd.to_datetime(df['start_time'], unit='ms')\n",
    "    start_date = df['start_time'].min().strftime('%Y-%m-%d')\n",
    "    end_date = df['start_time'].max().strftime('%Y-%m-%d')\n",
    "    # Plot close price on the left y-axis\n",
    "    fig, ax1 = plt.subplots(figsize=(15, 8))\n",
    "    ax1.plot(df['start_time'], df['close'], label='Close Price', color='green', linewidth=2)\n",
    "    ax1.set_xlabel(\"Date\", fontsize=12)\n",
    "    ax1.set_ylabel(\"Close Price\", fontsize=12, color='green')\n",
    "    ax1.tick_params(axis='y', labelcolor='green')\n",
    "    # Plot cumulative PnL on the right y-axis\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(df['start_time'], df['cumu_pnl'], label='Cumulative PnL', color='blue', linewidth=2)\n",
    "    ax2.set_ylabel(\"Cumulative PnL\", fontsize=12, color='blue')\n",
    "    ax2.tick_params(axis='y', labelcolor='blue')\n",
    "    # Add title and grid\n",
    "    plt.title(f\"Close Price and Cumulative PnL Plot (Full Length)-({start_date} ~ {end_date})\", fontsize=16)\n",
    "    fig.tight_layout()  # Adjust layout to prevent overlap\n",
    "    plt.grid(True)                  \n",
    "    plt.show()\n",
    "\n",
    "main(\n",
    "    c.candle_file,\n",
    "    c.factor_file,\n",
    "    c.factor2_file,\n",
    "    c.factor,\n",
    "    c.factor2,\n",
    "    c.interval,\n",
    "    c.operation,\n",
    "    c.preprocess,\n",
    "    c.model,\n",
    "    c.entry,\n",
    "    c.window,\n",
    "    c.threshold\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cybotrade",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
