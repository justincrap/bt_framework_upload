{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot SR HeatMap by input model name(From backtest json file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load JSON file\n",
    "def load_json(file_path):\n",
    "    with open(file_path, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "    return data[\"backtests\"]\n",
    "\n",
    "# Extract SR data grouped by backtest_mode\n",
    "def extract_sr_data(backtests, model):\n",
    "    sr_data_dict = {}\n",
    "    \n",
    "    for backtest_list in backtests:\n",
    "        for bt in backtest_list:\n",
    "            if bt[\"model\"] == model:\n",
    "                mode = bt[\"backtest_mode\"]\n",
    "                if mode not in sr_data_dict:\n",
    "                    sr_data_dict[mode] = []\n",
    "                sr_data_dict[mode].append((bt[\"rolling_window\"], bt[\"threshold\"], bt[\"SR\"]))\n",
    "\n",
    "    return sr_data_dict\n",
    "\n",
    "# Create heatmaps for each backtest_mode\n",
    "def plot_sr_heatmaps(sr_data_dict, model):\n",
    "    if not sr_data_dict:\n",
    "        print(\"No data found for the given model.\")\n",
    "        return\n",
    "\n",
    "    for mode, sr_data in sr_data_dict.items():\n",
    "        df = pd.DataFrame(sr_data, columns=[\"rolling_window\", \"threshold\", \"SR\"])\n",
    "        pivot_table = df.pivot(index=\"rolling_window\", columns=\"threshold\", values=\"SR\")\n",
    "        \n",
    "        plt.figure(figsize=(20,16))\n",
    "        sns.heatmap(pivot_table, annot=True, fmt=\".2f\", cmap=\"RdYlGn\", linewidths=0.5, cbar_kws={'label': 'Sharpe Ratio'})\n",
    "        plt.title(f\"Train Period BackTest Sharpe Ratio Heatmap - {mode} ({model})\")\n",
    "        plt.xlabel(\"Rolling Window\")\n",
    "        plt.ylabel(\"Threshold\")\n",
    "        plt.show()\n",
    "\n",
    "# Main function\n",
    "def main():\n",
    "    file_path = \"mvrv_1d_split_backtest.json\"  # Update with actual file path\n",
    "    model = \"zscore\"\n",
    "    \n",
    "    backtests = load_json(file_path)\n",
    "    sr_data_dict = extract_sr_data(backtests, model)\n",
    "    plot_sr_heatmaps(sr_data_dict, model)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resampling 1min data to wanted timeframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def prepare_price_data(\n",
    "    csv_path: str,  # è¼¸å…¥ CSV æª”æ¡ˆå®Œæ•´è·¯å¾‘\n",
    "    datasource: str = 'bybit_btcusdt',\n",
    "    factor: str = 'price',\n",
    "    timeframe: str = '1D', \n",
    "    delay_minutes: int = 0\n",
    "):\n",
    "    \"\"\"\n",
    "    è®€å– 1m è³‡æ–™ï¼Œè½‰æˆæŒ‡å®šçš„æ™‚é–“é€±æœŸ (timeframe)ï¼Œ\n",
    "    å¯é¸æ“‡å»¶é²(æ­£å€¼)æˆ–æå‰(è² å€¼)æ™‚é–“ç´¢å¼•ï¼Œä¸¦è‡ªå‹•å­˜æª”åˆ°ç•¶å‰å·¥ä½œç›®éŒ„ã€‚\n",
    "\n",
    "    åƒæ•¸ï¼š\n",
    "      csv_path      : ã€å®Œæ•´è·¯å¾‘ã€‘è¼¸å…¥çš„ 1m ç´šåˆ¥ CSV æª”æ¡ˆ\n",
    "      datasource    : è³‡æ–™ä¾†æºåç¨± (å¦‚ bybit_btcusdt)\n",
    "      factor        : å½±éŸ¿å› å­åç¨± (å¦‚ price)\n",
    "      timeframe     : è½‰æ›å¾Œçš„æ™‚é–“é€±æœŸï¼Œå¦‚ '1H'ã€'1D' ç­‰ (é è¨­ '1D')\n",
    "      delay_minutes : æ™‚é–“å¹³ç§»çš„åˆ†é˜æ•¸ (æ­£å€¼ = å»¶å¾Œï¼›è² å€¼ = æå‰)\n",
    "\n",
    "    å›žå‚³ï¼š\n",
    "      pandas DataFrame (resampled å¾Œçš„çµæžœ)ï¼Œ\n",
    "      ä¸¦å°‡çµæžœè¼¸å‡ºç‚º CSVï¼Œå‘½åæ ¼å¼ï¼š\n",
    "      {datasource}_{factor}_{timeframe}_{start_time}_{end_time}.csv\n",
    "    \"\"\"\n",
    "    # 1. è®€å– CSVï¼Œè§£æžæ™‚é–“\n",
    "    df = pd.read_csv(\n",
    "        csv_path, \n",
    "        parse_dates=['Time']  # pandas æœƒè‡ªå‹•è§£æžæ™‚é–“æ ¼å¼\n",
    "    )\n",
    "\n",
    "    # 2. å°‡ 'Time' æ¬„è¨­ç‚ºç´¢å¼•\n",
    "    df.set_index('Time', inplace=True)\n",
    "\n",
    "    # 3. æ™‚é–“å¹³ç§» (å»¶é² / æå‰)\n",
    "    if delay_minutes != 0:\n",
    "        df.index = df.index + pd.Timedelta(minutes=delay_minutes)\n",
    "\n",
    "    # 4. å®šç¾© resample èšåˆæ–¹å¼\n",
    "    ohlc_dict = {\n",
    "        'Open': 'first',\n",
    "        'High': 'max',\n",
    "        'Low': 'min',\n",
    "        'Close': 'last',\n",
    "        'Volume': 'sum',\n",
    "        'Turnover': 'sum'\n",
    "    }\n",
    "    \n",
    "    # 5. é€²è¡Œ resample\n",
    "    df_resampled = df.resample(timeframe).agg(ohlc_dict).dropna(how='any')\n",
    "\n",
    "    # Use Time to create one more column named 'start_time' that is in unix timestamp\n",
    "    df_resampled['start_time'] = df_resampled.index.astype('int64') // 10**6\n",
    "    df_resampled['start_time'] = df_resampled['start_time'].astype('float64')\n",
    "\n",
    "    # 6. ç²å–é–‹å§‹èˆ‡çµæŸæ™‚é–“ (æ ¼å¼ YYYY-MM-DD)\n",
    "    if not df_resampled.empty:\n",
    "        start_time = df_resampled.index[0].strftime('%Y-%m-%d')\n",
    "        end_time = df_resampled.index[-1].strftime('%Y-%m-%d')\n",
    "\n",
    "        # 7. æ§‹å»ºè¼¸å‡ºæª”æ¡ˆåç¨±\n",
    "        output_filename = f\"./data/resample_{datasource}_{timeframe}.csv\"\n",
    "        output_path = os.path.join(os.getcwd(), output_filename)  # ç•¶å‰å·¥ä½œç›®éŒ„\n",
    "\n",
    "        # 8. è¼¸å‡º CSV\n",
    "        df_resampled.to_csv(output_path)\n",
    "        print(f\"âœ… æª”æ¡ˆå·²å„²å­˜ï¼š{output_path}\")\n",
    "    else:\n",
    "        print(\"âš ï¸ Resampled DataFrame ç‚ºç©ºï¼Œæœªç”¢ç”Ÿè¼¸å‡ºæª”æ¡ˆï¼\")\n",
    "\n",
    "    return df_resampled\n",
    "\n",
    "df_r = prepare_price_data(\n",
    "    csv_path=\"./data/bybit_btcusdt_price_1m_2020-01-01.csv\",\n",
    "    datasource='bybit_btc',\n",
    "    factor='price',\n",
    "    timeframe='1d',\n",
    "    delay_minutes=-25 \n",
    ")\n",
    "\n",
    "print(df_r.head()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For checking data\n",
    "from utilsnumpy import load_data, data_processing\n",
    "data1 = \"./data/resample_bybit_btc_1D.csv\"\n",
    "data2 = \"./data/cryptoquant_btc_coinbase-premium-index_1d.csv\"\n",
    "factor = 'coinbase_premium_gap'\n",
    "\n",
    "unselected_df = load_data(data1, data2)\n",
    "df = unselected_df[[\"Time\",\"start_time\", \"Close\", factor]].copy()\n",
    "\n",
    "df[[\"Time\",\"start_time\", \"Close\", factor]].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Visualization of Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilsnumpy import load_data, data_processing\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data1 = \"./data/bybit_candle_btc_1h.csv\"\n",
    "data2 = \"./data/cryptoquant_btc_coinbase-premium-index_1h.csv\"\n",
    "factor = 'coinbase_premium_gap'\n",
    "\n",
    "unselected_df = load_data(data1, data2)\n",
    "df = unselected_df[[\"start_time\", \"close\", factor]].copy()\n",
    "df.columns = [\"start_time\", \"close\", factor]\n",
    "df = data_processing(df, \"sqrt\", factor)\n",
    "# df = data_processing(df, \"cbrt\", factor)\n",
    "\n",
    "# Visualize the raw data of factor do not need close price\n",
    "fig, ax1 = plt.subplots(figsize=(15, 8))\n",
    "ax1.plot(df['start_time'], df[factor], label=factor, color='green', linewidth=2)\n",
    "ax1.set_xlabel(\"Date\", fontsize=12)\n",
    "ax1.set_ylabel(factor, fontsize=12, color='green')\n",
    "ax1.tick_params(axis='y', labelcolor='green')\n",
    "# Add title and grid\n",
    "plt.title(f\"Raw Data of {factor}\", fontsize=16)\n",
    "fig.tight_layout()  # Adjust layout to prevent overlap\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Train Backtest + show heatmap of Split backtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from itertools import product\n",
    "from utilsnumpy import backtest , load_data, load_single_data, combine_factors\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "\n",
    "annualizer_dict = {\n",
    "    '1m': 525600,  # 1-minute intervals in a year\n",
    "    '5m': 105120,  # 5-minute intervals in a year\n",
    "    '15m': 35040,  # 15-minute intervals in a year\n",
    "    '30m': 17520,  # 30-minute intervals in a year\n",
    "    '1h': 8760,    # 1-hour intervals in a year\n",
    "    '4h': 2190,    # 4-hour intervals in a year\n",
    "    '1d': 365,     # 1-day intervals in a year\n",
    "    '1w': 52,      # 1-week intervals in a year\n",
    "    '1M': 12       # 1-month intervals in a year\n",
    "}\n",
    "\n",
    "all_backtest_results = []\n",
    "plot_data = []\n",
    "\n",
    "def main(data1, data2, data3, factor, factor2, interval, operation,preprocess, model, entry, window_end, window_step, threshold_end, threshold_step):\n",
    "    # Load data\n",
    "    unselected_df = load_data(data1, data2)\n",
    "    # Select wanted data column\n",
    "    df = unselected_df[[\"Time\", \"start_time\", \"Close\", factor]].copy()\n",
    "    # rename data column\n",
    "    df.columns = [\"Time\", \"start_time\", \"close\", factor]\n",
    "\n",
    "    # Load data3 if operation sign provided\n",
    "    if operation != 'none':\n",
    "        df1 = load_single_data(data3, factor2)\n",
    "        # Merge df and df1\n",
    "        df = pd.merge_asof(df, df1.sort_values('start_time'), on=\"start_time\", direction=\"nearest\")\n",
    "        df, new_column_name = combine_factors(df, factor, factor2, operation)\n",
    "        factor = new_column_name\n",
    "    # print(df.columns)\n",
    "    # print(factor)\n",
    "\n",
    "    # metrics setting\n",
    "    window_start= 5\n",
    "    threshold_start = 0.0\n",
    "    annualizer = annualizer_dict.get(interval, None) # Day data, so 365\n",
    "    train_split = annualizer * 3\n",
    "    \n",
    "    backtest_report = []\n",
    "\n",
    "    # Split data into train and test sets (Train set: 3Year, Test set use remaining data)\n",
    "    df_train = df[:train_split].reset_index(drop=True).copy()\n",
    "    # df_test = df[train_split:].reset_index(drop=True).copy()\n",
    "\n",
    "    # backtest\n",
    "    for rolling_window in range(window_start, window_end, window_step):\n",
    "        for threshold in np.arange(threshold_start, threshold_end, threshold_step):\n",
    "            backtest_report.append(backtest(df_train, rolling_window, threshold, preprocess, entry, annualizer, model, factor, interval, \"sr\"))\n",
    "    all_backtest_results.append(backtest_report)\n",
    "    \n",
    "\n",
    "    # Extract pivot table for SR to plot heatmap\n",
    "    backtest_df = pd.DataFrame(backtest_report)\n",
    "    plot_data.append((model, entry, backtest_df))\n",
    "\n",
    "def plot_heatmaps(sr_threshold=1.5):\n",
    "    for model, entry, backtest_df in plot_data:\n",
    "        # âœ… Dynamically set SR threshold based on entry name\n",
    "        sr_threshold = 1.2 if entry.startswith('S') else sr_threshold  # Use 1.2 for entries starting with 'S', otherwise 1.8\n",
    "        sr_threshold = sr_threshold if entry.startswith('L') else sr_threshold # Changing threshold backto 1.5 if startwith 'L'\n",
    "        # âœ… Optimized pivot using groupby instead of pivot\n",
    "        sr_pivot_data = backtest_df.groupby(['rolling_window', 'threshold'])['SR'].mean().unstack()\n",
    "\n",
    "        # âœ… Check if the entire heatmap is NaN\n",
    "        if sr_pivot_data.isna().all().all():\n",
    "            print(f\"âš ï¸ Skipping {model}_{entry} heatmap: All SR values are NaN.\")\n",
    "            continue  # Skip plotting\n",
    "\n",
    "        # âœ… Check if there is at least one SR > threshold\n",
    "        if not np.any(sr_pivot_data.to_numpy() > sr_threshold):\n",
    "            print(f\"âš ï¸ Skipping {model}_{entry} heatmap: No SR value exceeds {sr_threshold}.\")\n",
    "            continue  # Skip plotting\n",
    "\n",
    "        plt.figure(figsize=(18, 14))  # âœ… Reduced figure size for faster rendering\n",
    "        sns.heatmap(sr_pivot_data, annot=True, fmt=\".2f\", cmap=\"RdYlGn\", linewidths=0.3, cbar_kws={'label': 'Sharpe Ratio'})\n",
    "        plt.title(f\"{model}_{preprocess}_{entry} Train Period BackTest SR Heatmap\", fontsize=14)\n",
    "        plt.show()  # âœ… Display the plot\n",
    "        plt.close()  # âœ… Free memory after each plot\n",
    "\n",
    "# models = ['zscore', 'momentum', 'volatility', 'robust', 'sma_diff', 'ewm', 'minmax', 'percentile', 'maxabs', 'mean_norm', 'roc', 'rsi', 'psy', 'rvi', 'mad', 'ma_ratio']\n",
    "# entrys = ['Trend', 'Trend_Reverse', 'MR', 'MR_Reverse', 'Trend_NoHold',  'Trend_emaFilter', 'Trend_NoHold_emaFilter', \n",
    "#            'L_Trend', 'L_Trend_Reverse', 'L_MR', 'L_MR_Reverse', 'L_Trend_NoHold', 'L_Trend_emaFilter', 'L_Trend_NoHold_emaFilter', \n",
    "#            'S_Trend', 'S_Trend_Reverse', 'S_MR', 'S_MR_Reverse' ,'S_Trend_NoHold', 'S_Trend_emaFilter', 'S_Trend_NoHold_emaFilter']\n",
    "\n",
    "models = ['zscore']\n",
    "entrys = ['L_Trend']\n",
    "           \n",
    "factor = 'coinbase_premium_gap'\n",
    "factor2 = 'netflow_total'\n",
    "interval = '1h'\n",
    "operation = 'none'\n",
    "preprocess = 'none'\n",
    "total_combinations = len(models) * len(entrys)\n",
    "for model, entry in tqdm(product(models, entrys), \n",
    "                        total=total_combinations,\n",
    "                        desc=\"ðŸ” Backtesting Strategies\",\n",
    "                        unit=\"strategy\", \n",
    "                        leave = True):\n",
    "    main(\n",
    "        f\"./data/bybit_candle_btc_{interval}.csv\",\n",
    "        f\"./data/cryptoquant_btc_coinbase-premium-index_{interval}.csv\",\n",
    "        f\"./data/cryptoquant_btc_netflow_{interval}.csv\",\n",
    "        factor,\n",
    "        factor2,\n",
    "        interval,\n",
    "        operation,\n",
    "        preprocess,\n",
    "        model,\n",
    "        entry,\n",
    "        window_end=301,\n",
    "        window_step=10,\n",
    "        threshold_end=3.01,\n",
    "        threshold_step=0.2\n",
    ")\n",
    "\n",
    "# âœ… Plot all heatmaps that SR > 1.5after backtesting\n",
    "plot_heatmaps(1.5)\n",
    "   \n",
    "# Output backtest json file with all model and entry\n",
    "# output_filename = f\"{factor}_{interval}_split_backtest.json\" \n",
    "# output_backtest_data = {\"backtests\": all_backtest_results}\n",
    "# with open(output_filename, \"w\") as json_file:\n",
    "#     json.dump(output_backtest_data, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Forward Testing Period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilsnumpy import backtest , load_data, load_single_data, combine_factors\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import talib\n",
    "import math\n",
    "import json\n",
    "\n",
    "annualizer_dict = {\n",
    "    '1m': 525600,  # 1-minute intervals in a year\n",
    "    '5m': 105120,  # 5-minute intervals in a year\n",
    "    '15m': 35040,  # 15-minute intervals in a year\n",
    "    '30m': 17520,  # 30-minute intervals in a year\n",
    "    '1h': 8760,    # 1-hour intervals in a year\n",
    "    '4h': 2190,    # 4-hour intervals in a year\n",
    "    '1d': 365,     # 1-day intervals in a year\n",
    "    '1w': 52,      # 1-week intervals in a year\n",
    "    '1M': 12       # 1-month intervals in a year\n",
    "}\n",
    "\n",
    "def main(data1, data2, data3, factor, factor2, interval, operation, preprocess, model, entry, window, threshold):\n",
    "    # Merge Data\n",
    "    unselected_df = load_data(data1, data2)\n",
    "    # Select wanted data column\n",
    "    df = unselected_df[[\"Time\", \"start_time\", \"Close\", factor]].copy()\n",
    "    # rename data column\n",
    "    df.columns = [\"Time\", \"start_time\", \"close\", factor]\n",
    "    \n",
    "    # Load data3 if operation sign provided\n",
    "    if operation != 'none':\n",
    "        df1 = load_single_data(data3, factor2)\n",
    "        # Merge df and df1\n",
    "        df = pd.merge_asof(df, df1.sort_values('start_time'), on=\"start_time\", direction=\"nearest\")\n",
    "        df, new_column_name = combine_factors(df, factor, factor2, operation)\n",
    "        factor = new_column_name\n",
    "    \n",
    "    \n",
    "\n",
    "    # metrics setting\n",
    "    rolling_window = window\n",
    "    threshold = threshold\n",
    "    annualizer = annualizer_dict.get(interval, None) # Day data, so 365\n",
    "    train_split = annualizer * 3\n",
    "\n",
    "    # Split data into train and test sets (Train set: 3Year, Test set use remaining data)\n",
    "    # df_train = df[:train_split].reset_index(drop=True).copy()\n",
    "    df_test = df[train_split:].reset_index(drop=True).copy()\n",
    "\n",
    "    forwardtest_report = []\n",
    "    forwardtest_report.append(backtest(df_test, rolling_window, threshold, preprocess, entry, annualizer, model, factor, interval))\n",
    "\n",
    "    print(json.dumps(forwardtest_report, indent=4))\n",
    "    \n",
    "\n",
    "    # Plot close price on the left y-axis\n",
    "    fig, ax1 = plt.subplots(figsize=(15, 8))\n",
    "    ax1.plot(df_test['start_time'], df_test['close'], label='Close Price', color='green', linewidth=2)\n",
    "    ax1.set_xlabel(\"Date\", fontsize=12)\n",
    "    ax1.set_ylabel(\"Close Price\", fontsize=12, color='green')\n",
    "    ax1.tick_params(axis='y', labelcolor='green')\n",
    "    # Plot cumulative PnL on the right y-axis\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(df_test['start_time'], df_test['cumu_pnl'], label='Cumulative PnL', color='blue', linewidth=2)\n",
    "    ax2.set_ylabel(\"Cumulative PnL\", fontsize=12, color='blue')\n",
    "    ax2.tick_params(axis='y', labelcolor='blue')\n",
    "    # Add title and grid\n",
    "    plt.title(\"Close Price and Cumulative PnL Plot (Forward Test Period)\", fontsize=16)\n",
    "    fig.tight_layout()  # Adjust layout to prevent overlap\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    # output_forwardtest_data = {\"forward_test\": forwardtest_report}\n",
    "    # with open(f\"final_{factor}_{interval}_forward_test.json\", \"w\") as json_file:\n",
    "    #     json.dump(output_forwardtest_data, json_file, indent=4)\n",
    "    \n",
    "    # Export df to csv\n",
    "    # df.to_csv(\"backtest_df.csv\", index=False)\n",
    "\n",
    "factor = 'coinbase_premium_index'\n",
    "factor2 = 'netflow_total'\n",
    "interval = '1d'\n",
    "operation = 'none'\n",
    "preprocess = 'none'\n",
    "model = 'mad'\n",
    "entry = 'S_MR'\n",
    "window=70\n",
    "threshold=2.4\n",
    "\n",
    "main(\n",
    "    f\"./data/resample_bybit_btc_{interval}.csv\",\n",
    "    f\"./data/cryptoquant_btc_coinbase-premium-index_{interval}.csv\",\n",
    "    f\"./data/cryptoquant_btc_netflow_{interval}.csv\",\n",
    "    factor,\n",
    "    factor2,\n",
    "    interval,\n",
    "    operation,\n",
    "    preprocess,\n",
    "    model,\n",
    "    entry,\n",
    "    window,\n",
    "    threshold\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Train backtest (For cumuPNL Graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilsnumpy import backtest , load_data, load_single_data, combine_factors\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import talib\n",
    "import math\n",
    "import json\n",
    "\n",
    "annualizer_dict = {\n",
    "    '1m': 525600,  # 1-minute intervals in a year\n",
    "    '5m': 105120,  # 5-minute intervals in a year\n",
    "    '15m': 35040,  # 15-minute intervals in a year\n",
    "    '30m': 17520,  # 30-minute intervals in a year\n",
    "    '1h': 8760,    # 1-hour intervals in a year\n",
    "    '4h': 2190,    # 4-hour intervals in a year\n",
    "    '1d': 365,     # 1-day intervals in a year\n",
    "    '1w': 52,      # 1-week intervals in a year\n",
    "    '1M': 12       # 1-month intervals in a year\n",
    "}\n",
    "\n",
    "def main(data1, data2, data3, factor, factor2, interval, operation, preprocess, model, entry, window, threshold):\n",
    "    # Merge Data\n",
    "    unselected_df = load_data(data1, data2)\n",
    "    # Select wanted data column\n",
    "    df = unselected_df[[\"Time\", \"start_time\", \"Close\", factor]].copy()\n",
    "    # rename data column\n",
    "    df.columns = [\"Time\", \"start_time\", \"close\", factor]\n",
    "    \n",
    "    # Load data3 if operation sign provided\n",
    "    if operation != 'none':\n",
    "        df1 = load_single_data(data3, factor2)\n",
    "        # Merge df and df1\n",
    "        df = pd.merge_asof(df, df1.sort_values('start_time'), on=\"start_time\", direction=\"nearest\")\n",
    "        df, new_column_name = combine_factors(df, factor, factor2, operation)\n",
    "        factor = new_column_name    \n",
    "    # metrics setting\n",
    "    rolling_window = window\n",
    "    threshold = threshold\n",
    "    annualizer = annualizer_dict.get(interval, None)\n",
    "    train_split = annualizer * 3\n",
    "\n",
    "    # Split data into train and test sets (Train set: 3Year, Test set use remaining data)\n",
    "    df_train = df[:train_split].reset_index(drop=True).copy()\n",
    "    # df_test = df[train_split:].reset_index(drop=True).copy()\n",
    "\n",
    "    forwardtest_report = []\n",
    "    forwardtest_report.append(backtest(df_train, rolling_window, threshold, preprocess, entry, annualizer, model, factor, interval))\n",
    "\n",
    "    print(json.dumps(forwardtest_report, indent=4))\n",
    "    \n",
    "\n",
    "    # Plot close price on the left y-axis\n",
    "    fig, ax1 = plt.subplots(figsize=(15, 8))\n",
    "    ax1.plot(df_train['start_time'], df_train['close'], label='Close Price', color='green', linewidth=2)\n",
    "    ax1.set_xlabel(\"Date\", fontsize=12)\n",
    "    ax1.set_ylabel(\"Close Price\", fontsize=12, color='green')\n",
    "    ax1.tick_params(axis='y', labelcolor='green')\n",
    "    # Plot cumulative PnL on the right y-axis\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(df_train['start_time'], df_train['cumu_pnl'], label='Cumulative PnL', color='blue', linewidth=2)\n",
    "    ax2.set_ylabel(\"Cumulative PnL\", fontsize=12, color='blue')\n",
    "    ax2.tick_params(axis='y', labelcolor='blue')\n",
    "    # Add title and grid\n",
    "    plt.title(\"Close Price and Cumulative PnL Plot (Split Train Period)\", fontsize=16)\n",
    "    fig.tight_layout()  # Adjust layout to prevent overlap\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    # Export df to csv\n",
    "    # df.to_csv(\"backtest_df.csv\", index=False)\n",
    "\n",
    "factor = 'coinbase_premium_index'\n",
    "factor2 = 'netflow_total'\n",
    "interval = '1h'\n",
    "operation = 'none'\n",
    "preprocess = 'none'\n",
    "model = 'mad'\n",
    "entry = 'L_Trend'\n",
    "window=80\n",
    "threshold=1.9\n",
    "\n",
    "main(\n",
    "    f\"./data/resample_bybit_btc_{interval}.csv\",\n",
    "    f\"./data/cryptoquant_btc_coinbase-premium-index_{interval}.csv\",\n",
    "    f\"./data/cryptoquant_btc_netflow_{interval}.csv\",\n",
    "    factor,\n",
    "    factor2,\n",
    "    interval,\n",
    "    operation,\n",
    "    preprocess,\n",
    "    model,\n",
    "    entry,\n",
    "    window,\n",
    "    threshold\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backtest(No Permutation)(HandTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilsnumpy import backtest , load_data, load_single_data, combine_factors\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import talib\n",
    "import math\n",
    "import json\n",
    "import modin\n",
    "\n",
    "annualizer_dict = {\n",
    "    '1m': 525600,  # 1-minute intervals in a year\n",
    "    '5m': 105120,  # 5-minute intervals in a year\n",
    "    '15m': 35040,  # 15-minute intervals in a year\n",
    "    '30m': 17520,  # 30-minute intervals in a year\n",
    "    '1h': 8760,    # 1-hour intervals in a year\n",
    "    '4h': 2190,    # 4-hour intervals in a year\n",
    "    '1d': 365,     # 1-day intervals in a year\n",
    "    '1w': 52,      # 1-week intervals in a year\n",
    "    '1M': 12       # 1-month intervals in a year\n",
    "}\n",
    "\n",
    "def main(data1, data2, data3, factor, factor2, interval, operation, preprocess, model, entry, window, threshold):\n",
    "    # Merge Data\n",
    "    unselected_df = load_data(data1, data2)\n",
    "    # Select wanted data column\n",
    "    df = unselected_df[[\"Time\", \"start_time\", \"Close\", factor]].copy()\n",
    "    # rename data column\n",
    "    df.columns = [\"Time\", \"start_time\", \"close\", factor]\n",
    "\n",
    "    print(df.tail(5))\n",
    "    \n",
    "    # Load data3 if operation sign provided\n",
    "    if operation != 'none':\n",
    "        df1 = load_single_data(data3, factor2)\n",
    "        # Merge df and df1\n",
    "        df = pd.merge_asof(df, df1.sort_values('start_time'), on=\"start_time\", direction=\"nearest\")\n",
    "        df, new_column_name = combine_factors(df, factor, factor2, operation)\n",
    "        factor = new_column_name\n",
    "        \n",
    "    # metrics setting\n",
    "    rolling_window = window\n",
    "    threshold = threshold\n",
    "    annualizer = annualizer_dict.get(interval, None) # Day data, so 365\n",
    "\n",
    "    backtest_report = []\n",
    "    backtest_report.append(backtest(df, rolling_window, threshold, preprocess, entry, annualizer, model, factor, interval))\n",
    "\n",
    "    print(json.dumps(backtest_report, indent=4))\n",
    "\n",
    "    # Plot close price on the left y-axis\n",
    "    fig, ax1 = plt.subplots(figsize=(15, 8))\n",
    "    ax1.plot(df['start_time'], df['close'], label='Close Price', color='green', linewidth=2)\n",
    "    ax1.set_xlabel(\"Date\", fontsize=12)\n",
    "    ax1.set_ylabel(\"Close Price\", fontsize=12, color='green')\n",
    "    ax1.tick_params(axis='y', labelcolor='green')\n",
    "    # Plot cumulative PnL on the right y-axis\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(df['start_time'], df['cumu_pnl'], label='Cumulative PnL', color='blue', linewidth=2)\n",
    "    ax2.set_ylabel(\"Cumulative PnL\", fontsize=12, color='blue')\n",
    "    ax2.tick_params(axis='y', labelcolor='blue')\n",
    "    # Add title and grid\n",
    "    plt.title(\"Close Price and Cumulative PnL Plot (Full Length)\", fontsize=16)\n",
    "    fig.tight_layout()  # Adjust layout to prevent overlap\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    # Export df to csv\n",
    "    # df.to_csv(\"backtest_df.csv\", index=False)\n",
    "    # df.to_csv(f\"./liveRunning/excel_for_each_backtest/{factor}_{preprocess}_{interval}_{model}_{entry}_{window}_{threshold}.csv\", index=False)\n",
    "\n",
    "factor = 'coinbase_premium_index'\n",
    "factor2 = 'netflow_total'\n",
    "interval = '1d'\n",
    "operation = 'none'\n",
    "preprocess = 'none'\n",
    "model = 'mad'\n",
    "entry = 'L_Trend_emaFilter'\n",
    "window=55\n",
    "threshold=1.3\n",
    "\n",
    "main(\n",
    "    f\"./data/resample_bybit_btc_{interval}.csv\",\n",
    "    f\"./data/cryptoquant_btc_coinbase-premium-index_{interval}.csv\",\n",
    "    f\"./data/cryptoquant_btc_netflow_{interval}.csv\",\n",
    "    factor,\n",
    "    factor2,\n",
    "    interval,\n",
    "    operation,\n",
    "    preprocess,\n",
    "    model,\n",
    "    entry,\n",
    "    window,\n",
    "    threshold\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cybotrade",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
