{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resampling 1min data to wanted timeframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import config as c\n",
    "\n",
    "def prepare_price_data(\n",
    "    csv_path: str,  # 輸入 CSV 檔案完整路徑\n",
    "    datasource: str = 'bybit_btcusdt',\n",
    "    factor: str = 'price',\n",
    "    timeframe: str = '1D', \n",
    "    delay_minutes: int = 0\n",
    "):\n",
    "    \"\"\"\n",
    "    讀取 1m 資料，轉成指定的時間週期 (timeframe)，\n",
    "    可選擇延遲(正值)或提前(負值)時間索引，並自動存檔到當前工作目錄。\n",
    "\n",
    "    參數：\n",
    "      csv_path      : 【完整路徑】輸入的 1m 級別 CSV 檔案\n",
    "      datasource    : 資料來源名稱 (如 bybit_btcusdt)\n",
    "      factor        : 影響因子名稱 (如 price)\n",
    "      timeframe     : 轉換後的時間週期，如 '1H'、'1D' 等 (預設 '1D')\n",
    "      delay_minutes : 時間平移的分鐘數 (正值 = 延後；負值 = 提前)\n",
    "\n",
    "    回傳：\n",
    "      pandas DataFrame (resampled 後的結果)，\n",
    "      並將結果輸出為 CSV，命名格式：\n",
    "      {datasource}_{factor}_{timeframe}_{start_time}_{end_time}.csv\n",
    "    \"\"\"\n",
    "    # 1. 讀取 CSV，解析時間\n",
    "    df = pd.read_csv(\n",
    "        csv_path, \n",
    "        parse_dates=['Time']  # pandas 會自動解析時間格式\n",
    "    )\n",
    "\n",
    "    # 2. 將 'Time' 欄設為索引\n",
    "    df.set_index('Time', inplace=True)\n",
    "\n",
    "    # 3. 時間平移 (延遲 / 提前)\n",
    "    if delay_minutes != 0:\n",
    "        df.index = df.index + pd.Timedelta(minutes=delay_minutes)\n",
    "\n",
    "    # 4. 定義 resample 聚合方式\n",
    "    ohlc_dict = {\n",
    "        'Open': 'first',\n",
    "        'High': 'max',\n",
    "        'Low': 'min',\n",
    "        'Close': 'last',\n",
    "        'Volume': 'sum',\n",
    "        'Turnover': 'sum'\n",
    "    }\n",
    "    \n",
    "    # 5. 進行 resample\n",
    "    df_resampled = df.resample(timeframe).agg(ohlc_dict).dropna(how='any')\n",
    "\n",
    "    # Use Time to create one more column named 'start_time' that is in unix timestamp\n",
    "    df_resampled['start_time'] = df_resampled.index.astype('int64') // 10**6\n",
    "    # df_resampled['start_time'] = df_resampled['start_time'].astype('float64')\n",
    "\n",
    "    # 6. 獲取開始與結束時間 (格式 YYYY-MM-DD)\n",
    "    if not df_resampled.empty:\n",
    "        start_time = df_resampled.index[0].strftime('%Y-%m-%d')\n",
    "        end_time = df_resampled.index[-1].strftime('%Y-%m-%d')\n",
    "\n",
    "        # 7. 構建輸出檔案名稱\n",
    "        output_filename = f\"./data/resample_{datasource}_{timeframe}.csv\"\n",
    "        output_path = os.path.join(os.getcwd(), output_filename)  # 當前工作目錄\n",
    "\n",
    "        # 8. 輸出 CSV\n",
    "        df_resampled.to_csv(output_path)\n",
    "        print(f\"✅ 檔案已儲存：{output_path}\")\n",
    "    else:\n",
    "        print(\"⚠️ Resampled DataFrame 為空，未產生輸出檔案！\")\n",
    "\n",
    "    return df_resampled\n",
    "\n",
    "df_r = prepare_price_data(\n",
    "    csv_path=\"./data/bybit_btcusdt_price_1m_2020-01-01.csv\",\n",
    "    datasource='bybit_btc',\n",
    "    factor='price',\n",
    "    timeframe=c.candle_timeframe,\n",
    "    delay_minutes=c.candle_delay\n",
    ")\n",
    "\n",
    "print(df_r.head()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilsnumpy import load_data, data_processing\n",
    "import config as c\n",
    "\n",
    "unselected_df = load_data(c.candle_file, c.factor_file)\n",
    "df = unselected_df[[\"Time\",\"start_time\", \"Close\", c.factor]].copy()\n",
    "\n",
    "df[[\"Time\",\"start_time\", \"Close\", c.factor]].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Visualization of Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilsnumpy import load_data, data_processing\n",
    "import matplotlib.pyplot as plt\n",
    "import config as c\n",
    "\n",
    "unselected_df = load_data(c.candle_file, c.factor_file)\n",
    "df = unselected_df[[\"start_time\", \"Close\", c.factor]].copy()\n",
    "df.columns = [\"start_time\", \"close\", c.factor]\n",
    "df = data_processing(df, \"diff\", c.factor)\n",
    "# df = data_processing(df, \"cbrt\", factor)\n",
    "\n",
    "# Visualize the raw data of factor do not need close price\n",
    "fig, ax1 = plt.subplots(figsize=(15, 8))\n",
    "ax1.plot(df['start_time'], df[c.factor], label=c.factor, color='green', linewidth=2)\n",
    "ax1.set_xlabel(\"Date\", fontsize=12)\n",
    "ax1.set_ylabel(c.factor, fontsize=12, color='green')\n",
    "ax1.tick_params(axis='y', labelcolor='green')\n",
    "# Add title and grid\n",
    "plt.title(f\"Raw Data of {c.factor}\", fontsize=16)\n",
    "fig.tight_layout()  # Adjust layout to prevent overlap\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Train Backtest + show heatmap of Split backtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from itertools import product\n",
    "from utilsnumpy import backtest , load_data, load_single_data, combine_factors\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import config as c\n",
    "\n",
    "\n",
    "annualizer_dict = {\n",
    "    '1m': 525600,  # 1-minute intervals in a year\n",
    "    '5m': 105120,  # 5-minute intervals in a year\n",
    "    '15m': 35040,  # 15-minute intervals in a year\n",
    "    '30m': 17520,  # 30-minute intervals in a year\n",
    "    '1h': 8760,    # 1-hour intervals in a year\n",
    "    '4h': 2190,    # 4-hour intervals in a year\n",
    "    '1d': 365,     # 1-day intervals in a year\n",
    "    '1w': 52,      # 1-week intervals in a year\n",
    "    '1M': 12       # 1-month intervals in a year\n",
    "}\n",
    "\n",
    "all_backtest_results = []\n",
    "plot_data = []\n",
    "\n",
    "# ✅ 第三階段（細跑）：從手動記錄讀取特定 models & entrys\n",
    "# ✅ 自動分類 models & entrys\n",
    "def parse_manual_selection(filepath, all_models):\n",
    "    \"\"\"\n",
    "    解析手動記錄的文件，根據 ALL_MODELS 自動分類 model & entry（並維持對應關係）。\n",
    "    :param filepath: 手動記錄檔案的路徑\n",
    "    :param all_models: 所有可用 models（用來辨識哪些是 model）\n",
    "    :return: 字典 { model: [entry1, entry2, ...] }\n",
    "    \"\"\"\n",
    "    with open(filepath, \"r\") as file:\n",
    "        lines = [line.strip() for line in file.readlines() if line.strip()]  # 去除空行\n",
    "\n",
    "    models_entrys = {}\n",
    "    current_model = None\n",
    "\n",
    "    for line in lines:\n",
    "        if line in all_models:  # 如果是已知 model，則開啟新的 entry 清單\n",
    "            current_model = line\n",
    "            models_entrys[current_model] = []\n",
    "        elif current_model:  # 如果是 entry，則加到當前的 model\n",
    "            models_entrys[current_model].append(line)\n",
    "\n",
    "    return models_entrys\n",
    "\n",
    "def main(data1, data2, data3, factor, factor2, interval, operation,preprocess, model, entry, window_end, window_step, threshold_end, threshold_step):\n",
    "    # Load data\n",
    "    unselected_df = load_data(data1, data2)\n",
    "    # Select wanted data column\n",
    "    df = unselected_df[[\"Time\", \"start_time\", \"Close\", factor]].copy()\n",
    "    # rename data column\n",
    "    df.columns = [\"Time\", \"start_time\", \"close\", factor]\n",
    "\n",
    "    # Load data3 if operation sign provided\n",
    "    if operation != 'none':\n",
    "        df1 = load_single_data(data3, factor2)\n",
    "        # Merge df and df1\n",
    "        df = pd.merge_asof(df, df1.sort_values('start_time'), on=\"start_time\", direction=\"nearest\")\n",
    "        df, new_column_name = combine_factors(df, factor, factor2, operation)\n",
    "        factor = new_column_name\n",
    "\n",
    "    # Check NaNs\n",
    "    num_nans = df[factor].isna().sum().sum()\n",
    "    total_rows = len(df)\n",
    "\n",
    "    if num_nans > total_rows * 0.03:\n",
    "        print(f\"Skipping factor {factor}: {num_nans} NaNs exceed 3% Threshold.\")\n",
    "        return\n",
    "    else:\n",
    "        df.dropna(inplace=True)\n",
    "\n",
    "    # metrics setting\n",
    "    window_start= 5\n",
    "    threshold_start = 0.0\n",
    "    annualizer = annualizer_dict.get(interval, None) # Day data, so 365\n",
    "    train_split = annualizer * 3\n",
    "    \n",
    "    backtest_report = []\n",
    "\n",
    "    # Split data into train and test sets (Train set: 3Year, Test set use remaining data)\n",
    "    df_train = df[:train_split].reset_index(drop=True).copy()\n",
    "    # df_test = df[train_split:].reset_index(drop=True).copy()\n",
    "\n",
    "    # backtest\n",
    "    for rolling_window in range(window_start, window_end, window_step):\n",
    "        for threshold in np.arange(threshold_start, threshold_end, threshold_step):\n",
    "            backtest_report.append(backtest(df_train, rolling_window, threshold, preprocess, entry, annualizer, model, factor, interval, \"sr\"))\n",
    "    all_backtest_results.append(backtest_report)\n",
    "    \n",
    "    # Extract pivot table for SR to plot heatmap\n",
    "    backtest_df = pd.DataFrame(backtest_report)\n",
    "    plot_data.append((model, entry, backtest_df))\n",
    "\n",
    "def plot_heatmaps(sr_threshold=1.5):\n",
    "    for model, entry, backtest_df in plot_data:\n",
    "        # ✅ Dynamically set SR threshold based on entry name\n",
    "        if 'short' in entry:\n",
    "            srthreshold = 1.2\n",
    "        elif 'long' in entry:\n",
    "            srthreshold = sr_threshold\n",
    "        else: \n",
    "            srthreshold = sr_threshold\n",
    "\n",
    "        # ✅ Optimized pivot using groupby instead of pivot\n",
    "        sr_pivot_data = backtest_df.groupby(['rolling_window', 'threshold'])['SR'].mean().unstack()\n",
    "\n",
    "        # ✅ Check if the entire heatmap is NaN\n",
    "        if sr_pivot_data.isna().all().all():\n",
    "            print(f\"⚠️ Skipping {model}_{entry} heatmap: All SR values are NaN.\")\n",
    "            continue  # Skip plotting\n",
    "\n",
    "        # ✅ Check if there is at least one SR > threshold\n",
    "        if not np.any(sr_pivot_data.to_numpy() > srthreshold):\n",
    "            print(f\"⚠️ Skipping {model}_{entry} heatmap: No SR value exceeds {srthreshold}.\")\n",
    "            continue  # Skip plotting\n",
    "\n",
    "        plt.figure(figsize=(18, 14))  # ✅ Reduced figure size for faster rendering\n",
    "        sns.heatmap(sr_pivot_data, annot=True, fmt=\".2f\", cmap=\"RdYlGn\", linewidths=0.3, cbar_kws={'label': 'Sharpe Ratio'})\n",
    "        plt.xticks(ticks=range(len(sr_pivot_data.columns)), labels=[f\"{col:.2f}\" for col in sr_pivot_data.columns], rotation=45)\n",
    "        plt.yticks(ticks=range(len(sr_pivot_data.index)), labels=[f\"{row:.2f}\" for row in sr_pivot_data.index], rotation=0)\n",
    "        plt.title(f\"{model}_{c.preprocess}_{entry} Train Period BackTest SR Heatmap\", fontsize=14)\n",
    "        plt.show()  # ✅ Display the plot\n",
    "        # 儲存 SR Heatmap\n",
    "        if c.save_plot == True:\n",
    "            plt.savefig(f\"{model}_{entry}_heatmap\", dpi=300, bbox_inches='tight')\n",
    "            print(f\"已儲存 {model}_{entry}_heatmap.png\")\n",
    "        plt.close()  # ✅ Free memory after each plot\n",
    "\n",
    "# ✅ 選擇模式：第一步 or 第三步\n",
    "USE_ALL_MODELS = True  # ✅ 設定為 True → 第一步（完整跑），設定為 False → 第三步（僅跑選擇的組合）\n",
    "\n",
    "if USE_ALL_MODELS:\n",
    "    models = c.ALL_MODELS\n",
    "    entry_map = {model: c.ALL_ENTRYS for model in c.ALL_MODELS}  # 每個 model 跑所有 entries\n",
    "    window_step = 20  # 第一階段用較大步長\n",
    "    threshold_step = 0.2\n",
    "else:\n",
    "    entry_map = parse_manual_selection(\"manual_selected.txt\", c.ALL_MODELS)  # 讀取手動篩選\n",
    "    models = list(entry_map.keys())  # 取得所有模型\n",
    "    window_step = 10  # 第三階段用較小步長\n",
    "    threshold_step = 0.1\n",
    "\n",
    "total_combinations = sum(len(entry_map[model]) for model in models)\n",
    "\n",
    "# ✅ 加入 `tqdm` 進度條（顯示每個 `model` 和 `entry`）\n",
    "with tqdm(total=total_combinations, desc=\"🔍 Backtesting Strategies\", unit=\"strategy\", leave=True) as pbar:\n",
    "    for model in models:\n",
    "        for entry in entry_map[model]:  # ✅ 只運行該 model 相關的 entries\n",
    "            pbar.set_postfix({\"Model\": model, \"Entry\": entry})  # ✅ 動態顯示當前 `model` & `entry`\n",
    "            main(\n",
    "                c.candle_file,\n",
    "                c.factor_file,\n",
    "                c.factor2_file,\n",
    "                c.factor,\n",
    "                c.factor2,\n",
    "                c.interval,\n",
    "                c.operation,\n",
    "                c.preprocess,\n",
    "                model,\n",
    "                entry,\n",
    "                window_end=301,\n",
    "                window_step=window_step,\n",
    "                threshold_end=3.51,\n",
    "                threshold_step=threshold_step\n",
    "            )\n",
    "            pbar.update(1)  # ✅ 每完成一個組合，更新進度條\n",
    "\n",
    "# ✅ Plot all heatmaps that SR > 1.5after backtesting\n",
    "plot_heatmaps(1.5)\n",
    "   \n",
    "# 生成一個json file 儲存全部 models 和 entry 的 metrics\n",
    "# output_filename = f\"{factor}_{interval}_split_backtest.json\" \n",
    "# output_backtest_data = {\"backtests\": all_backtest_results}\n",
    "# with open(output_filename, \"w\") as json_file:\n",
    "#     json.dump(output_backtest_data, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Forward Testing Period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilsnumpy import backtest , load_data, load_single_data, combine_factors\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import talib\n",
    "import math\n",
    "import json\n",
    "import config as c\n",
    "\n",
    "annualizer_dict = {\n",
    "    '1m': 525600,  # 1-minute intervals in a year\n",
    "    '5m': 105120,  # 5-minute intervals in a year\n",
    "    '15m': 35040,  # 15-minute intervals in a year\n",
    "    '30m': 17520,  # 30-minute intervals in a year\n",
    "    '1h': 8760,    # 1-hour intervals in a year\n",
    "    '4h': 2190,    # 4-hour intervals in a year\n",
    "    '1d': 365,     # 1-day intervals in a year\n",
    "    '1w': 52,      # 1-week intervals in a year\n",
    "    '1M': 12       # 1-month intervals in a year\n",
    "}\n",
    "\n",
    "def main(data1, data2, data3, factor, factor2, interval, operation, preprocess, model, entry, window, threshold):\n",
    "    # Merge Data\n",
    "    unselected_df = load_data(data1, data2)\n",
    "    # Select wanted data column\n",
    "    df = unselected_df[[\"Time\", \"start_time\", \"Close\", factor]].copy()\n",
    "    # rename data column\n",
    "    df.columns = [\"Time\", \"start_time\", \"close\", factor]\n",
    "    \n",
    "    # Load data3 if operation sign provided\n",
    "    if operation != 'none':\n",
    "        df1 = load_single_data(data3, factor2)\n",
    "        # Merge df and df1\n",
    "        df = pd.merge_asof(df, df1.sort_values('start_time'), on=\"start_time\", direction=\"nearest\")\n",
    "        df, new_column_name = combine_factors(df, factor, factor2, operation)\n",
    "        factor = new_column_name\n",
    "    \n",
    "    # metrics setting\n",
    "    rolling_window = window\n",
    "    threshold = threshold\n",
    "    annualizer = annualizer_dict.get(interval, None) # Day data, so 365\n",
    "    train_split = annualizer * 3\n",
    "\n",
    "    # Split data into train and test sets (Train set: 3Year, Test set use remaining data)\n",
    "    # df_train = df[:train_split].reset_index(drop=True).copy()\n",
    "    df_test = df[train_split:].reset_index(drop=True).copy()\n",
    "\n",
    "    forwardtest_report = []\n",
    "    forwardtest_report.append(backtest(df_test, rolling_window, threshold, preprocess, entry, annualizer, model, factor, interval))\n",
    "\n",
    "    print(json.dumps(forwardtest_report, indent=4))\n",
    "\n",
    "    # 儲存 forward test csv\n",
    "    if c.save_plot == True:\n",
    "        df.to_csv(f\"{c.alpha_id}_forward_test_df.csv\", index=False)\n",
    "        print(f\"已儲存 {c.alpha_id}_forward_test_df.csv\")\n",
    "    \n",
    "    df_test['start_time'] = pd.to_datetime(df_test['start_time'], unit='ms')\n",
    "    start_date = df_test['start_time'].min().strftime('%Y-%m-%d')\n",
    "    end_date = df_test['start_time'].max().strftime('%Y-%m-%d')\n",
    "    # Plot close price on the left y-axis\n",
    "    fig, ax1 = plt.subplots(figsize=(15, 8))\n",
    "    ax1.plot(df_test['start_time'], df_test['close'], label='Close Price', color='green', linewidth=2)\n",
    "    ax1.set_xlabel(\"Date\", fontsize=12)\n",
    "    ax1.set_ylabel(\"Close Price\", fontsize=12, color='green')\n",
    "    ax1.tick_params(axis='y', labelcolor='green')\n",
    "    # Plot cumulative PnL on the right y-axis\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(df_test['start_time'], df_test['cumu_pnl'], label='Cumulative PnL', color='blue', linewidth=2)\n",
    "    ax2.set_ylabel(\"Cumulative PnL\", fontsize=12, color='blue')\n",
    "    ax2.tick_params(axis='y', labelcolor='blue')\n",
    "    # Add title and grid\n",
    "    plt.title(f\"Close Price and Cumulative PnL Plot (Forward Test Period)-({start_date} ~ {end_date})\", fontsize=16)\n",
    "    fig.tight_layout()  # Adjust layout to prevent overlap\n",
    "    plt.grid(True)\n",
    "\n",
    "    # 儲存 Forwardtest CumuPnL 和 json file\n",
    "    if c.save_plot == True:\n",
    "        plt.savefig(f\"Forwardtest_Equity_Curve_{start_date}_{end_date}\", dpi=300, bbox_inches='tight')\n",
    "        print(f\"已儲存 Forwardtest_Equity_Curve_{start_date}_{end_date}.png\")\n",
    "        output_forwardtest_data = {\"forward_test\": forwardtest_report}\n",
    "        with open(f\"{c.alpha_id}_final_{factor}_{interval}_forward_test.json\", \"w\") as json_file:\n",
    "            json.dump(output_forwardtest_data, json_file, indent=4)\n",
    "    plt.show()\n",
    "\n",
    "main(\n",
    "    c.candle_file,\n",
    "    c.factor_file,\n",
    "    c.factor2_file,\n",
    "    c.factor,\n",
    "    c.factor2,\n",
    "    c.interval,\n",
    "    c.operation,\n",
    "    c.preprocess,\n",
    "    c.model,\n",
    "    c.entry,\n",
    "    c.window,\n",
    "    c.threshold\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Train backtest (For cumuPNL Graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilsnumpy import backtest , load_data, load_single_data, combine_factors\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import talib\n",
    "import math\n",
    "import json\n",
    "import config as c\n",
    "\n",
    "annualizer_dict = {\n",
    "    '1m': 525600,  # 1-minute intervals in a year\n",
    "    '5m': 105120,  # 5-minute intervals in a year\n",
    "    '15m': 35040,  # 15-minute intervals in a year\n",
    "    '30m': 17520,  # 30-minute intervals in a year\n",
    "    '1h': 8760,    # 1-hour intervals in a year\n",
    "    '4h': 2190,    # 4-hour intervals in a year\n",
    "    '1d': 365,     # 1-day intervals in a year\n",
    "    '1w': 52,      # 1-week intervals in a year\n",
    "    '1M': 12       # 1-month intervals in a year\n",
    "}\n",
    "\n",
    "def main(data1, data2, data3, factor, factor2, interval, operation, preprocess, model, entry, window, threshold):\n",
    "    # Merge Data\n",
    "    unselected_df = load_data(data1, data2)\n",
    "    # Select wanted data column\n",
    "    df = unselected_df[[\"Time\", \"start_time\", \"Close\", factor]].copy()\n",
    "    # rename data column\n",
    "    df.columns = [\"Time\", \"start_time\", \"close\", factor]\n",
    "    \n",
    "    # Load data3 if operation sign provided\n",
    "    if operation != 'none':\n",
    "        df1 = load_single_data(data3, factor2)\n",
    "        # Merge df and df1\n",
    "        df = pd.merge_asof(df, df1.sort_values('start_time'), on=\"start_time\", direction=\"nearest\")\n",
    "        df, new_column_name = combine_factors(df, factor, factor2, operation)\n",
    "        factor = new_column_name    \n",
    "    # metrics setting\n",
    "    rolling_window = window\n",
    "    threshold = threshold\n",
    "    annualizer = annualizer_dict.get(interval, None)\n",
    "    train_split = annualizer * 3\n",
    "\n",
    "    # Split data into train and test sets (Train set: 3Year, Test set use remaining data)\n",
    "    df_train = df[:train_split].reset_index(drop=True).copy()\n",
    "    # df_test = df[train_split:].reset_index(drop=True).copy()\n",
    "\n",
    "    backtest_report = []\n",
    "    backtest_report.append(backtest(df_train, rolling_window, threshold, preprocess, entry, annualizer, model, factor, interval))\n",
    "\n",
    "    print(json.dumps(backtest_report, indent=4))\n",
    "\n",
    "    # 儲存 backtest csv\n",
    "    if c.save_plot == True:\n",
    "        df.to_csv(f\"{c.alpha_id}_backtest_df.csv\", index=False)\n",
    "        print(f\"已儲存 {c.alpha_id}_backtest_df.csv\")\n",
    "    \n",
    "    df_train['start_time'] = pd.to_datetime(df_train['start_time'], unit='ms')\n",
    "    start_date = df_train['start_time'].min().strftime('%Y-%m-%d')\n",
    "    end_date = df_train['start_time'].max().strftime('%Y-%m-%d')\n",
    "    # Plot close price on the left y-axis\n",
    "    fig, ax1 = plt.subplots(figsize=(15, 8))\n",
    "    ax1.plot(df_train['start_time'], df_train['close'], label='Close Price', color='green', linewidth=2)\n",
    "    ax1.set_xlabel(\"Date\", fontsize=12)\n",
    "    ax1.set_ylabel(\"Close Price\", fontsize=12, color='green')\n",
    "    ax1.tick_params(axis='y', labelcolor='green')\n",
    "    # Plot cumulative PnL on the right y-axis\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(df_train['start_time'], df_train['cumu_pnl'], label='Cumulative PnL', color='blue', linewidth=2)\n",
    "    ax2.set_ylabel(\"Cumulative PnL\", fontsize=12, color='blue')\n",
    "    ax2.tick_params(axis='y', labelcolor='blue')\n",
    "    # Add title and grid\n",
    "    plt.title(f\"Close Price and Cumulative PnL Plot (Split Backtest Period)-({start_date} ~ {end_date})\", fontsize=16)\n",
    "    fig.tight_layout()  # Adjust layout to prevent overlap\n",
    "    plt.grid(True)\n",
    "    # 儲存 Backtest CumuPnL\n",
    "    if c.save_plot == True:\n",
    "        plt.savefig(f\"Backtest_Equity_Curve_{start_date}_{end_date}\", dpi=300, bbox_inches='tight')\n",
    "        print(f\"已儲存 Backtest_Equity_Curve_{start_date}_{end_date}.png\")\n",
    "        output_backtest_data = {\"back_test\": backtest_report}\n",
    "        with open(f\"{c.alpha_id}_final_{factor}_{interval}_back_test.json\", \"w\") as json_file:\n",
    "            json.dump(output_backtest_data, json_file, indent=4)\n",
    "    plt.show()\n",
    "\n",
    "main(\n",
    "    c.candle_file,\n",
    "    c.factor_file,\n",
    "    c.factor2_file,\n",
    "    c.factor,\n",
    "    c.factor2,\n",
    "    c.interval,\n",
    "    c.operation,\n",
    "    c.preprocess,\n",
    "    c.model,\n",
    "    c.entry,\n",
    "    c.window,\n",
    "    c.threshold\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backtest(No Permutation)(HandTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilsnumpy import backtest , load_data, load_single_data, combine_factors\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import talib\n",
    "import math\n",
    "import json\n",
    "import modin\n",
    "import config as c\n",
    "\n",
    "annualizer_dict = {\n",
    "    '1m': 525600,  # 1-minute intervals in a year\n",
    "    '5m': 105120,  # 5-minute intervals in a year\n",
    "    '15m': 35040,  # 15-minute intervals in a year\n",
    "    '30m': 17520,  # 30-minute intervals in a year\n",
    "    '1h': 8760,    # 1-hour intervals in a year\n",
    "    '4h': 2190,    # 4-hour intervals in a year\n",
    "    '1d': 365,     # 1-day intervals in a year\n",
    "    '1w': 52,      # 1-week intervals in a year\n",
    "    '1M': 12       # 1-month intervals in a year\n",
    "}\n",
    "\n",
    "def main(data1, data2, data3, factor, factor2, interval, operation, preprocess, model, entry, window, threshold):\n",
    "    # Merge Data\n",
    "    unselected_df = load_data(data1, data2)\n",
    "    # Select wanted data column\n",
    "    df = unselected_df[[\"Time\", \"start_time\", \"Close\", factor]].copy()\n",
    "    # rename data column\n",
    "    df.columns = [\"Time\", \"start_time\", \"close\", factor]\n",
    "    \n",
    "    # Load data3 if operation sign provided\n",
    "    if operation != 'none':\n",
    "        df1 = load_single_data(data3, factor2)\n",
    "        # Merge df and df1\n",
    "        df = pd.merge_asof(df, df1.sort_values('start_time'), on=\"start_time\", direction=\"nearest\")\n",
    "        df, new_column_name = combine_factors(df, factor, factor2, operation)\n",
    "        factor = new_column_name\n",
    "        \n",
    "    # metrics setting\n",
    "    rolling_window = window\n",
    "    threshold = threshold\n",
    "    annualizer = annualizer_dict.get(interval, None) # Day data, so 365\n",
    "\n",
    "    backtest_report = []\n",
    "    backtest_report.append(backtest(df, rolling_window, threshold, preprocess, entry, annualizer, model, factor, interval))\n",
    "\n",
    "    print(json.dumps(backtest_report, indent=4))\n",
    "\n",
    "\n",
    "    # 儲存 full time test csv\n",
    "    if c.save_plot == True:\n",
    "        df.to_csv(f\"{c.alpha_id}_full_time_backtest_df.csv\", index=False)\n",
    "        print(f\"已儲存 {c.alpha_id}_full_time_backtest_df.csv\")\n",
    "    # df.to_csv(f\"./liveRunning/excel_for_each_backtest/{factor}_{preprocess}_{interval}_{model}_{entry}_{window}_{threshold}.csv\", index=False)\n",
    "\n",
    "    df['start_time'] = pd.to_datetime(df['start_time'], unit='ms')\n",
    "    start_date = df['start_time'].min().strftime('%Y-%m-%d')\n",
    "    end_date = df['start_time'].max().strftime('%Y-%m-%d')\n",
    "    # Plot close price on the left y-axis\n",
    "    fig, ax1 = plt.subplots(figsize=(15, 8))\n",
    "    ax1.plot(df['start_time'], df['close'], label='Close Price', color='green', linewidth=2)\n",
    "    ax1.set_xlabel(\"Date\", fontsize=12)\n",
    "    ax1.set_ylabel(\"Close Price\", fontsize=12, color='green')\n",
    "    ax1.tick_params(axis='y', labelcolor='green')\n",
    "    # Plot cumulative PnL on the right y-axis\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(df['start_time'], df['cumu_pnl'], label='Cumulative PnL', color='blue', linewidth=2)\n",
    "    ax2.set_ylabel(\"Cumulative PnL\", fontsize=12, color='blue')\n",
    "    ax2.tick_params(axis='y', labelcolor='blue')\n",
    "    # Add title and grid\n",
    "    plt.title(f\"Close Price and Cumulative PnL Plot (Full Length)-({start_date} ~ {end_date})\", fontsize=16)\n",
    "    fig.tight_layout()  # Adjust layout to prevent overlap\n",
    "    plt.grid(True)                  \n",
    "    plt.show()\n",
    "\n",
    "main(\n",
    "    c.candle_file,\n",
    "    c.factor_file,\n",
    "    c.factor2_file,\n",
    "    c.factor,\n",
    "    c.factor2,\n",
    "    c.interval,\n",
    "    c.operation,\n",
    "    c.preprocess,\n",
    "    c.model,\n",
    "    c.entry,\n",
    "    c.window,\n",
    "    c.threshold\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cybotrade",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
