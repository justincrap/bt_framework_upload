{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 每次运行时重新加载config.py\n",
    "from importlib import reload\n",
    "import config as c\n",
    "reload(c)\n",
    "\n",
    "print(f\"已加载配置：factor={c.factor}, factor2={c.factor2}, operation={c.operation}, preprocess={c.preprocess}, USE_ALL_MODELS={c.USE_ALL_MODELS}\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resampling 1min data to wanted timeframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import config as c\n",
    "\n",
    "def prepare_price_data(\n",
    "    csv_path: str,  # 輸入 CSV 檔案完整路徑\n",
    "    datasource: str = 'bybit_btcusdt',\n",
    "    factor: str = 'price',\n",
    "    timeframe: str = '1D', \n",
    "    delay_minutes: int = 0\n",
    "):\n",
    "    \"\"\"\n",
    "    讀取 1m 資料，轉成指定的時間週期 (timeframe)，\n",
    "    可選擇延遲(正值)或提前(負值)時間索引，並自動存檔到當前工作目錄。\n",
    "\n",
    "    參數：\n",
    "      csv_path      : 【完整路徑】輸入的 1m 級別 CSV 檔案\n",
    "      datasource    : 資料來源名稱 (如 bybit_btcusdt)\n",
    "      factor        : 影響因子名稱 (如 price)\n",
    "      timeframe     : 轉換後的時間週期，如 '1H'、'1D' 等 (預設 '1D')\n",
    "      delay_minutes : 時間平移的分鐘數 (正值 = 延後；負值 = 提前)\n",
    "\n",
    "    回傳：\n",
    "      pandas DataFrame (resampled 後的結果)，\n",
    "      並將結果輸出為 CSV，命名格式：\n",
    "      {datasource}_{factor}_{timeframe}_{start_time}_{end_time}.csv\n",
    "    \"\"\"\n",
    "    # 1. 讀取 CSV，解析時間\n",
    "    df = pd.read_csv(\n",
    "        csv_path, \n",
    "        parse_dates=['Time']  # pandas 會自動解析時間格式\n",
    "    )\n",
    "\n",
    "    # 2. 將 'Time' 欄設為索引\n",
    "    df.set_index('Time', inplace=True)\n",
    "\n",
    "    # 3. 時間平移 (延遲 / 提前)\n",
    "    if delay_minutes != 0:\n",
    "        df.index = df.index + pd.Timedelta(minutes=delay_minutes)\n",
    "\n",
    "    # 4. 定義 resample 聚合方式\n",
    "    if c.exchange_name == 'bybit':\n",
    "        ohlc_dict = {\n",
    "            'Open': 'first',\n",
    "            'High': 'max',\n",
    "            'Low': 'min',\n",
    "            'Close': 'last',\n",
    "            'Volume': 'sum',\n",
    "            'Turnover': 'sum'\n",
    "        }\n",
    "    else:\n",
    "        ohlc_dict = {\n",
    "        'Open': 'first',\n",
    "        'High': 'max',\n",
    "        'Low': 'min',\n",
    "        'Close': 'last',\n",
    "        'Volume': 'sum',\n",
    "    }\n",
    "    \n",
    "    # 5. 進行 resample\n",
    "    df_resampled = df.resample(timeframe).agg(ohlc_dict).dropna(how='any')\n",
    "\n",
    "    # Use Time to create one more column named 'start_time' that is in unix timestamp\n",
    "    df_resampled['start_time'] = df_resampled.index.astype('int64') // 10**6\n",
    "    # df_resampled['start_time'] = df_resampled['start_time'].astype('float64')\n",
    "\n",
    "    # 6. 獲取開始與結束時間 (格式 YYYY-MM-DD)\n",
    "    if not df_resampled.empty:\n",
    "        start_time = df_resampled.index[0].strftime('%Y-%m-%d')\n",
    "        end_time = df_resampled.index[-1].strftime('%Y-%m-%d')\n",
    "\n",
    "        # 7. 構建輸出檔案名稱\n",
    "        output_filename = f\"./data/resample_{datasource}_{timeframe}_-{c.candle_delay}m.csv\"\n",
    "        output_path = os.path.join(os.getcwd(), output_filename)  # 當前工作目錄\n",
    "\n",
    "        # 8. 輸出 CSV\n",
    "        df_resampled.to_csv(output_path)\n",
    "        print(f\"✅ 檔案已儲存：{output_path}\")\n",
    "    else:\n",
    "        print(\"⚠️ Resampled DataFrame 為空，未產生輸出檔案！\")\n",
    "\n",
    "    return df_resampled\n",
    "\n",
    "df_r = prepare_price_data(\n",
    "    csv_path=f\"./data/{c.exchange_name}_{c.coin}usdt_price_1m.csv\",\n",
    "    datasource=f'{c.exchange_name}_{c.coin}',\n",
    "    factor='price',\n",
    "    timeframe=c.candle_timeframe,\n",
    "    delay_minutes=-c.candle_delay\n",
    ")\n",
    "\n",
    "print(df_r.head()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilsnumpy import load_all_data, combines_data, data_processing\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import config as c\n",
    "\n",
    "def visualize_factors():\n",
    "    \"\"\"\n",
    "    加载并可视化因子数据，支持因子组合和预处理\n",
    "    \"\"\"\n",
    "    print(\"Loading data...\")\n",
    "    # 加载原始数据\n",
    "    raw_candle, raw_factor = load_all_data(c.candle_file, c.factor_file, c.factor2_file, c.factor, c.factor2)\n",
    "    \n",
    "    # 处理时间格式以便于绘图\n",
    "    raw_factor['time'] = pd.to_datetime(raw_factor['start_time'], unit='ms')\n",
    "    raw_candle['time'] = pd.to_datetime(raw_candle['start_time'], unit='ms')\n",
    "    \n",
    "    # 初始化绘图\n",
    "    fig, axes = plt.subplots(3, 1, figsize=(15, 18), sharex=True)\n",
    "    \n",
    "    # 绘制价格数据\n",
    "    axes[0].plot(raw_candle['time'], raw_candle['Close'], color='blue', linewidth=1.5)\n",
    "    axes[0].set_title(f'Price Data: {c.symbol}', fontsize=16)\n",
    "    axes[0].set_ylabel('Price', fontsize=14)\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 绘制原始因子数据\n",
    "    axes[1].plot(raw_factor['time'], raw_factor[c.factor], color='green', linewidth=1.5, label=c.factor)\n",
    "    if c.factor2 in raw_factor.columns:\n",
    "        axes[1].plot(raw_factor['time'], raw_factor[c.factor2], color='red', linewidth=1.5, label=c.factor2)\n",
    "    axes[1].set_title('Raw Factor Data', fontsize=16)\n",
    "    axes[1].set_ylabel('Factor Value', fontsize=14)\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    axes[1].legend()\n",
    "    \n",
    "    # 处理组合因子和预处理\n",
    "    factor_data = raw_factor.copy()\n",
    "    factor_to_display = c.factor\n",
    "    factor_label = c.factor\n",
    "    \n",
    "    # 如果需要組合因子\n",
    "    if c.operation != 'none' and c.factor2 in factor_data.columns:\n",
    "        print(f\"Combining factors with operation: {c.operation}\")\n",
    "        combined_values, combined_name = combines_data(\n",
    "            factor_data[c.factor].values, \n",
    "            factor_data[c.factor2].values, \n",
    "            c.operation, \n",
    "            c.factor, \n",
    "            c.factor2\n",
    "        )\n",
    "        factor_data[combined_name] = combined_values\n",
    "        factor_to_display = combined_name\n",
    "        factor_label = f\"{c.factor} {c.operation} {c.factor2}\"\n",
    "    \n",
    "    # 如果需要预处理\n",
    "    if c.preprocess != \"direct\":\n",
    "        print(f\"Applying preprocessing: {c.preprocess}\")\n",
    "        processed_values = data_processing(factor_data[factor_to_display], c.preprocess, factor_to_display)\n",
    "        factor_data[f\"{factor_to_display}_{c.preprocess}\"] = processed_values\n",
    "        factor_to_display = f\"{factor_to_display}_{c.preprocess}\"\n",
    "        factor_label = f\"{factor_label} ({c.preprocess})\"\n",
    "    \n",
    "    # 绘制处理后的因子数据\n",
    "    axes[2].plot(factor_data['time'], factor_data[factor_to_display], color='purple', linewidth=1.5)\n",
    "    axes[2].set_title('Processed Factor Data', fontsize=16)\n",
    "    axes[2].set_ylabel('Factor Value', fontsize=14)\n",
    "    axes[2].set_xlabel('Date', fontsize=14)\n",
    "    axes[2].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 在标题旁添加因子描述\n",
    "    axes[2].text(0.01, 0.95, f\"Factor: {factor_label}\", transform=axes[2].transAxes, \n",
    "                 fontsize=12, verticalalignment='top', bbox=dict(boxstyle='round', facecolor='white', alpha=0.5))\n",
    "    \n",
    "    # 格式化X轴日期\n",
    "    for ax in axes:\n",
    "        ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n",
    "        ax.xaxis.set_major_locator(mdates.MonthLocator(interval=3))\n",
    "        plt.setp(ax.xaxis.get_majorticklabels(), rotation=45)\n",
    "    \n",
    "    # 添加整体标题\n",
    "    plt.suptitle(f'Factor Analysis - {c.symbol}', fontsize=20, y=0.98)\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.97])\n",
    "    \n",
    "    # 保存图像\n",
    "    if hasattr(c, 'save_plot') and c.save_plot:\n",
    "        # plt.savefig(f\"factor_analysis_{c.symbol}_{c.factor}.png\", dpi=300, bbox_inches='tight')\n",
    "        # print(f\"图像已保存为: factor_analysis_{c.symbol}_{c.factor}.png\")\n",
    "        pass\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    # 返回处理后的数据，以便进一步分析\n",
    "    return {\n",
    "        'candle': raw_candle,\n",
    "        'factor': factor_data,\n",
    "        'factor_name': factor_to_display\n",
    "    }\n",
    "\n",
    "def visualize_factor_correlation(data):\n",
    "    \"\"\"\n",
    "    可视化因子与价格的相关性\n",
    "    \n",
    "    参数:\n",
    "    data: 由visualize_factors函数返回的数据字典\n",
    "    \"\"\"\n",
    "    if data is None:\n",
    "        print(\"没有数据可用于相关性分析\")\n",
    "        return\n",
    "    \n",
    "    candle_df = data['candle']\n",
    "    factor_df = data['factor']\n",
    "    factor_name = data['factor_name']\n",
    "    \n",
    "    # 合并价格和因子数据\n",
    "    merged_df = pd.merge_asof(\n",
    "        candle_df.sort_values('time'), \n",
    "        factor_df[['time', factor_name]].sort_values('time'),\n",
    "        on='time',\n",
    "        direction='nearest'\n",
    "    )\n",
    "    \n",
    "    # 计算价格变化\n",
    "    merged_df['price_change'] = merged_df['Close'].pct_change()\n",
    "    \n",
    "    # 去除NaN值\n",
    "    merged_df = merged_df.dropna()\n",
    "    \n",
    "    # 计算相关系数\n",
    "    correlation = merged_df['price_change'].corr(merged_df[factor_name])\n",
    "    \n",
    "    # 绘制散点图\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.scatter(merged_df[factor_name], merged_df['price_change'], alpha=0.5)\n",
    "    plt.axhline(y=0, color='r', linestyle='-', alpha=0.3)\n",
    "    plt.axvline(x=0, color='r', linestyle='-', alpha=0.3)\n",
    "    \n",
    "    # 添加趋势线\n",
    "    z = np.polyfit(merged_df[factor_name], merged_df['price_change'], 1)\n",
    "    p = np.poly1d(z)\n",
    "    plt.plot(merged_df[factor_name], p(merged_df[factor_name]), \"r--\", alpha=0.8)\n",
    "    \n",
    "    plt.title(f'Correlation between Price Change and {factor_name}: {correlation:.4f}', fontsize=16)\n",
    "    plt.xlabel(factor_name, fontsize=14)\n",
    "    plt.ylabel('Price Change (%)', fontsize=14)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 保存图像\n",
    "    if hasattr(c, 'save_plot') and c.save_plot:\n",
    "        # plt.savefig(f\"correlation_{c.symbol}_{factor_name}.png\", dpi=300, bbox_inches='tight')\n",
    "        # print(f\"图像已保存为: correlation_{c.symbol}_{factor_name}.png\")\n",
    "        pass\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "def visualize_factor_distribution(data):\n",
    "    \"\"\"\n",
    "    可视化因子值的分布\n",
    "    \n",
    "    参数:\n",
    "    data: 由visualize_factors函数返回的数据字典\n",
    "    \"\"\"\n",
    "    if data is None:\n",
    "        print(\"没有数据可用于分布分析\")\n",
    "        return\n",
    "    \n",
    "    factor_df = data['factor']\n",
    "    factor_name = data['factor_name']\n",
    "    \n",
    "    # 去除NaN值\n",
    "    factor_values = factor_df[factor_name].dropna().values\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # 绘制直方图\n",
    "    plt.hist(factor_values, bins=50, alpha=0.7, color='blue')\n",
    "    \n",
    "    # 添加基本统计信息\n",
    "    mean_val = np.mean(factor_values)\n",
    "    median_val = np.median(factor_values)\n",
    "    std_val = np.std(factor_values)\n",
    "    \n",
    "    plt.axvline(mean_val, color='r', linestyle='dashed', linewidth=1, label=f'Mean: {mean_val:.4f}')\n",
    "    plt.axvline(median_val, color='g', linestyle='dashed', linewidth=1, label=f'Median: {median_val:.4f}')\n",
    "    \n",
    "    plt.title(f'Distribution of {factor_name}', fontsize=16)\n",
    "    plt.xlabel('Value', fontsize=14)\n",
    "    plt.ylabel('Frequency', fontsize=14)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend()\n",
    "    \n",
    "    # 文本框显示统计信息\n",
    "    stats_text = f\"\"\"\n",
    "    Mean: {mean_val:.4f}\n",
    "    Median: {median_val:.4f}\n",
    "    Std Dev: {std_val:.4f}\n",
    "    Min: {np.min(factor_values):.4f}\n",
    "    Max: {np.max(factor_values):.4f}\n",
    "    \"\"\"\n",
    "    plt.text(0.01, 0.95, stats_text, transform=plt.gca().transAxes, \n",
    "             fontsize=12, verticalalignment='top', \n",
    "             bbox=dict(boxstyle='round', facecolor='white', alpha=0.5))\n",
    "    \n",
    "    # 保存图像\n",
    "    if hasattr(c, 'save_plot') and c.save_plot:\n",
    "        # plt.savefig(f\"distribution_{c.symbol}_{factor_name}.png\", dpi=300, bbox_inches='tight')\n",
    "        # print(f\"图像已保存为: distribution_{c.symbol}_{factor_name}.png\")\n",
    "        pass\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 执行因子可视化\n",
    "    data = visualize_factors()\n",
    "    \n",
    "    # 可视化因子与价格的相关性\n",
    "    visualize_factor_correlation(data)\n",
    "    \n",
    "    # 可视化因子分布\n",
    "    visualize_factor_distribution(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Split Loop + Heatmap (Inclding looping Preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilsnumpy import nan_count, load_all_data, combines_data, data_processing, precompute_rolling_stats, backtest_cached\n",
    "import matplotlib.pyplot as plt\n",
    "import config as c\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import sys\n",
    "from dask import delayed, compute\n",
    "from dask.diagnostics import ProgressBar\n",
    "\n",
    "def parse_manual_selection(filepath, all_models):\n",
    "    \"\"\"\n",
    "    解析手動選擇的模型與進出場方式。\n",
    "    \"\"\"\n",
    "    with open(filepath, \"r\") as file:\n",
    "        lines = [line.strip() for line in file.readlines() if line.strip()]\n",
    "    models_entrys = {}\n",
    "    current_model = None\n",
    "    for line in lines:\n",
    "        if line in all_models:\n",
    "            current_model = line\n",
    "            models_entrys[current_model] = []\n",
    "        elif current_model:\n",
    "            models_entrys[current_model].append(line)\n",
    "    return models_entrys\n",
    "\n",
    "def plot_heatmaps(sr_threshold=1.5, preprocess_method=\"direct\"):\n",
    "    \"\"\"\n",
    "    依據回測結果繪製 SR 熱力圖，\n",
    "    並在標題中標示當前使用的 preprocess_method。\n",
    "    \"\"\"\n",
    "    for model, entry, backtest_df in plot_data:\n",
    "        if 'short' in entry:\n",
    "            srthreshold = 1.2\n",
    "        elif 'long' in entry:\n",
    "            srthreshold = sr_threshold\n",
    "        else:\n",
    "            srthreshold = sr_threshold\n",
    "        sr_pivot_data = backtest_df.groupby(['rolling_window', 'threshold'])['SR'].mean().unstack()\n",
    "        sr_pivot_data.columns = sr_pivot_data.columns.round(2)\n",
    "        if sr_pivot_data.isna().all().all():\n",
    "            print(f\"⚠️ Skipping {model}_{entry} heatmap: All SR values are NaN.\")\n",
    "            continue\n",
    "        if not np.any(sr_pivot_data.to_numpy() > srthreshold):\n",
    "            print(f\"⚠️ Skipping {model}_{entry} heatmap: No SR value exceeds {srthreshold}.\")\n",
    "            continue\n",
    "        plt.figure(figsize=(18, 14))\n",
    "        sns.heatmap(sr_pivot_data, annot=True, fmt=\".2f\", cmap=\"RdYlGn\", linewidths=0.3,\n",
    "                    cbar_kws={'label': 'Sharpe Ratio'})\n",
    "        plt.xticks(ticks=range(len(sr_pivot_data.columns)),\n",
    "                   labels=[f\"{col:.2f}\" for col in sr_pivot_data.columns], rotation=45)\n",
    "        plt.yticks(ticks=range(len(sr_pivot_data.index)),\n",
    "                   labels=[f\"{row:.2f}\" for row in sr_pivot_data.index], rotation=0)\n",
    "        plt.title(f\"{model}_{preprocess_method}_{entry} Train Period BackTest SR Heatmap\", fontsize=14)\n",
    "        if c.save_plot:\n",
    "            plt.savefig(f\"{model}_{entry}_heatmap\", dpi=300, bbox_inches='tight')\n",
    "            print(f\"已儲存 {model}_{entry}_heatmap.png\")\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "\n",
    "def process_and_validate(factor_series, method, factor_name):\n",
    "    \"\"\"\n",
    "    使用指定的 preprocess 方法處理資料並檢查 NaN 百分比。\n",
    "    \n",
    "    Parameters:\n",
    "        factor_series (pd.Series): 原始的因子資料。\n",
    "        method (str): 要套用的預處理方法。\n",
    "        factor_name (str): 因子名稱，用於 debug 訊息。\n",
    "        \n",
    "    Returns:\n",
    "        pd.Series: 若處理後 NaN 低於 3%，則回傳處理後的資料；否則回傳 None 表示跳過此方法。\n",
    "    \"\"\"\n",
    "    if method != \"direct\":\n",
    "        processed = data_processing(factor_series, method, factor_name)\n",
    "    else:\n",
    "        processed = factor_series.copy()\n",
    "    if processed.isna().sum() / len(processed) > 0.03:\n",
    "        preprocess_nan_count = nan_count(processed)\n",
    "        print(f\"nan count After {method} Preprocessing: {preprocess_nan_count}\")\n",
    "        print(f\"{factor_name} after {method} transformation exceed 3% NaN. Skipping this preprocess method.\")\n",
    "        return None\n",
    "    else:\n",
    "        print(f\"NaN% after preprocess: {processed.isna().sum()/len(processed)}%.\")\n",
    "        processed.dropna(inplace=True)\n",
    "    return processed\n",
    "\n",
    "def main(candle_data, factor_data, factor, interval, operation, model, entry,\n",
    "         window_start, window_end, window_step, threshold_start, threshold_end,\n",
    "         threshold_step, rolling_stats, preprocess_method, date_range):\n",
    "    \"\"\"\n",
    "    回測主程式，根據參數進行多參數回測。\n",
    "    \"\"\"\n",
    "    candle_df_copy = candle_data[['start_time', 'Close']].copy()\n",
    "    candle_df_copy.columns = ['start_time', 'close']\n",
    "    factor_df_copy = factor_data[['start_time', factor]].copy()\n",
    "\n",
    "    candle_df_copy['time'] = pd.to_datetime(candle_df_copy['start_time'], unit='ms')\n",
    "    \n",
    "    annualizer = annualizer_dict.get(interval, None)\n",
    "    backtest_report = []\n",
    "    for rolling_window in range(window_start, window_end, window_step):\n",
    "        for threshold in np.arange(threshold_start, threshold_end, threshold_step):\n",
    "            result, _, log_msgs = backtest_cached(candle_df_copy, factor_df_copy, rolling_window, threshold, \n",
    "                                          preprocess_method, entry, annualizer, model, factor, interval, date_range,\n",
    "                                          rolling_stats)\n",
    "            backtest_report.append(result)\n",
    "    \n",
    "    backtest_df = pd.DataFrame(backtest_report)\n",
    "    return (model, entry, backtest_df, log_msgs)\n",
    "\n",
    "# 定義 annualizer 字典\n",
    "annualizer_dict = {\n",
    "    '1m': 525600, '5m': 105120, '15m': 35040,\n",
    "    '30m': 17520, '1h': 8760, '4h': 2190,\n",
    "    '1d': 365, '1w': 52, '1M': 12\n",
    "}\n",
    "\n",
    "# 載入原始資料\n",
    "raw_candle, raw_factor = load_all_data(c.candle_file, c.factor_file, c.factor2_file, c.factor, c.factor2)\n",
    "train_split = annualizer_dict.get(c.interval, None) * 3\n",
    "\n",
    "candle_train = raw_candle[:train_split].reset_index(drop=True).copy()\n",
    "factor_train_original = raw_factor[:train_split].reset_index(drop=True).copy()\n",
    "\n",
    "# Start and end Time and date_range\n",
    "start_time = max(candle_train['start_time'].min(), factor_train_original['start_time'].min())\n",
    "end_time = min(candle_train['start_time'].max(), factor_train_original['start_time'].max())\n",
    "date_range = pd.date_range(start=pd.to_datetime(start_time, unit='ms'),\n",
    "                           end=pd.to_datetime(end_time, unit='ms'),\n",
    "                           freq=c.interval)\n",
    "candle_train['time'] = pd.to_datetime(candle_train['start_time'], unit='ms')\n",
    "factor_train_original['time'] = pd.to_datetime(factor_train_original['start_time'], unit='ms')\n",
    "candle_train.set_index('time', inplace=True)\n",
    "factor_train_original.set_index('time', inplace=True)\n",
    "\n",
    "# 若 operation 不是 'none'，則合併兩個因子\n",
    "if c.operation != 'none':\n",
    "    combined_data, merged_col_name = combines_data(factor_train_original[c.factor].values, \n",
    "                                                 factor_train_original[c.factor2].values, \n",
    "                                                 c.operation, c.factor, c.factor2)\n",
    "    factor_train_original[merged_col_name] = combined_data\n",
    "    factor_used = merged_col_name\n",
    "else:\n",
    "    factor_used = c.factor\n",
    "\n",
    "################################################\n",
    "# Step 0: 判斷 c.preprocess 是單一字串，還是串列\n",
    "################################################\n",
    "if isinstance(c.preprocess, list):\n",
    "    all_preprocess_methods = c.preprocess\n",
    "else:\n",
    "    all_preprocess_methods = [c.preprocess]\n",
    "\n",
    "# 對每個 preprocess 方法進行迴圈\n",
    "for current_preprocess in all_preprocess_methods:\n",
    "    print(f\"\\n===== Processing with preprocess method: {current_preprocess} =====\")\n",
    "    # 從原始資料複製一份\n",
    "    factor_train = factor_train_original.copy()\n",
    "    # 對指定因子進行預處理\n",
    "    processed_factor = process_and_validate(factor_train[factor_used], current_preprocess, factor_used)\n",
    "    if processed_factor is None:\n",
    "        continue  # 如果驗證不通過，則跳到下一個 preprocess 方法\n",
    "    factor_train[factor_used] = processed_factor\n",
    "\n",
    "    # 模型與進出場設定\n",
    "    if c.USE_ALL_MODELS:\n",
    "        models = c.ALL_MODELS\n",
    "        entry_map = {model: c.ALL_ENTRYS for model in c.ALL_MODELS}\n",
    "        window_step = 20\n",
    "        threshold_step = 0.2\n",
    "    else:\n",
    "        entry_map = parse_manual_selection(\"manual_selected.txt\", c.ALL_MODELS)\n",
    "        models = list(entry_map.keys())\n",
    "        window_step = 10\n",
    "        threshold_step = 0.1\n",
    "\n",
    "    # 預先計算滾動統計值\n",
    "    windows = list(range(5, 351, window_step))\n",
    "    rolling_stats_dict = precompute_rolling_stats(factor_train[factor_used], windows)\n",
    "\n",
    "    # 重置 plot_data 以儲存當前 preprocess 方法的回測結果\n",
    "    plot_data = []\n",
    "    tasks = []\n",
    "    for model in models:\n",
    "        for entry in entry_map[model]:\n",
    "            task = delayed(main)(\n",
    "                candle_train,\n",
    "                factor_train,\n",
    "                factor_used,\n",
    "                c.interval,\n",
    "                c.operation,\n",
    "                model,\n",
    "                entry,\n",
    "                window_start=5,\n",
    "                window_end=351,\n",
    "                window_step=window_step,\n",
    "                threshold_start=0,\n",
    "                threshold_end=4.01,\n",
    "                threshold_step=threshold_step,\n",
    "                rolling_stats=rolling_stats_dict,\n",
    "                preprocess_method=current_preprocess,\n",
    "                date_range=date_range\n",
    "            )\n",
    "            tasks.append(task)\n",
    "\n",
    "    with ProgressBar():\n",
    "        results = compute(*tasks, scheduler='processes')\n",
    "    \n",
    "    for res in results:\n",
    "        if res is not None:\n",
    "            m, e, backtest_df, log_msgs = res\n",
    "            for msg in log_msgs:\n",
    "                print(msg)\n",
    "            plot_data.append((m, e, backtest_df))\n",
    "\n",
    "    # 繪製當前 preprocess 方法的熱力圖\n",
    "    plot_heatmaps(1.65, preprocess_method=current_preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split forward, Split Backtest, full_length_backtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilsnumpy import nan_count, load_all_data, combines_data, data_processing, backtest_cached, additional_metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import config as c\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import sys\n",
    "import os\n",
    "\n",
    "annualizer_dict = {\n",
    "    '1m': 525600, '5m': 105120, '15m': 35040,\n",
    "    '30m': 17520, '1h': 8760, '4h': 2190,\n",
    "    '1d': 365, '1w': 52, '1M': 12\n",
    "}\n",
    "\n",
    "def prepare_backtest_data(candle_data, factor_data, factor, interval):\n",
    "    \"\"\"\n",
    "    Preprocesses candle and factor data for backtesting by:\n",
    "    1. Creating time columns and setting them as indices\n",
    "    2. Computing the common time range\n",
    "    3. Creating a full date range for the time period\n",
    "    4. Reindexing and forward-filling the data\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary containing preprocessed candle and factor DataFrames, and the date range\n",
    "    \"\"\"\n",
    "    # Create copies to avoid modifying original data\n",
    "    candle_df_copy = candle_data[['start_time', 'Close']].copy()\n",
    "    candle_df_copy.columns = ['start_time', 'close']\n",
    "    factor_df_copy = factor_data[['start_time', factor]].copy()\n",
    "    \n",
    "    # Compute common time range\n",
    "    start_time = max(candle_df_copy['start_time'].min(), factor_df_copy['start_time'].min())\n",
    "    end_time = min(candle_df_copy['start_time'].max(), factor_df_copy['start_time'].max())\n",
    "    \n",
    "    # Convert timestamps to datetime and set as index\n",
    "    candle_df_copy['time'] = pd.to_datetime(candle_df_copy['start_time'], unit='ms')\n",
    "    factor_df_copy['time'] = pd.to_datetime(factor_df_copy['start_time'], unit='ms')\n",
    "    \n",
    "    candle_df_copy.set_index('time', inplace=True)\n",
    "    factor_df_copy.set_index('time', inplace=True)\n",
    "    \n",
    "    # Create full date range\n",
    "    full_range = pd.date_range(\n",
    "        start=pd.to_datetime(start_time, unit='ms'),\n",
    "        end=pd.to_datetime(end_time, unit='ms'),\n",
    "        freq=interval\n",
    "    )\n",
    "    \n",
    "    # Reindex and forward fill factor data\n",
    "    # factor_df_copy = factor_df_copy.reindex(full_range)\n",
    "    # factor_df_copy = factor_df_copy.ffill()\n",
    "    \n",
    "    # For consistency, also reindex candle data\n",
    "    # candle_df_copy = candle_df_copy.reindex(full_range)\n",
    "    # candle_df_copy = candle_df_copy.ffill()\n",
    "    \n",
    "    return {\n",
    "        'candle_df': candle_df_copy,\n",
    "        'factor_df': factor_df_copy,\n",
    "        'date_range': full_range\n",
    "    }\n",
    "\n",
    "def load_and_prepare_data():\n",
    "    \"\"\"加载和准备回测数据，包括分割和预处理\"\"\"\n",
    "    print(\"Loading data...\")\n",
    "    raw_candle, raw_factor = load_all_data(c.candle_file, c.factor_file, c.factor2_file, c.factor, c.factor2)\n",
    "\n",
    "    # 计算分割点\n",
    "    annualizer = annualizer_dict.get(c.interval, 365)\n",
    "    train_split = annualizer * 3  # 训练使用3年数据\n",
    "    \n",
    "    print(f\"Using {c.interval} data, annualizer: {annualizer}, train_split points: {train_split}\")\n",
    "\n",
    "    # 按时间戳排序\n",
    "    raw_candle = raw_candle.sort_values('start_time').reset_index(drop=True)\n",
    "    raw_factor = raw_factor.sort_values('start_time').reset_index(drop=True)\n",
    "\n",
    "    # 检查数据长度\n",
    "    if len(raw_candle) <= train_split:\n",
    "        print(f\"Error: Candle data ({len(raw_candle)} points) is insufficient for the specified train_split ({train_split} points)\")\n",
    "        return None\n",
    "    \n",
    "    if len(raw_factor) <= train_split:\n",
    "        print(f\"Error: Factor data ({len(raw_factor)} points) is insufficient for the specified train_split ({train_split} points)\")\n",
    "        return None\n",
    "\n",
    "    # 分割数据\n",
    "    split_time_candle = raw_candle.iloc[train_split]['start_time']\n",
    "    print(f\"Split timestamp: {pd.to_datetime(split_time_candle, unit='ms')}\")\n",
    "    \n",
    "    candle_train = raw_candle.iloc[:train_split].copy()\n",
    "    factor_train = raw_factor.iloc[:train_split].copy()\n",
    "\n",
    "    candle_test = raw_candle.iloc[train_split:].copy()\n",
    "    factor_test = raw_factor.iloc[train_split:].copy()\n",
    "\n",
    "    candle_full = raw_candle.copy()\n",
    "    factor_full = raw_factor.copy()\n",
    "\n",
    "    # 应用因子操作和预处理\n",
    "    factor_name = c.factor\n",
    "    if c.operation != 'none':\n",
    "        print(f\"\\nApplying operation '{c.operation}' to factors...\")\n",
    "        \n",
    "        # 处理训练数据\n",
    "        combined_train, merged_col_name = combines_data(factor_train[c.factor].values, \n",
    "                                                     factor_train[c.factor2].values, \n",
    "                                                     c.operation, c.factor, c.factor2)\n",
    "        factor_train[merged_col_name] = combined_train\n",
    "        \n",
    "        # 处理测试数据\n",
    "        combined_test, _ = combines_data(factor_test[c.factor].values, \n",
    "                                       factor_test[c.factor2].values, \n",
    "                                       c.operation, c.factor, c.factor2)\n",
    "        factor_test[merged_col_name] = combined_test\n",
    "        \n",
    "        # 处理完整数据\n",
    "        combined_full, _ = combines_data(factor_full[c.factor].values, \n",
    "                                       factor_full[c.factor2].values, \n",
    "                                       c.operation, c.factor, c.factor2)\n",
    "        factor_full[merged_col_name] = combined_full\n",
    "        \n",
    "        factor_name = merged_col_name\n",
    "        print(f\"Created merged factor: {factor_name}\")\n",
    "\n",
    "    # 应用预处理\n",
    "    if c.preprocess != \"direct\":\n",
    "        print(f\"\\nApplying preprocessing method '{c.preprocess}'...\")\n",
    "        \n",
    "        # 处理训练数据\n",
    "        processed_train = data_processing(factor_train[factor_name], c.preprocess, factor_name)\n",
    "        factor_train[factor_name] = processed_train\n",
    "        \n",
    "        # 处理测试数据\n",
    "        processed_test = data_processing(factor_test[factor_name], c.preprocess, factor_name)\n",
    "        factor_test[factor_name] = processed_test\n",
    "        \n",
    "        # 处理完整数据\n",
    "        processed_full = data_processing(factor_full[factor_name], c.preprocess, factor_name)\n",
    "        factor_full[factor_name] = processed_full\n",
    "\n",
    "    # 检查NaN百分比\n",
    "    nan_train = factor_train[factor_name].isna().sum() / len(factor_train[factor_name]) if len(factor_train) > 0 else 0\n",
    "    nan_test = factor_test[factor_name].isna().sum() / len(factor_test[factor_name]) if len(factor_test) > 0 else 0\n",
    "    nan_full = factor_full[factor_name].isna().sum() / len(factor_full[factor_name]) if len(factor_full) > 0 else 0\n",
    "    print(f\"Train data NaN %: {nan_train:.3f}, Test data NaN %: {nan_test:.3f}, Test Full NaN %: {nan_full:.3f}\")\n",
    "\n",
    "    if nan_train > 0.03:\n",
    "        print(f\"Warning: Train data {factor_name} after {c.preprocess} transformation exceeds 3% NaN.\")\n",
    "    else:\n",
    "        print(f\"Dropna after transformation, Train nan%: {factor_train[factor_name].isna().sum() / len(factor_train)}\")\n",
    "        factor_train.dropna(inplace=True)\n",
    "    if nan_test > 0.03:\n",
    "        print(f\"Warning: Test data {factor_name} after {c.preprocess} transformation exceeds 3% NaN.\")\n",
    "    else:\n",
    "        print(f\"Dropna after transformation, Test nan%: {factor_train[factor_name].isna().sum() / len(factor_test)}\")\n",
    "        factor_test.dropna(inplace=True)\n",
    "    if nan_full > 0.03:\n",
    "        print(f\"Warning: Full data {factor_name} after {c.preprocess} transformation exceeds 3% NaN.\")\n",
    "    else:\n",
    "        print(f\"Dropna after transformation, Full nan%: {factor_full[factor_name].isna().sum() / len(factor_full)}\")\n",
    "        factor_full.dropna(inplace=True)\n",
    "        \n",
    "    # Preprocess data for efficient backtesting\n",
    "    train_data = prepare_backtest_data(candle_train, factor_train, factor_name, c.interval)\n",
    "    test_data = prepare_backtest_data(candle_test, factor_test, factor_name, c.interval)\n",
    "    full_data = prepare_backtest_data(candle_full, factor_full, factor_name, c.interval)\n",
    "        \n",
    "    return {\n",
    "        'train': {'candle': train_data['candle_df'], 'factor': train_data['factor_df'], 'date_range': train_data['date_range']},\n",
    "        'test': {'candle': test_data['candle_df'], 'factor': test_data['factor_df'], 'date_range': test_data['date_range']},\n",
    "        'full': {'candle': full_data['candle_df'], 'factor': full_data['factor_df'], 'date_range': full_data['date_range']},\n",
    "        'factor_name': factor_name\n",
    "    }\n",
    "\n",
    "def parse_manual_selection(filepath):\n",
    "    \"\"\"\n",
    "    解析manual_selected.txt文件，返回参数元组列表[(model, entry, window, threshold)]\n",
    "    使用config.py中的ALL_MODELS列表来识别模型和条目\n",
    "    如果文件格式不完整或不存在，返回空列表\n",
    "    \"\"\"\n",
    "    if not os.path.exists(filepath):\n",
    "        print(f\"Warning: {filepath} not found\")\n",
    "        return []\n",
    "        \n",
    "    try:        \n",
    "        with open(filepath, \"r\") as file:\n",
    "            lines = [line.strip() for line in file.readlines() if line.strip()]\n",
    "        \n",
    "        params_list = []\n",
    "        current_model = None\n",
    "        \n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "                \n",
    "            # 检查当前行是否是模型名称\n",
    "            if line in c.ALL_MODELS:\n",
    "                current_model = line\n",
    "                continue\n",
    "            \n",
    "            # 如果没有当前模型，则跳过\n",
    "            if current_model is None:\n",
    "                print(f\"Warning: Entry '{line}' found without a model specified\")\n",
    "                continue\n",
    "                \n",
    "            # 解析条目和参数\n",
    "            parts = line.split(maxsplit=1)\n",
    "            if len(parts) < 1:\n",
    "                continue\n",
    "                \n",
    "            entry = parts[0]\n",
    "            \n",
    "            if len(parts) > 1:\n",
    "                params_str = parts[1]\n",
    "                params_entries = params_str.split(\",\")\n",
    "                for param_entry in params_entries:\n",
    "                    param_entry = param_entry.strip()\n",
    "                    if param_entry:\n",
    "                        try:\n",
    "                            window_str, threshold_str = param_entry.split(\"/\")\n",
    "                            window = float(window_str) if \".\" in window_str else int(window_str)\n",
    "                            threshold = float(threshold_str)\n",
    "                            params_list.append((current_model, entry, window, threshold))\n",
    "                        except ValueError:\n",
    "                            print(f\"Warning: Could not parse parameter {param_entry}\")\n",
    "            else:\n",
    "                # 没有参数，使用默认值\n",
    "                params_list.append((current_model, entry, c.window, c.threshold))\n",
    "    \n",
    "        return params_list\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing manual_selected.txt: {e}\")\n",
    "        return []\n",
    "\n",
    "def backtest_for_pnl(candle_df, factor_df, factor, factor2, interval, operation, preprocess, model,\n",
    "                  entry, window, threshold, backtest_style, date_range=None):\n",
    "    \"\"\"执行回测并绘制结果图表\"\"\"\n",
    "    print(f\"\\n=== Running {backtest_style} ===\")\n",
    "    print(f\"Using model: {model}, entry: {entry}, window: {window}, threshold: {threshold}\")\n",
    "    \n",
    "    # 检查空数据框\n",
    "    if candle_df.empty or factor_df.empty:\n",
    "        print(f\"Error: Empty dataframe in {backtest_style}\")\n",
    "        return None\n",
    "        \n",
    "    # 检查数据长度是否足够\n",
    "    if len(candle_df) < window or len(factor_df) < window:\n",
    "        print(f\"Error: Insufficient data points (candle: {len(candle_df)}, factor: {len(factor_df)}) compared to window size ({window}) in {backtest_style}\")\n",
    "        return None\n",
    "    \n",
    "    candle_df['time'] = pd.to_datetime(candle_df['start_time'], unit='ms')\n",
    "\n",
    "    # 获取annualizer值\n",
    "    annualizer = annualizer_dict.get(interval, 365)\n",
    "    \n",
    "    # 添加额外的指标\n",
    "    additional_metric = additional_metrics(c.alpha_id, c.symbol, factor, factor2,\n",
    "                                          operation, c.candle_delay, backtest_style)\n",
    "    \n",
    "    try:\n",
    "        # 运行回测 - now passing date_range\n",
    "        backtest_result, df, log_msgs = backtest_cached(\n",
    "            candle_df, factor_df, window, threshold, preprocess, \n",
    "            entry, annualizer, model, factor, interval, date_range\n",
    "        )\n",
    "        \n",
    "        # 打印日志消息\n",
    "        for msg in log_msgs:\n",
    "            print(msg)\n",
    "            \n",
    "        # 检查df是否为空\n",
    "        if df is None or df.empty:\n",
    "            print(f\"Warning: No results returned from backtest_cached for {backtest_style}\")\n",
    "            return None\n",
    "        \n",
    "        # # 正确处理时间戳\n",
    "        if isinstance(df.index, pd.DatetimeIndex):\n",
    "            # 直接使用DatetimeIndex\n",
    "            start_date = df.index.min().strftime('%Y-%m-%d')\n",
    "            end_date = df.index.max().strftime('%Y-%m-%d')\n",
    "            start_time = df.index.min().strftime('%Y-%m-%d %H:%M:%S')\n",
    "            end_time = df.index.max().strftime('%Y-%m-%d %H:%M:%S')\n",
    "        elif 'start_time' in df.columns:\n",
    "            # 使用start_time列\n",
    "            start_date = pd.to_datetime(df['start_time'].min(), unit='ms').strftime('%Y-%m-%d')\n",
    "            end_date = pd.to_datetime(df['start_time'].max(), unit='ms').strftime('%Y-%m-%d')\n",
    "            start_time = pd.to_datetime(df['start_time'].min(), unit='ms').strftime('%Y-%m-%d %H:%M:%S')\n",
    "            end_time = pd.to_datetime(df['start_time'].max(), unit='ms').strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "        additional_metric.update({\"start_time\": start_time, \"end_time\": end_time})\n",
    "        combined_report = [{**additional_metric, **backtest_result}]\n",
    "\n",
    "        print(f\"{backtest_style} Report:\")\n",
    "        print(json.dumps(combined_report, indent=4))\n",
    "\n",
    "        # 绘制结果\n",
    "        fig, ax1 = plt.subplots(figsize=(15, 8))\n",
    "        ax1.plot(df.index, df['close'], label='Close Price', color='green', linewidth=2)\n",
    "        ax1.set_xlabel(\"Date\", fontsize=12)\n",
    "        ax1.set_ylabel(\"Close Price\", fontsize=12, color='green')\n",
    "        ax1.tick_params(axis='y', labelcolor='green')\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 在右侧y轴上绘制累积PnL\n",
    "        ax2 = ax1.twinx()\n",
    "        ax2.plot(df.index, df['cumu_pnl'], label='Cumulative PnL', color='blue', linewidth=2)\n",
    "        ax2.set_ylabel(\"Cumulative PnL\", fontsize=12, color='blue')\n",
    "        ax2.tick_params(axis='y', labelcolor='blue')\n",
    "\n",
    "        plt.title(f\"Close Price and Cumulative PnL Plot (Split {backtest_style})-({start_date} ~ {end_date})\", fontsize=16)\n",
    "        fig.tight_layout()\n",
    "        # plt.show()\n",
    "\n",
    "        if c.save_plot:\n",
    "            plt.savefig(f\"{backtest_style}_Equity_Curve_{start_date}_{end_date}.png\", dpi=300, bbox_inches='tight')\n",
    "            print(f\"已儲存 {backtest_style}_Equity_Curve_{start_date}_{end_date}.png\")\n",
    "            output_backtest_data = {f\"{backtest_style}\": combined_report}\n",
    "            with open(f\"{c.alpha_id}_{backtest_style}.json\", \"w\") as json_file:\n",
    "                json.dump(output_backtest_data, json_file, indent=4)\n",
    "            df.to_csv(f\"{c.alpha_id}_{backtest_style}_df.csv\", index=True)\n",
    "            print(f\"已儲存 {c.alpha_id}_{backtest_style}_df.csv\")\n",
    "        plt.show()\n",
    "        return combined_report\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error in backtest_for_pnl for {backtest_style}: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "def run_backtest(data, model, entry, window, threshold):\n",
    "    \"\"\"运行完整的回测流程（前向测试、回测、全时段回测）\"\"\"\n",
    "    factor_name = data['factor_name']\n",
    "    \n",
    "    # 设置SR阈值\n",
    "    if \"short\" in entry.lower():\n",
    "        required_sr = 1.0\n",
    "    elif \"long\" in entry.lower():\n",
    "        required_sr = 1.7\n",
    "    else:\n",
    "        required_sr = 1.7\n",
    "    \n",
    "    print(f\"\\n执行测试: model = {model}, entry = {entry}, window = {window}, threshold = {threshold}\")\n",
    "    \n",
    "    # 执行forward test\n",
    "    fwd_report = backtest_for_pnl(\n",
    "        data['test']['candle'],\n",
    "        data['test']['factor'],\n",
    "        factor_name,\n",
    "        c.factor2,\n",
    "        c.interval,\n",
    "        c.operation,\n",
    "        c.preprocess,\n",
    "        model,\n",
    "        entry,\n",
    "        window,\n",
    "        threshold,\n",
    "        \"forwardtest\",\n",
    "        data['test']['date_range']\n",
    "    )\n",
    "    \n",
    "    # 检查forward test结果\n",
    "    if fwd_report is None:\n",
    "        print(\"Warning: forward test failed, skipping this parameter set.\")\n",
    "        return\n",
    "        \n",
    "    # 获取SR值\n",
    "    fwd_sr = fwd_report[0].get(\"SR\", 0)\n",
    "    \n",
    "    # 如果SR未达到要求则跳过后续测试\n",
    "    if fwd_sr < required_sr:\n",
    "        print(f\"Forward test SR = {fwd_sr} 未达到要求 (需 > {required_sr})，跳过此组参数的后续测试。\")\n",
    "        return\n",
    "        \n",
    "    # 如果符合SR要求则继续执行其他backtest\n",
    "    backtest_for_pnl(\n",
    "        data['train']['candle'],\n",
    "        data['train']['factor'],\n",
    "        factor_name,\n",
    "        c.factor2,\n",
    "        c.interval,\n",
    "        c.operation,\n",
    "        c.preprocess,\n",
    "        model,\n",
    "        entry,\n",
    "        window,\n",
    "        threshold,\n",
    "        \"backtest\",\n",
    "        data['train']['date_range']\n",
    "    )\n",
    "    \n",
    "    backtest_for_pnl(\n",
    "        data['full']['candle'],\n",
    "        data['full']['factor'],\n",
    "        factor_name,\n",
    "        c.factor2,\n",
    "        c.interval,\n",
    "        c.operation,\n",
    "        c.preprocess,\n",
    "        model,\n",
    "        entry,\n",
    "        window,\n",
    "        threshold,\n",
    "        \"full_time_backtest\",\n",
    "        data['full']['date_range']\n",
    "    )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 一次性加载和处理数据\n",
    "    data = load_and_prepare_data()\n",
    "    if data is None:\n",
    "        print(\"Error preparing data. Aborting backtest.\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    # 尝试从manual_selected.txt读取参数列表\n",
    "    params_list = parse_manual_selection(\"manual_selected.txt\")\n",
    "    \n",
    "    if params_list:\n",
    "        print(f\"Found {len(params_list)} parameter sets in manual_selected.txt\")\n",
    "        # 使用每组参数运行回测\n",
    "        for model, entry, window, threshold in params_list:\n",
    "            run_backtest(data, model, entry, window, threshold)\n",
    "    else:\n",
    "        # 如果文件不存在或参数不齐全，使用config中的参数\n",
    "        print(\"Using parameters from config.py\")\n",
    "        run_backtest(data, c.model, c.entry, c.window, c.threshold)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cybotrade",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
