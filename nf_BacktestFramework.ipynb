{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ¯æ¬¡è¿è¡Œæ—¶é‡æ–°åŠ è½½config.py\n",
    "from importlib import reload\n",
    "import config as c\n",
    "reload(c)\n",
    "\n",
    "print(f\"å·²åŠ è½½é…ç½®ï¼šfactor={c.factor}, factor2={c.factor2}, operation={c.operation}, preprocess={c.preprocess}, USE_ALL_MODELS={c.USE_ALL_MODELS}\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resampling 1min data to wanted timeframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import config as c\n",
    "\n",
    "def prepare_price_data(\n",
    "    csv_path: str,  # è¼¸å…¥ CSV æª”æ¡ˆå®Œæ•´è·¯å¾‘\n",
    "    datasource: str = 'bybit_btcusdt',\n",
    "    factor: str = 'price',\n",
    "    timeframe: str = '1D', \n",
    "    delay_minutes: int = 0\n",
    "):\n",
    "    \"\"\"\n",
    "    è®€å– 1m è³‡æ–™ï¼Œè½‰æˆæŒ‡å®šçš„æ™‚é–“é€±æœŸ (timeframe)ï¼Œ\n",
    "    å¯é¸æ“‡å»¶é²(æ­£å€¼)æˆ–æå‰(è² å€¼)æ™‚é–“ç´¢å¼•ï¼Œä¸¦è‡ªå‹•å­˜æª”åˆ°ç•¶å‰å·¥ä½œç›®éŒ„ã€‚\n",
    "\n",
    "    åƒæ•¸ï¼š\n",
    "      csv_path      : ã€å®Œæ•´è·¯å¾‘ã€‘è¼¸å…¥çš„ 1m ç´šåˆ¥ CSV æª”æ¡ˆ\n",
    "      datasource    : è³‡æ–™ä¾†æºåç¨± (å¦‚ bybit_btcusdt)\n",
    "      factor        : å½±éŸ¿å› å­åç¨± (å¦‚ price)\n",
    "      timeframe     : è½‰æ›å¾Œçš„æ™‚é–“é€±æœŸï¼Œå¦‚ '1H'ã€'1D' ç­‰ (é è¨­ '1D')\n",
    "      delay_minutes : æ™‚é–“å¹³ç§»çš„åˆ†é˜æ•¸ (æ­£å€¼ = å»¶å¾Œï¼›è² å€¼ = æå‰)\n",
    "\n",
    "    å›å‚³ï¼š\n",
    "      pandas DataFrame (resampled å¾Œçš„çµæœ)ï¼Œ\n",
    "      ä¸¦å°‡çµæœè¼¸å‡ºç‚º CSVï¼Œå‘½åæ ¼å¼ï¼š\n",
    "      {datasource}_{factor}_{timeframe}_{start_time}_{end_time}.csv\n",
    "    \"\"\"\n",
    "    # 1. è®€å– CSVï¼Œè§£ææ™‚é–“\n",
    "    df = pd.read_csv(\n",
    "        csv_path, \n",
    "        parse_dates=['Time']  # pandas æœƒè‡ªå‹•è§£ææ™‚é–“æ ¼å¼\n",
    "    )\n",
    "\n",
    "    # 2. å°‡ 'Time' æ¬„è¨­ç‚ºç´¢å¼•\n",
    "    df.set_index('Time', inplace=True)\n",
    "\n",
    "    # 3. æ™‚é–“å¹³ç§» (å»¶é² / æå‰)\n",
    "    if delay_minutes != 0:\n",
    "        df.index = df.index + pd.Timedelta(minutes=delay_minutes)\n",
    "\n",
    "    # 4. å®šç¾© resample èšåˆæ–¹å¼\n",
    "    if c.candle_exchange == 'bybit':\n",
    "        ohlc_dict = {\n",
    "            'Open': 'first',\n",
    "            'High': 'max',\n",
    "            'Low': 'min',\n",
    "            'Close': 'last',\n",
    "            'Volume': 'sum',\n",
    "            'Turnover': 'sum'\n",
    "        }\n",
    "    else:\n",
    "        ohlc_dict = {\n",
    "        'Open': 'first',\n",
    "        'High': 'max',\n",
    "        'Low': 'min',\n",
    "        'Close': 'last',\n",
    "        'Volume': 'sum',\n",
    "    }\n",
    "    \n",
    "    # 5. é€²è¡Œ resample\n",
    "    df_resampled = df.resample(timeframe).agg(ohlc_dict).dropna(how='any')\n",
    "\n",
    "    # Use Time to create one more column named 'start_time' that is in unix timestamp\n",
    "    df_resampled['start_time'] = df_resampled.index.astype('int64') // 10**6\n",
    "    # df_resampled['start_time'] = df_resampled['start_time'].astype('float64')\n",
    "\n",
    "    # 6. ç²å–é–‹å§‹èˆ‡çµæŸæ™‚é–“ (æ ¼å¼ YYYY-MM-DD)\n",
    "    if not df_resampled.empty:\n",
    "        start_time = df_resampled.index[0].strftime('%Y-%m-%d')\n",
    "        end_time = df_resampled.index[-1].strftime('%Y-%m-%d')\n",
    "\n",
    "        # 7. æ§‹å»ºè¼¸å‡ºæª”æ¡ˆåç¨±\n",
    "        output_filename = f\"./data/resample_{datasource}_{timeframe}_-{c.shift_candle_minite}m.csv\"\n",
    "        output_path = os.path.join(os.getcwd(), output_filename)  # ç•¶å‰å·¥ä½œç›®éŒ„\n",
    "\n",
    "        # 8. è¼¸å‡º CSV\n",
    "        df_resampled.to_csv(output_path)\n",
    "        print(f\"âœ… æª”æ¡ˆå·²å„²å­˜ï¼š{output_path}\")\n",
    "    else:\n",
    "        print(\"âš ï¸ Resampled DataFrame ç‚ºç©ºï¼Œæœªç”¢ç”Ÿè¼¸å‡ºæª”æ¡ˆï¼\")\n",
    "\n",
    "    return df_resampled\n",
    "\n",
    "df_r = prepare_price_data(\n",
    "    csv_path=f\"./data/{c.candle_exchange}_{c.symbol.lower()}usdt_price_1m.csv\",\n",
    "    datasource=f'{c.candle_exchange}_{c.symbol.lower()}',\n",
    "    factor='price',\n",
    "    timeframe=c.interval,\n",
    "    delay_minutes=-c.shift_candle_minite\n",
    ")\n",
    "\n",
    "print(df_r.head()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilsnumpy import load_all_data, combines_data, data_processing\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import config as c\n",
    "from datetime import datetime\n",
    "\n",
    "def visualize_factors():\n",
    "    \"\"\"\n",
    "    åŠ è¼‰ä¸¦è¦–è¦ºåŒ–å› å­æ•¸æ“šï¼Œæ”¯æŒå› å­çµ„åˆå’Œé è™•ç†\n",
    "    \n",
    "    Returns:\n",
    "        dict: åŒ…å«è Ÿç‡­åœ–æ•¸æ“šã€å› å­æ•¸æ“šå’Œå› å­åç¨±çš„å­—å…¸\n",
    "    \"\"\"\n",
    "    print(\"Loading data...\")\n",
    "    # åŠ è¼‰åŸå§‹æ•¸æ“š\n",
    "    raw_candle, raw_factor = load_all_data(c.candle_file, c.factor_file, c.factor2_file, c.factor, c.factor2)\n",
    "    \n",
    "    # è™•ç†æ™‚é–“æ ¼å¼ä»¥ä¾¿æ–¼ç¹ªåœ–\n",
    "    raw_factor['time'] = pd.to_datetime(raw_factor['start_time'], unit='ms')\n",
    "    raw_candle['time'] = pd.to_datetime(raw_candle['start_time'], unit='ms')\n",
    "    \n",
    "    # ç²å–æ™‚é–“ç¯„åœä¿¡æ¯ç”¨æ–¼æ¨™é¡Œ\n",
    "    start_date = raw_candle['time'].min().strftime('%Y-%m-%d')\n",
    "    end_date = raw_candle['time'].max().strftime('%Y-%m-%d')\n",
    "    date_range = f\"{start_date} to {end_date}\"\n",
    "    \n",
    "    # åˆå§‹åŒ–ç¹ªåœ–\n",
    "    fig, axes = plt.subplots(3, 1, figsize=(15, 18), sharex=True, dpi=100)\n",
    "    \n",
    "    # ç¹ªè£½åƒ¹æ ¼æ•¸æ“š\n",
    "    axes[0].plot(raw_candle['time'], raw_candle['Close'], color='#1f77b4', linewidth=1.5)\n",
    "    axes[0].set_title(f'Price Data: {c.symbol.upper()}', fontsize=16, fontweight='bold')\n",
    "    axes[0].set_ylabel('Price (USD)', fontsize=14)\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # æ·»åŠ åƒ¹æ ¼è®ŠåŒ–ç™¾åˆ†æ¯”\n",
    "    price_change = (raw_candle['Close'].iloc[-1] - raw_candle['Close'].iloc[0]) / raw_candle['Close'].iloc[0] * 100\n",
    "    price_text = f\"Price Change: {price_change:.2f}%\"\n",
    "    axes[0].text(0.99, 0.05, price_text, transform=axes[0].transAxes, \n",
    "                ha='right', fontsize=12, \n",
    "                bbox=dict(boxstyle='round,pad=0.5', facecolor='white', alpha=0.8))\n",
    "    \n",
    "    # ç¹ªè£½åŸå§‹å› å­æ•¸æ“š\n",
    "    axes[1].plot(raw_factor['time'], raw_factor[c.factor], color='#2ca02c', linewidth=1.5, label=c.factor)\n",
    "    if c.factor2 in raw_factor.columns:\n",
    "        axes[1].plot(raw_factor['time'], raw_factor[c.factor2], color='#d62728', linewidth=1.5, label=c.factor2)\n",
    "    axes[1].set_title('Raw Factor Data', fontsize=16, fontweight='bold')\n",
    "    axes[1].set_ylabel('Factor Value', fontsize=14)\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    axes[1].legend(loc='best', frameon=True, framealpha=0.8)\n",
    "    \n",
    "    # è™•ç†çµ„åˆå› å­å’Œé è™•ç†\n",
    "    factor_data = raw_factor.copy()\n",
    "    factor_to_display = c.factor\n",
    "    factor_label = c.factor\n",
    "    \n",
    "    # å¦‚æœéœ€è¦çµ„åˆå› å­\n",
    "    if c.operation != 'none' and c.factor2 in factor_data.columns:\n",
    "        print(f\"Combining factors with operation: {c.operation}\")\n",
    "        factor_data, combined_name = combines_data(\n",
    "            factor_data,\n",
    "            c.factor,\n",
    "            c.factor2,\n",
    "            c.operation\n",
    "        )\n",
    "        factor_to_display = combined_name\n",
    "        factor_label = f\"{c.factor} {c.operation} {c.factor2}\"\n",
    "    \n",
    "    # å¦‚æœéœ€è¦é è™•ç†\n",
    "    if c.preprocess != \"direct\":\n",
    "        print(f\"Applying preprocessing: {c.preprocess}\")\n",
    "        # æ³¨æ„ï¼šå¾paste.txtçš„ä»£ç¢¼ä¾†çœ‹ï¼Œé€™è£¡å¯èƒ½æœ‰ä¸€å€‹å•é¡Œ\n",
    "        # data_processingåœ¨å…¶ä»–éƒ¨åˆ†å¯èƒ½è¿”å›å…©å€‹å€¼ï¼Œä½†é€™è£¡åªæ•ç²äº†ä¸€å€‹\n",
    "        try:\n",
    "            # å˜—è©¦æ–°ç‰ˆAPIï¼ˆè¿”å›DataFrameå’Œåç¨±ï¼‰\n",
    "            processed_df, new_factor_name = data_processing(factor_data, c.preprocess, factor_to_display)\n",
    "            factor_data = processed_df\n",
    "            factor_to_display = new_factor_name\n",
    "        except ValueError:\n",
    "            # èˆŠç‰ˆAPIï¼ˆç›´æ¥è¿”å›è™•ç†å¾Œçš„å€¼ï¼‰\n",
    "            processed_values = data_processing(factor_data[factor_to_display], c.preprocess, factor_to_display)\n",
    "            factor_data[f\"{factor_to_display}_{c.preprocess}\"] = processed_values\n",
    "            factor_to_display = f\"{factor_to_display}_{c.preprocess}\"\n",
    "        \n",
    "        factor_label = f\"{factor_label} ({c.preprocess})\"\n",
    "    \n",
    "    # ç¹ªè£½è™•ç†å¾Œçš„å› å­æ•¸æ“š\n",
    "    axes[2].plot(factor_data['time'], factor_data[factor_to_display], color='#9467bd', linewidth=1.5)\n",
    "    axes[2].set_title('Processed Factor Data', fontsize=16, fontweight='bold')\n",
    "    axes[2].set_ylabel('Factor Value', fontsize=14)\n",
    "    axes[2].set_xlabel('Date', fontsize=14)\n",
    "    axes[2].grid(True, alpha=0.3)\n",
    "    \n",
    "    # åœ¨æ¨™é¡Œæ—æ·»åŠ å› å­æè¿°\n",
    "    axes[2].text(0.01, 0.95, f\"Factor: {factor_label}\", transform=axes[2].transAxes, \n",
    "                 fontsize=12, verticalalignment='top', \n",
    "                 bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "    \n",
    "    # æ·»åŠ æ•¸æ“šç¯„åœ\n",
    "    factor_data_description = f\"\"\"\n",
    "    Data points: {len(factor_data)}\n",
    "    Date range: {date_range}\n",
    "    \"\"\"\n",
    "    axes[2].text(0.99, 0.05, factor_data_description, transform=axes[2].transAxes,\n",
    "                ha='right', fontsize=10, va='bottom',\n",
    "                bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "    \n",
    "    # æ ¼å¼åŒ–Xè»¸æ—¥æœŸ\n",
    "    for ax in axes:\n",
    "        ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n",
    "        ax.xaxis.set_major_locator(mdates.MonthLocator(interval=3))\n",
    "        plt.setp(ax.xaxis.get_majorticklabels(), rotation=45)\n",
    "    \n",
    "    # æ·»åŠ æ•´é«”æ¨™é¡Œ\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M\")\n",
    "    plt.suptitle(f'Factor Analysis - {c.symbol.upper()} ({timestamp})', fontsize=20, y=0.98)\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.97])\n",
    "    \n",
    "    # ä¿å­˜åœ–åƒ\n",
    "    if hasattr(c, 'save_plot') and c.save_plot:\n",
    "        filename = f\"factor_analysis_{c.symbol}_{c.factor}_{datetime.now().strftime('%Y%m%d')}.png\"\n",
    "        plt.savefig(filename, dpi=300, bbox_inches='tight')\n",
    "        print(f\"åœ–åƒå·²ä¿å­˜ç‚º: {filename}\")\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    # è¿”å›è™•ç†å¾Œçš„æ•¸æ“šï¼Œä»¥ä¾¿é€²ä¸€æ­¥åˆ†æ\n",
    "    return {\n",
    "        'candle': raw_candle,\n",
    "        'factor': factor_data,\n",
    "        'factor_name': factor_to_display\n",
    "    }\n",
    "\n",
    "def visualize_factor_correlation(data, n_bins=20):\n",
    "    \"\"\"\n",
    "    è¦–è¦ºåŒ–å› å­èˆ‡åƒ¹æ ¼çš„ç›¸é—œæ€§\n",
    "    \n",
    "    Args:\n",
    "        data: ç”±visualize_factorså‡½æ•¸è¿”å›çš„æ•¸æ“šå­—å…¸\n",
    "        n_bins: æ•£é»åœ–é¡è‰²åˆ†çµ„æ•¸é‡ï¼Œç”¨æ–¼å¯†åº¦é¡¯ç¤º\n",
    "    \"\"\"\n",
    "    if data is None:\n",
    "        print(\"æ²’æœ‰æ•¸æ“šå¯ç”¨æ–¼ç›¸é—œæ€§åˆ†æ\")\n",
    "        return\n",
    "    \n",
    "    candle_df = data['candle']\n",
    "    factor_df = data['factor']\n",
    "    factor_name = data['factor_name']\n",
    "    \n",
    "    # åˆä½µåƒ¹æ ¼å’Œå› å­æ•¸æ“š\n",
    "    merged_df = pd.merge_asof(\n",
    "        candle_df.sort_values('time'), \n",
    "        factor_df[['time', factor_name]].sort_values('time'),\n",
    "        on='time',\n",
    "        direction='nearest'\n",
    "    )\n",
    "    \n",
    "    # è¨ˆç®—ä¸åŒçš„åƒ¹æ ¼è®ŠåŒ–ç‡\n",
    "    merged_df['price_change_1d'] = merged_df['Close'].pct_change()\n",
    "    \n",
    "    # å¦‚æœæ™‚é–“é–“éš”å…è¨±ï¼Œä¹Ÿè¨ˆç®—5æ—¥å’Œ10æ—¥è®ŠåŒ–ç‡\n",
    "    if len(merged_df) > 10:\n",
    "        merged_df['price_change_5d'] = merged_df['Close'].pct_change(5)\n",
    "        merged_df['price_change_10d'] = merged_df['Close'].pct_change(10)\n",
    "    \n",
    "    # å»é™¤NaNå€¼\n",
    "    merged_df = merged_df.dropna()\n",
    "    \n",
    "    # å¤šå€‹æ™‚é–“æ¡†æ¶çš„ç›¸é—œä¿‚æ•¸\n",
    "    correlation_1d = merged_df['price_change_1d'].corr(merged_df[factor_name])\n",
    "    corr_results = [f\"1-day: {correlation_1d:.4f}\"]\n",
    "    \n",
    "    if 'price_change_5d' in merged_df.columns:\n",
    "        correlation_5d = merged_df['price_change_5d'].corr(merged_df[factor_name])\n",
    "        corr_results.append(f\"5-day: {correlation_5d:.4f}\")\n",
    "    \n",
    "    if 'price_change_10d' in merged_df.columns:\n",
    "        correlation_10d = merged_df['price_change_10d'].corr(merged_df[factor_name])\n",
    "        corr_results.append(f\"10-day: {correlation_10d:.4f}\")\n",
    "    \n",
    "    # å‰µå»ºä¸€å€‹2x2ç¶²æ ¼åœ–è¡¨ç”¨æ–¼å¤šå€‹ç›¸é—œæ€§è¦–åœ–\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "    plt.subplots_adjust(hspace=0.3, wspace=0.3)\n",
    "    \n",
    "    # åŸºæœ¬æ•£é»åœ–ï¼ˆå·¦ä¸Šï¼‰\n",
    "    axes[0, 0].scatter(merged_df[factor_name], merged_df['price_change_1d'], \n",
    "                      alpha=0.6, c='#1f77b4', edgecolors='none')\n",
    "    axes[0, 0].axhline(y=0, color='r', linestyle='-', alpha=0.3)\n",
    "    axes[0, 0].axvline(x=0, color='r', linestyle='-', alpha=0.3)\n",
    "    \n",
    "    # æ·»åŠ è¶¨å‹¢ç·š\n",
    "    z = np.polyfit(merged_df[factor_name], merged_df['price_change_1d'], 1)\n",
    "    p = np.poly1d(z)\n",
    "    x_range = np.linspace(merged_df[factor_name].min(), merged_df[factor_name].max(), 100)\n",
    "    axes[0, 0].plot(x_range, p(x_range), \"r--\", alpha=0.8)\n",
    "    \n",
    "    axes[0, 0].set_title(f'Price Change vs {factor_name}\\nCorrelation: {correlation_1d:.4f}', fontsize=14)\n",
    "    axes[0, 0].set_xlabel(factor_name, fontsize=12)\n",
    "    axes[0, 0].set_ylabel('1-Day Price Change (%)', fontsize=12)\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # æ ¸å¯†åº¦æ•£é»åœ–ï¼ˆå³ä¸Šï¼‰- ä½¿ç”¨é¡è‰²è¡¨ç¤ºé»çš„å¯†åº¦\n",
    "    from scipy.stats import gaussian_kde\n",
    "    \n",
    "    # è¨ˆç®—2Då¯†åº¦\n",
    "    xy = np.vstack([merged_df[factor_name], merged_df['price_change_1d']])\n",
    "    try:\n",
    "        z = gaussian_kde(xy)(xy)\n",
    "        # æ ¹æ“šå¯†åº¦æ’åºé»ï¼Œé€™æ¨£å¯†åº¦é«˜çš„é»æœƒå‡ºç¾åœ¨é ‚éƒ¨\n",
    "        idx = z.argsort()\n",
    "        x, y, z = merged_df[factor_name].iloc[idx], merged_df['price_change_1d'].iloc[idx], z[idx]\n",
    "        scatter = axes[0, 1].scatter(x, y, c=z, cmap='viridis', \n",
    "                                   edgecolor='none', alpha=0.8, s=30)\n",
    "        plt.colorbar(scatter, ax=axes[0, 1], label='Density')\n",
    "    except Exception as e:\n",
    "        print(f\"ç„¡æ³•å‰µå»ºå¯†åº¦åœ–: {e}\")\n",
    "        axes[0, 1].scatter(merged_df[factor_name], merged_df['price_change_1d'], \n",
    "                          alpha=0.6, c='#2ca02c', edgecolors='none')\n",
    "    \n",
    "    axes[0, 1].axhline(y=0, color='r', linestyle='-', alpha=0.3)\n",
    "    axes[0, 1].axvline(x=0, color='r', linestyle='-', alpha=0.3)\n",
    "    axes[0, 1].set_title('Density Plot', fontsize=14)\n",
    "    axes[0, 1].set_xlabel(factor_name, fontsize=12)\n",
    "    axes[0, 1].set_ylabel('1-Day Price Change (%)', fontsize=12)\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # äºŒç¶­ç›´æ–¹åœ–ï¼ˆå·¦ä¸‹ï¼‰\n",
    "    h = axes[1, 0].hist2d(merged_df[factor_name], merged_df['price_change_1d'], \n",
    "                         bins=n_bins, cmap='Blues')\n",
    "    plt.colorbar(h[3], ax=axes[1, 0], label='Count')\n",
    "    axes[1, 0].set_title('2D Histogram', fontsize=14)\n",
    "    axes[1, 0].set_xlabel(factor_name, fontsize=12)\n",
    "    axes[1, 0].set_ylabel('1-Day Price Change (%)', fontsize=12)\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # å¤šæ™‚é–“æ¡†æ¶ç›¸é—œæ€§ï¼ˆå³ä¸‹ï¼‰\n",
    "    if 'price_change_5d' in merged_df.columns and 'price_change_10d' in merged_df.columns:\n",
    "        correlations = [\n",
    "            correlation_1d,\n",
    "            correlation_5d,\n",
    "            correlation_10d\n",
    "        ]\n",
    "        labels = ['1-Day', '5-Day', '10-Day']\n",
    "        colors = ['#1f77b4', '#ff7f0e', '#2ca02c']\n",
    "        \n",
    "        axes[1, 1].bar(labels, correlations, color=colors, alpha=0.7)\n",
    "        axes[1, 1].axhline(y=0, color='r', linestyle='-', alpha=0.3)\n",
    "        axes[1, 1].set_title('Correlation Across Timeframes', fontsize=14)\n",
    "        axes[1, 1].set_ylabel('Correlation Coefficient', fontsize=12)\n",
    "        axes[1, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # æ·»åŠ ç›¸é—œæ€§å€¼ä½œç‚ºæ¨™ç±¤\n",
    "        for i, v in enumerate(correlations):\n",
    "            axes[1, 1].text(i, v + 0.02 if v >= 0 else v - 0.08,\n",
    "                          f\"{v:.4f}\", ha='center', fontsize=10)\n",
    "    else:\n",
    "        # å¦‚æœæ•¸æ“šä¸è¶³ï¼Œåªé¡¯ç¤ºå› å­åˆ†ä½ˆ\n",
    "        axes[1, 1].hist(merged_df[factor_name], bins=30, alpha=0.7, color='#1f77b4')\n",
    "        axes[1, 1].set_title(f'Distribution of {factor_name}', fontsize=14)\n",
    "        axes[1, 1].set_xlabel(factor_name, fontsize=12)\n",
    "        axes[1, 1].set_ylabel('Frequency', fontsize=12)\n",
    "        axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # æ·»åŠ è¶…ç´šæ¨™é¡Œ\n",
    "    plt.suptitle(f'Correlation Analysis: {c.symbol.upper()} vs {factor_name}', \n",
    "                fontsize=16, y=0.98, fontweight='bold')\n",
    "    \n",
    "    # ä¿å­˜åœ–åƒ\n",
    "    if hasattr(c, 'save_plot') and c.save_plot:\n",
    "        filename = f\"correlation_{c.symbol}_{factor_name}_{datetime.now().strftime('%Y%m%d')}.png\"\n",
    "        plt.savefig(filename, dpi=300, bbox_inches='tight')\n",
    "        print(f\"åœ–åƒå·²ä¿å­˜ç‚º: {filename}\")\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    # è¿”å›ç›¸é—œä¿‚æ•¸çµæœ\n",
    "    return {\n",
    "        'correlations': corr_results,\n",
    "        'merged_data': merged_df\n",
    "    }\n",
    "\n",
    "def visualize_factor_distribution(data):\n",
    "    \"\"\"\n",
    "    è¦–è¦ºåŒ–å› å­å€¼çš„åˆ†ä½ˆ\n",
    "    \n",
    "    Args:\n",
    "        data: ç”±visualize_factorså‡½æ•¸è¿”å›çš„æ•¸æ“šå­—å…¸\n",
    "    \"\"\"\n",
    "    if data is None:\n",
    "        print(\"æ²’æœ‰æ•¸æ“šå¯ç”¨æ–¼åˆ†ä½ˆåˆ†æ\")\n",
    "        return\n",
    "    \n",
    "    factor_df = data['factor']\n",
    "    factor_name = data['factor_name']\n",
    "    \n",
    "    # å»é™¤NaNå€¼\n",
    "    factor_values = factor_df[factor_name].dropna().values\n",
    "    \n",
    "    # å‰µå»º2x2ç¶²æ ¼åœ–è¡¨ç”¨æ–¼å¤šç¨®åˆ†ä½ˆè¦–åœ–\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "    plt.subplots_adjust(hspace=0.3, wspace=0.3)\n",
    "    \n",
    "    # 1. åŸºæœ¬ç›´æ–¹åœ–ï¼ˆå·¦ä¸Šï¼‰\n",
    "    n, bins, patches = axes[0, 0].hist(factor_values, bins=50, alpha=0.7, color='#1f77b4', \n",
    "                                      edgecolor='black', linewidth=0.5)\n",
    "    \n",
    "    # æ·»åŠ åŸºæœ¬çµ±è¨ˆä¿¡æ¯\n",
    "    mean_val = np.mean(factor_values)\n",
    "    median_val = np.median(factor_values)\n",
    "    std_val = np.std(factor_values)\n",
    "    \n",
    "    axes[0, 0].axvline(mean_val, color='r', linestyle='dashed', linewidth=1.5, label=f'Mean: {mean_val:.4f}')\n",
    "    axes[0, 0].axvline(median_val, color='g', linestyle='dashed', linewidth=1.5, label=f'Median: {median_val:.4f}')\n",
    "    \n",
    "    axes[0, 0].set_title('Histogram', fontsize=14)\n",
    "    axes[0, 0].set_xlabel(factor_name, fontsize=12)\n",
    "    axes[0, 0].set_ylabel('Frequency', fontsize=12)\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    axes[0, 0].legend()\n",
    "    \n",
    "    # 2. æ ¸å¯†åº¦ä¼°è¨ˆï¼ˆå³ä¸Šï¼‰\n",
    "    try:\n",
    "        from scipy.stats import gaussian_kde\n",
    "        density = gaussian_kde(factor_values)\n",
    "        x = np.linspace(min(factor_values), max(factor_values), 1000)\n",
    "        axes[0, 1].plot(x, density(x), 'r-', linewidth=2)\n",
    "        axes[0, 1].fill_between(x, density(x), alpha=0.3, color='#ff7f0e')\n",
    "        axes[0, 1].set_title('Kernel Density Estimation', fontsize=14)\n",
    "    except Exception as e:\n",
    "        print(f\"ç„¡æ³•å‰µå»ºå¯†åº¦åœ–: {e}\")\n",
    "        axes[0, 1].hist(factor_values, bins=50, alpha=0.7, color='#ff7f0e', \n",
    "                      density=True, edgecolor='black', linewidth=0.5)\n",
    "        axes[0, 1].set_title('Normalized Histogram', fontsize=14)\n",
    "    \n",
    "    axes[0, 1].axvline(mean_val, color='r', linestyle='dashed', linewidth=1.5)\n",
    "    axes[0, 1].axvline(median_val, color='g', linestyle='dashed', linewidth=1.5)\n",
    "    axes[0, 1].set_xlabel(factor_name, fontsize=12)\n",
    "    axes[0, 1].set_ylabel('Density', fontsize=12)\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. ç®±å‹åœ–ï¼ˆå·¦ä¸‹ï¼‰\n",
    "    axes[1, 0].boxplot(factor_values, vert=False, showmeans=True, \n",
    "                      meanprops={'marker':'o', 'markerfacecolor':'red', 'markeredgecolor':'red'},\n",
    "                      flierprops={'marker':'x', 'markerfacecolor':'red', 'markeredgecolor':'red'})\n",
    "    axes[1, 0].set_title('Boxplot', fontsize=14)\n",
    "    axes[1, 0].set_xlabel(factor_name, fontsize=12)\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. ECDFï¼ˆå³ä¸‹ï¼‰- ç¶“é©—ç´¯ç©åˆ†ä½ˆå‡½æ•¸\n",
    "    sorted_data = np.sort(factor_values)\n",
    "    ecdf = np.arange(1, len(sorted_data) + 1) / len(sorted_data)\n",
    "    axes[1, 1].step(sorted_data, ecdf, linewidth=2, color='#2ca02c')\n",
    "    \n",
    "    # ç¹ªè£½é—œéµç™¾åˆ†ä½æ•¸çš„å‚ç›´ç·š\n",
    "    percentiles = [25, 50, 75]\n",
    "    colors = ['#ff9896', '#9467bd', '#8c564b']\n",
    "    \n",
    "    for p, color in zip(percentiles, colors):\n",
    "        percentile_val = np.percentile(factor_values, p)\n",
    "        axes[1, 1].axvline(percentile_val, color=color, linestyle='--', \n",
    "                          label=f'{p}th percentile: {percentile_val:.4f}')\n",
    "    \n",
    "    axes[1, 1].set_title('Empirical Cumulative Distribution', fontsize=14)\n",
    "    axes[1, 1].set_xlabel(factor_name, fontsize=12)\n",
    "    axes[1, 1].set_ylabel('Cumulative Probability', fontsize=12)\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    axes[1, 1].legend(loc='lower right')\n",
    "    \n",
    "    # æ·»åŠ æ–‡æœ¬æ¡†é¡¯ç¤ºçµ±è¨ˆä¿¡æ¯\n",
    "    stats_text = f\"\"\"\n",
    "    Mean: {mean_val:.4f}\n",
    "    Median: {median_val:.4f}\n",
    "    Std Dev: {std_val:.4f}\n",
    "    Skewness: {np.percentile(factor_values, 75) - 2*np.percentile(factor_values, 50) + np.percentile(factor_values, 25):.4f}\n",
    "    Min: {np.min(factor_values):.4f}\n",
    "    Max: {np.max(factor_values):.4f}\n",
    "    Count: {len(factor_values)}\n",
    "    \"\"\"\n",
    "    \n",
    "    # æ”¾åœ¨ç®±å‹åœ–ä¸Šæ–¹\n",
    "    axes[1, 0].text(0.05, 0.95, stats_text, transform=axes[1, 0].transAxes, \n",
    "                  fontsize=10, verticalalignment='top', \n",
    "                  bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "    \n",
    "    # æ·»åŠ è¶…ç´šæ¨™é¡Œ\n",
    "    plt.suptitle(f'Distribution Analysis: {factor_name}', \n",
    "                fontsize=16, y=0.98, fontweight='bold')\n",
    "    \n",
    "    # ä¿å­˜åœ–åƒ\n",
    "    if hasattr(c, 'save_plot') and c.save_plot:\n",
    "        filename = f\"distribution_{c.symbol}_{factor_name}_{datetime.now().strftime('%Y%m%d')}.png\"\n",
    "        plt.savefig(filename, dpi=300, bbox_inches='tight')\n",
    "        print(f\"åœ–åƒå·²ä¿å­˜ç‚º: {filename}\")\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    # è¨ˆç®—ä¸¦è¿”å›æ›´å¤šçµ±è¨ˆä¿¡æ¯\n",
    "    from scipy import stats\n",
    "    \n",
    "    try:\n",
    "        # å˜—è©¦è¨ˆç®—é«˜ç´šçµ±è¨ˆé‡\n",
    "        factor_skewness = stats.skew(factor_values)\n",
    "        factor_kurtosis = stats.kurtosis(factor_values)\n",
    "        shapiro_test = stats.shapiro(factor_values)\n",
    "        ks_normal_test = stats.kstest(factor_values, 'norm', args=(mean_val, std_val))\n",
    "        \n",
    "        print(f\"åˆ†ä½ˆçµ±è¨ˆè³‡è¨Šï¼š\")\n",
    "        print(f\"  ååº¦: {factor_skewness:.4f} ({'æ­£å' if factor_skewness > 0 else 'è² å'})\")\n",
    "        print(f\"  å³°åº¦: {factor_kurtosis:.4f} ({'å°–å³°' if factor_kurtosis > 0 else 'å¹³å³°'})\")\n",
    "        print(f\"  Shapiro-Wilkæ­£æ…‹æ€§æª¢é©— på€¼: {shapiro_test.pvalue:.6f} ({'å¯èƒ½æ˜¯æ­£æ…‹åˆ†ä½ˆ' if shapiro_test.pvalue > 0.05 else 'éæ­£æ…‹åˆ†ä½ˆ'})\")\n",
    "        print(f\"  KSæ­£æ…‹æ€§æª¢é©— på€¼: {ks_normal_test.pvalue:.6f} ({'å¯èƒ½æ˜¯æ­£æ…‹åˆ†ä½ˆ' if ks_normal_test.pvalue > 0.05 else 'éæ­£æ…‹åˆ†ä½ˆ'})\")\n",
    "    except Exception as e:\n",
    "        print(f\"è¨ˆç®—é«˜ç´šçµ±è¨ˆé‡æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}\")\n",
    "    \n",
    "    # è¿”å›åŸºæœ¬çµ±è¨ˆä¿¡æ¯\n",
    "    return {\n",
    "        \"mean\": mean_val,\n",
    "        \"median\": median_val,\n",
    "        \"std\": std_val,\n",
    "        \"min\": np.min(factor_values),\n",
    "        \"max\": np.max(factor_values),\n",
    "        \"count\": len(factor_values),\n",
    "        \"quartiles\": [np.percentile(factor_values, p) for p in [25, 50, 75]]\n",
    "    }\n",
    "\n",
    "def visualize_factor_time_series(data):\n",
    "    \"\"\"\n",
    "    é€²éšæ™‚é–“åºåˆ—åˆ†æè¦–åœ–\n",
    "    \n",
    "    Args:\n",
    "        data: ç”±visualize_factorså‡½æ•¸è¿”å›çš„æ•¸æ“šå­—å…¸\n",
    "    \"\"\"\n",
    "    if data is None:\n",
    "        print(\"æ²’æœ‰æ•¸æ“šå¯ç”¨æ–¼æ™‚é–“åºåˆ—åˆ†æ\")\n",
    "        return\n",
    "    \n",
    "    candle_df = data['candle']\n",
    "    factor_df = data['factor']\n",
    "    factor_name = data['factor_name']\n",
    "    \n",
    "    # ç¢ºä¿æ™‚é–“åˆ—æ ¼å¼æ­£ç¢º\n",
    "    factor_df['time'] = pd.to_datetime(factor_df['time'])\n",
    "    candle_df['time'] = pd.to_datetime(candle_df['time'])\n",
    "    \n",
    "    # åˆä½µæ•¸æ“š\n",
    "    merged_df = pd.merge_asof(\n",
    "        candle_df.sort_values('time'), \n",
    "        factor_df[['time', factor_name]].sort_values('time'),\n",
    "        on='time',\n",
    "        direction='nearest'\n",
    "    )\n",
    "    \n",
    "    # è¨­ç½®æ™‚é–“ç´¢å¼•\n",
    "    merged_df.set_index('time', inplace=True)\n",
    "    \n",
    "    # å¦‚æœæ•¸æ“šé»è¶³å¤ ï¼Œè¨ˆç®—ç§»å‹•å¹³å‡\n",
    "    has_ma = len(merged_df) >= 30\n",
    "    if has_ma:\n",
    "        merged_df[f'{factor_name}_MA30'] = merged_df[factor_name].rolling(window=30).mean()\n",
    "        merged_df[f'{factor_name}_MA60'] = merged_df[factor_name].rolling(window=60).mean()\n",
    "    \n",
    "    # å‰µå»º2x2ç¶²æ ¼åœ–è¡¨\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "    plt.subplots_adjust(hspace=0.3, wspace=0.3)\n",
    "    \n",
    "    # 1. ä¸»è¦æ™‚é–“åºåˆ—åœ–ï¼ˆå·¦ä¸Šï¼‰\n",
    "    axes[0, 0].plot(merged_df.index, merged_df[factor_name], color='#1f77b4', linewidth=1.5, label=factor_name)\n",
    "    \n",
    "    if has_ma:\n",
    "        axes[0, 0].plot(merged_df.index, merged_df[f'{factor_name}_MA30'], \n",
    "                       color='#ff7f0e', linewidth=1.5, label='30-Period MA')\n",
    "        axes[0, 0].plot(merged_df.index, merged_df[f'{factor_name}_MA60'], \n",
    "                       color='#2ca02c', linewidth=1.5, label='60-Period MA')\n",
    "    \n",
    "    axes[0, 0].set_title(f'{factor_name} Time Series', fontsize=14)\n",
    "    axes[0, 0].set_ylabel('Value', fontsize=12)\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    axes[0, 0].legend(loc='best')\n",
    "    axes[0, 0].xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n",
    "    axes[0, 0].xaxis.set_major_locator(mdates.MonthLocator(interval=3))\n",
    "    plt.setp(axes[0, 0].xaxis.get_majorticklabels(), rotation=45)\n",
    "    \n",
    "    # 2. å› å­è®ŠåŒ–ç‡ï¼ˆå³ä¸Šï¼‰\n",
    "    merged_df[f'{factor_name}_chg'] = merged_df[factor_name].pct_change()\n",
    "    axes[0, 1].plot(merged_df.index, merged_df[f'{factor_name}_chg'], \n",
    "                   color='#d62728', linewidth=1.5)\n",
    "    axes[0, 1].set_title(f'{factor_name} Percent Change', fontsize=14)\n",
    "    axes[0, 1].set_ylabel('% Change', fontsize=12)\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    axes[0, 1].xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n",
    "    axes[0, 1].xaxis.set_major_locator(mdates.MonthLocator(interval=3))\n",
    "    plt.setp(axes[0, 1].xaxis.get_majorticklabels(), rotation=45)\n",
    "    \n",
    "    # 3. ç´¯ç©åˆ†ä½ˆéš¨æ™‚é–“è®ŠåŒ–ï¼ˆå·¦ä¸‹ï¼‰\n",
    "    try:\n",
    "        # å‰µå»ºä¸€å€‹colormapï¼Œç”¨æ–¼å€åˆ†æ™‚é–“\n",
    "        import matplotlib.cm as cm\n",
    "        import matplotlib.colors as mcolors\n",
    "        from matplotlib.collections import LineCollection\n",
    "        \n",
    "        # å°‡æ™‚é–“åºåˆ—åˆ†ç‚ºè‹¥å¹²æ®µ\n",
    "        n_segments = 4\n",
    "        segment_size = len(merged_df) // n_segments\n",
    "        \n",
    "        # ä¾æ™‚é–“é †åºç‚ºæ¯å€‹æ®µè½è¨­ç½®ä¸åŒé¡è‰²\n",
    "        colors = plt.cm.viridis(np.linspace(0, 1, n_segments))\n",
    "        \n",
    "        # ç¹ªè£½åˆ†æ®µçš„ECDF\n",
    "        for i in range(n_segments):\n",
    "            start_idx = i * segment_size\n",
    "            end_idx = start_idx + segment_size if i < n_segments - 1 else len(merged_df)\n",
    "            \n",
    "            segment_data = merged_df[factor_name].iloc[start_idx:end_idx].dropna().values\n",
    "            if len(segment_data) > 0:\n",
    "                sorted_data = np.sort(segment_data)\n",
    "                ecdf = np.arange(1, len(sorted_data) + 1) / len(sorted_data)\n",
    "                \n",
    "                # ç²å–è©²æ®µçš„æ™‚é–“æ¨™ç±¤\n",
    "                time_label = merged_df.index[start_idx].strftime('%Y-%m-%d')\n",
    "                \n",
    "                axes[1, 0].step(sorted_data, ecdf, linewidth=2, color=colors[i], \n",
    "                              label=f'Period {i+1}: from {time_label}')\n",
    "        \n",
    "        axes[1, 0].set_title('ECDF Evolution Over Time', fontsize=14)\n",
    "        axes[1, 0].set_xlabel(factor_name, fontsize=12)\n",
    "        axes[1, 0].set_ylabel('Cumulative Probability', fontsize=12)\n",
    "        axes[1, 0].grid(True, alpha=0.3)\n",
    "        axes[1, 0].legend(loc='best')\n",
    "    except Exception as e:\n",
    "        print(f\"ç„¡æ³•å‰µå»ºç´¯ç©åˆ†ä½ˆè®ŠåŒ–åœ–: {e}\")\n",
    "        # å‚™ç”¨åœ–ï¼šä½¿ç”¨æœ€ç°¡å–®çš„æ»¾å‹•çµ±è¨ˆ\n",
    "        if has_ma:\n",
    "            axes[1, 0].plot(merged_df.index, merged_df[f'{factor_name}'].rolling(window=30).std(), \n",
    "                         color='#9467bd', linewidth=1.5, label='30-Period Std')\n",
    "            axes[1, 0].set_title(f'{factor_name} Rolling Volatility', fontsize=14)\n",
    "            axes[1, 0].set_ylabel('Standard Deviation', fontsize=12)\n",
    "            axes[1, 0].grid(True, alpha=0.3)\n",
    "            axes[1, 0].legend(loc='best')\n",
    "            axes[1, 0].xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n",
    "            axes[1, 0].xaxis.set_major_locator(mdates.MonthLocator(interval=3))\n",
    "            plt.setp(axes[1, 0].xaxis.get_majorticklabels(), rotation=45)\n",
    "        else:\n",
    "            axes[1, 0].text(0.5, 0.5, 'Insufficient data for rolling metrics', \n",
    "                          transform=axes[1, 0].transAxes, ha='center', va='center',\n",
    "                          fontsize=12)\n",
    "    \n",
    "    # 4. åƒ¹æ ¼èˆ‡å› å­ç–ŠåŠ åœ–ï¼ˆå³ä¸‹ï¼‰\n",
    "    ax1 = axes[1, 1]\n",
    "    ax2 = ax1.twinx()\n",
    "    \n",
    "    # ç¹ªè£½åƒ¹æ ¼ç·š\n",
    "    price_line, = ax1.plot(merged_df.index, merged_df['Close'], color='#1f77b4', linewidth=1.5, label='Price')\n",
    "    ax1.set_ylabel('Price', fontsize=12, color='#1f77b4')\n",
    "    ax1.tick_params(axis='y', colors='#1f77b4')\n",
    "    \n",
    "    # ç¹ªè£½å› å­ç·š\n",
    "    factor_line, = ax2.plot(merged_df.index, merged_df[factor_name], color='#d62728', linewidth=1.5, label=factor_name)\n",
    "    ax2.set_ylabel(factor_name, fontsize=12, color='#d62728')\n",
    "    ax2.tick_params(axis='y', colors='#d62728')\n",
    "    \n",
    "    # çµ„åˆå…©å€‹åœ–ä¾‹\n",
    "    lines = [price_line, factor_line]\n",
    "    ax1.legend(lines, [line.get_label() for line in lines], loc='upper left')\n",
    "    \n",
    "    ax1.set_title('Price vs Factor Overlay', fontsize=14)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n",
    "    ax1.xaxis.set_major_locator(mdates.MonthLocator(interval=3))\n",
    "    plt.setp(ax1.xaxis.get_majorticklabels(), rotation=45)\n",
    "    \n",
    "    # æ·»åŠ è¶…ç´šæ¨™é¡Œ\n",
    "    plt.suptitle(f'Advanced Time Series Analysis: {factor_name}', \n",
    "                fontsize=16, y=0.98, fontweight='bold')\n",
    "    \n",
    "    # ä¿å­˜åœ–åƒ\n",
    "    if hasattr(c, 'save_plot') and c.save_plot:\n",
    "        filename = f\"time_series_{c.symbol}_{factor_name}_{datetime.now().strftime('%Y%m%d')}.png\"\n",
    "        plt.savefig(filename, dpi=300, bbox_inches='tight')\n",
    "        print(f\"åœ–åƒå·²ä¿å­˜ç‚º: {filename}\")\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    return merged_df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(f\"æ­£åœ¨åŸ·è¡Œ {c.symbol} çš„å› å­åˆ†æï¼Œä½¿ç”¨å› å­: {c.factor}\" + \n",
    "          (f\" å’Œ {c.factor2}\" if c.factor2 and c.operation != 'none' else \"\"))\n",
    "    \n",
    "    # åŸ·è¡Œå› å­è¦–è¦ºåŒ–\n",
    "    data = visualize_factors()\n",
    "    \n",
    "    # å¯è¦–åŒ–å› å­èˆ‡åƒ¹æ ¼çš„ç›¸é—œæ€§\n",
    "    correlation_results = visualize_factor_correlation(data)\n",
    "    \n",
    "    # å¯è¦–åŒ–å› å­åˆ†ä½ˆ\n",
    "    stats = visualize_factor_distribution(data)\n",
    "    \n",
    "    # é¡å¤–åŸ·è¡Œæ™‚é–“åºåˆ—åˆ†æ\n",
    "    try:\n",
    "        print(\"\\nåŸ·è¡Œæ™‚é–“åºåˆ—åˆ†æ...\")\n",
    "        merged_data = visualize_factor_time_series(data)\n",
    "        print(\"åˆ†æå®Œæˆï¼\")\n",
    "    except Exception as e:\n",
    "        print(f\"æ™‚é–“åºåˆ—åˆ†ææ™‚ç™¼ç”ŸéŒ¯èª¤: {e}\")\n",
    "        \n",
    "    print(\"\\nåˆ†æç¸½çµ:\")\n",
    "    print(f\"- å› å­: {data['factor_name']}\")\n",
    "    if 'correlation_results' in locals() and correlation_results and 'correlations' in correlation_results:\n",
    "        print(f\"- ç›¸é—œæ€§: {', '.join(correlation_results['correlations'])}\")\n",
    "    if 'stats' in locals() and stats:\n",
    "        print(f\"- å¹³å‡å€¼: {stats['mean']:.4f}, ä¸­ä½æ•¸: {stats['median']:.4f}, æ¨™æº–å·®: {stats['std']:.4f}\")\n",
    "        print(f\"- ç¯„åœ: {stats['min']:.4f} åˆ° {stats['max']:.4f}, æ¨£æœ¬æ•¸: {stats['count']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Split Loop + Heatmap (Inclding looping Preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilsnumpy import nan_count, load_all_data, combines_data, data_processing, precompute_rolling_stats, backtest_cached\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "import config as c\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import sys\n",
    "from dask import delayed, compute\n",
    "from dask.diagnostics import ProgressBar\n",
    "\n",
    "def parse_manual_selection(filepath, all_models):\n",
    "    \"\"\"\n",
    "    è§£ææ‰‹å‹•é¸æ“‡çš„æ¨¡å‹èˆ‡é€²å‡ºå ´æ–¹å¼ã€‚\n",
    "    \"\"\"\n",
    "    with open(filepath, \"r\") as file:\n",
    "        lines = [line.strip() for line in file.readlines() if line.strip()]\n",
    "    models_entrys = {}\n",
    "    current_model = None\n",
    "    for line in lines:\n",
    "        if line in all_models:\n",
    "            current_model = line\n",
    "            models_entrys[current_model] = []\n",
    "        elif current_model:\n",
    "            models_entrys[current_model].append(line)\n",
    "    return models_entrys\n",
    "\n",
    "def plot_heatmaps(sr_threshold=1.5, preprocess_method=None, highlight_x=0.5, highlight_y=50):\n",
    "    for model, entry, backtest_df in plot_data:\n",
    "        # âœ… Dynamically set SR threshold based on entry name\n",
    "        sr_threshold = 1.2 if entry.startswith('S') else sr_threshold  # Use 1.2 for entries starting with 'S', otherwise 1.8\n",
    "        sr_threshold = sr_threshold if entry.startswith('L') else sr_threshold # Changing threshold backto 1.5 if startwith 'L'\n",
    "        # âœ… Optimized pivot using groupby instead of pivot\n",
    "        sr_pivot_data = backtest_df.groupby(['rolling_window', 'threshold'])['SR'].mean().unstack()\n",
    "\n",
    "        # âœ… Check if the entire heatmap is NaN\n",
    "        if sr_pivot_data.isna().all().all():\n",
    "            print(f\"âš ï¸ Skipping {model}_{entry} heatmap: All SR values are NaN.\")\n",
    "            continue  # Skip plotting\n",
    "\n",
    "        # âœ… Check if there is at least one SR > threshold\n",
    "        if not np.any(sr_pivot_data.to_numpy() > sr_threshold):\n",
    "            print(f\"âš ï¸ Skipping {model}_{entry} heatmap: No SR value exceeds {sr_threshold}.\")\n",
    "            continue  # Skip plotting\n",
    "\n",
    "        plt.figure(figsize=(20, 16))  # âœ… Reduced figure size for faster rendering\n",
    "        \n",
    "        sr_pivot_data.columns = sr_pivot_data.columns.round(2)\n",
    "\n",
    "        ax = sns.heatmap(sr_pivot_data, annot=True, fmt=\".2f\", cmap=\"RdYlGn\", linewidths=0.3, cbar_kws={'label': 'Sharpe Ratio'})           \n",
    "        plt.xticks(np.arange(len(sr_pivot_data.columns)) + 0.5, [f\"{col:.2f}\" for col in sr_pivot_data.columns], rotation=45)\n",
    "        plt.yticks(np.arange(len(sr_pivot_data.index)) + 0.5, [f\"{row:.2f}\" for row in sr_pivot_data.index], rotation=0)\n",
    "        # plt.grid(visible=True, linestyle='--', linewidth=0.5)  # é¡¯ç¤ºæ ¼ç·š\n",
    "        \n",
    "        # å¦‚æœä½ æƒ³è¦é«˜äº®æŸå€‹ (highlight_x, highlight_y) å°æ‡‰çš„ pivot cellï¼š\n",
    "        if highlight_x is not None and highlight_y is not None:\n",
    "            # æ‰¾å‡º x, y çš„å¯¦éš›ç´¢å¼•ä½ç½®\n",
    "            # highlight_x å°æ‡‰ columns (threshold)ï¼Œ highlight_y å°æ‡‰ index (rolling_window)\n",
    "            try:\n",
    "                col_idx = sr_pivot_data.columns.get_loc(highlight_x)\n",
    "                row_idx = sr_pivot_data.index.get_loc(highlight_y)\n",
    "                # è¨­å®š 3x3 å€åŸŸï¼Œä¸¦è®“ highlight cell ç‚ºä¸­å¤®\n",
    "                top_left_col = col_idx - 1 if col_idx > 0 else col_idx\n",
    "                top_left_row = row_idx - 1 if row_idx > 0 else row_idx\n",
    "                # æ ¹æ“šè³‡æ–™è¡¨å¤§å°ï¼Œè¨ˆç®—å¯¬èˆ‡é«˜ï¼ˆé¿å…è¶…å‡ºé‚Šç•Œï¼‰\n",
    "                max_cols = len(sr_pivot_data.columns)\n",
    "                max_rows = len(sr_pivot_data.index)\n",
    "                width = 3 if top_left_col + 3 <= max_cols else max_cols - top_left_col\n",
    "                height = 3 if top_left_row + 3 <= max_rows else max_rows - top_left_row\n",
    "                # ç•«å‡º 3x3 çš„æ¡†æ¡†\n",
    "                ax.add_patch(Rectangle(\n",
    "                    (top_left_col, top_left_row),  # èµ·å§‹ä½ç½®\n",
    "                    width, height,\n",
    "                    fill=False,          # åªæ¡†ç·šï¼Œä¸å¡«æ»¿\n",
    "                    edgecolor='red',\n",
    "                    linewidth=2\n",
    "                ))\n",
    "                print(f\"âœ¨ Highlighted 3x3 block centered at x={highlight_x}, y={highlight_y}\")\n",
    "            except KeyError:\n",
    "                print(f\"âš ï¸ Cannot highlight ({highlight_x}, {highlight_y}): value not found in pivot.\")\n",
    "        # ============== é—œéµåŠ äº®éƒ¨åˆ†çµæŸ ==============\n",
    "        \n",
    "        plt.title(f\"{model}_{preprocess_method}_{entry} Train Period BackTest SR Heatmap\", fontsize=14)\n",
    "        # âœ… Save the heatmap in the current working directory\n",
    "        if c.save_plot:\n",
    "            save_path = f\"{model}_{entry}_heatmap.png\"\n",
    "            plt.savefig(save_path, dpi=300, bbox_inches='tight', pad_inches=0.1)\n",
    "            print(f\"ğŸ“ Heatmap saved to: {save_path}\")\n",
    "\n",
    "        plt.show()  # âœ… Display the plot\n",
    "        plt.close()  # âœ… Free memory after each plot\n",
    "\n",
    "def process_and_validate(factor_series, method, factor_name):\n",
    "    \"\"\"\n",
    "    ä½¿ç”¨æŒ‡å®šçš„ preprocess æ–¹æ³•è™•ç†è³‡æ–™ä¸¦æª¢æŸ¥ NaN ç™¾åˆ†æ¯”ã€‚\n",
    "    \n",
    "    Parameters:\n",
    "        factor_series (pd.Series): åŸå§‹çš„å› å­è³‡æ–™ã€‚\n",
    "        method (str): è¦å¥—ç”¨çš„é è™•ç†æ–¹æ³•ã€‚\n",
    "        factor_name (str): å› å­åç¨±ï¼Œç”¨æ–¼ debug è¨Šæ¯ã€‚\n",
    "        \n",
    "    Returns:\n",
    "        pd.Series: è‹¥è™•ç†å¾Œ NaN ä½æ–¼ 3%ï¼Œå‰‡å›å‚³è™•ç†å¾Œçš„è³‡æ–™ï¼›å¦å‰‡å›å‚³ None è¡¨ç¤ºè·³éæ­¤æ–¹æ³•ã€‚\n",
    "    \"\"\"\n",
    "    if method != \"direct\":\n",
    "        # Update to handle the return values from data_processing\n",
    "        factor_df, new_factor_name = data_processing(pd.DataFrame({factor_name: factor_series}), method, factor_name)\n",
    "        processed = factor_df[new_factor_name]\n",
    "    else:\n",
    "        processed = factor_series.copy()\n",
    "    if processed.isna().sum() / len(processed) > c.nan_perc:\n",
    "        preprocess_nan_count = nan_count(processed)\n",
    "        print(f\"nan count After {method} Preprocessing: {preprocess_nan_count}\")\n",
    "        print(f\"{factor_name} after {method} transformation exceed 3% NaN. Skipping this preprocess method.\")\n",
    "        return None\n",
    "    else:\n",
    "        print(f\"NaN% after preprocess: {processed.isna().sum()/len(processed)}%.\")\n",
    "        processed.dropna(inplace=True)\n",
    "    return processed\n",
    "\n",
    "def main(candle_data, factor_data, factor, interval, operation, model, entry,\n",
    "         window_start, window_end, window_step, threshold_start, threshold_end,\n",
    "         threshold_step, rolling_stats, preprocess_method, date_range):\n",
    "    \"\"\"\n",
    "    å›æ¸¬ä¸»ç¨‹å¼ï¼Œæ ¹æ“šåƒæ•¸é€²è¡Œå¤šåƒæ•¸å›æ¸¬ã€‚\n",
    "    \"\"\"\n",
    "    candle_df_copy = candle_data[['start_time', 'Close']].copy()\n",
    "    candle_df_copy.columns = ['start_time', 'close']\n",
    "    factor_df_copy = factor_data[['start_time', factor]].copy()\n",
    "\n",
    "    candle_df_copy['time'] = pd.to_datetime(candle_df_copy['start_time'], unit='ms')\n",
    "    \n",
    "    annualizer = annualizer_dict.get(interval, None)\n",
    "    backtest_report = []\n",
    "    for rolling_window in range(window_start, window_end, window_step):\n",
    "        for threshold in np.arange(threshold_start, threshold_end, threshold_step):\n",
    "            result, _, log_msgs = backtest_cached(candle_df_copy, factor_df_copy, rolling_window, threshold, \n",
    "                                          preprocess_method, entry, annualizer, model, factor, interval, date_range,\n",
    "                                          rolling_stats)\n",
    "            backtest_report.append(result)\n",
    "    \n",
    "    backtest_df = pd.DataFrame(backtest_report)\n",
    "    return (model, entry, backtest_df, log_msgs)\n",
    "\n",
    "# å®šç¾© annualizer å­—å…¸\n",
    "annualizer_dict = {\n",
    "    '1m': 525600, '5m': 105120, '15m': 35040,\n",
    "    '30m': 17520, '1h': 8760, '4h': 2190,\n",
    "    '1d': 365, '1w': 52, '1M': 12\n",
    "}\n",
    "\n",
    "# è¼‰å…¥åŸå§‹è³‡æ–™\n",
    "raw_candle, raw_factor = load_all_data(c.candle_file, c.factor_file, c.factor2_file, c.factor, c.factor2)\n",
    "train_split = annualizer_dict.get(c.interval, None) * 3\n",
    "\n",
    "candle_train = raw_candle[:train_split].reset_index(drop=True).copy()\n",
    "factor_train_original = raw_factor[:train_split].reset_index(drop=True).copy()\n",
    "\n",
    "# Start and end Time and date_range\n",
    "start_time = max(candle_train['start_time'].min(), factor_train_original['start_time'].min())\n",
    "end_time = min(candle_train['start_time'].max(), factor_train_original['start_time'].max())\n",
    "date_range = pd.date_range(start=pd.to_datetime(start_time, unit='ms'),\n",
    "                           end=pd.to_datetime(end_time, unit='ms'),\n",
    "                           freq=c.interval)\n",
    "candle_train['time'] = pd.to_datetime(candle_train['start_time'], unit='ms')\n",
    "factor_train_original['time'] = pd.to_datetime(factor_train_original['start_time'], unit='ms')\n",
    "candle_train.set_index('time', inplace=True)\n",
    "factor_train_original.set_index('time', inplace=True)\n",
    "\n",
    "# è‹¥ operation ä¸æ˜¯ 'none'ï¼Œå‰‡åˆä½µå…©å€‹å› å­\n",
    "if c.operation != 'none':\n",
    "    # ç›´æ¥å‚³å…¥ DataFrame é€²è¡Œè™•ç†\n",
    "    factor_train_original, merged_col_name = combines_data(\n",
    "        factor_train_original,\n",
    "        c.factor,\n",
    "        c.factor2,\n",
    "        c.operation\n",
    "    )\n",
    "    factor_used = merged_col_name\n",
    "else:\n",
    "    factor_used = c.factor\n",
    "\n",
    "################################################\n",
    "# Step 0: åˆ¤æ–· c.preprocess æ˜¯å–®ä¸€å­—ä¸²ï¼Œé‚„æ˜¯ä¸²åˆ—\n",
    "################################################\n",
    "if isinstance(c.preprocess, list):\n",
    "    all_preprocess_methods = c.preprocess\n",
    "else:\n",
    "    all_preprocess_methods = [c.preprocess]\n",
    "\n",
    "# å°æ¯å€‹ preprocess æ–¹æ³•é€²è¡Œè¿´åœˆ\n",
    "for current_preprocess in all_preprocess_methods:\n",
    "    print(f\"\\n===== Processing with preprocess method: {current_preprocess} =====\")\n",
    "    # å¾åŸå§‹è³‡æ–™è¤‡è£½ä¸€ä»½\n",
    "    factor_train = factor_train_original.copy()\n",
    "    # å°æŒ‡å®šå› å­é€²è¡Œé è™•ç†\n",
    "    processed_factor = process_and_validate(factor_train[factor_used], current_preprocess, factor_used)\n",
    "    if processed_factor is None:\n",
    "        continue  # å¦‚æœé©—è­‰ä¸é€šéï¼Œå‰‡è·³åˆ°ä¸‹ä¸€å€‹ preprocess æ–¹æ³•\n",
    "    factor_train[factor_used] = processed_factor\n",
    "\n",
    "    # æ¨¡å‹èˆ‡é€²å‡ºå ´è¨­å®š\n",
    "    if c.USE_ALL_MODELS:\n",
    "        models = c.models\n",
    "        entry_map = {model: c.entrys for model in c.models}\n",
    "        # window_step = 20\n",
    "        # threshold_step = 0.2\n",
    "    else:\n",
    "        entry_map = parse_manual_selection(\"manual_selected.txt\", c.models)\n",
    "        models = list(entry_map.keys())\n",
    "        # window_step = 10\n",
    "        # threshold_step = 0.1\n",
    "\n",
    "    # é å…ˆè¨ˆç®—æ»¾å‹•çµ±è¨ˆå€¼\n",
    "    windows = list(range(5, c.window_end, c.window_step))\n",
    "    rolling_stats_dict = precompute_rolling_stats(factor_train[factor_used], windows)\n",
    "\n",
    "    # é‡ç½® plot_data ä»¥å„²å­˜ç•¶å‰ preprocess æ–¹æ³•çš„å›æ¸¬çµæœ\n",
    "    plot_data = []\n",
    "    tasks = []\n",
    "    for model in models:\n",
    "        for entry in entry_map[model]:\n",
    "            task = delayed(main)(\n",
    "                candle_train,\n",
    "                factor_train,\n",
    "                factor_used,\n",
    "                c.interval,\n",
    "                c.operation,\n",
    "                model,\n",
    "                entry,\n",
    "                window_start=5,\n",
    "                window_end=c.window_end,\n",
    "                window_step=c.window_step,\n",
    "                threshold_start=0,\n",
    "                threshold_end=c.threshold_end,\n",
    "                threshold_step=c.threshold_step,\n",
    "                rolling_stats=rolling_stats_dict,\n",
    "                preprocess_method=current_preprocess,\n",
    "                date_range=date_range\n",
    "            )\n",
    "            tasks.append(task)\n",
    "\n",
    "    with ProgressBar():\n",
    "        results = compute(*tasks, scheduler='processes')\n",
    "    \n",
    "    for res in results:\n",
    "        if res is not None:\n",
    "            m, e, backtest_df, log_msgs = res\n",
    "            for msg in log_msgs:\n",
    "                print(msg)\n",
    "            plot_data.append((m, e, backtest_df))\n",
    "\n",
    "    # ç¹ªè£½ç•¶å‰ preprocess æ–¹æ³•çš„ç†±åŠ›åœ–\n",
    "    plot_heatmaps(1.65, preprocess_method=current_preprocess, highlight_x=c.highlight_threshold, highlight_y=c.highlight_window)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split forward, Split Backtest, full_length_backtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilsnumpy import split_data, load_all_data, combines_data, data_processing, backtest_cached, additional_metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import config as c\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import sys\n",
    "import os\n",
    "\n",
    "annualizer_dict = {\n",
    "    '1m': 525600, '5m': 105120, '15m': 35040,\n",
    "    '30m': 17520, '1h': 8760, '4h': 2190,\n",
    "    '1d': 365, '1w': 52, '1M': 12\n",
    "}\n",
    "\n",
    "def load_and_prepare_data():\n",
    "    \"\"\"åŠ è½½å’Œå‡†å¤‡å›æµ‹æ•°æ®ï¼ŒåŒ…æ‹¬åˆ†å‰²ã€é¢„å¤„ç†ï¼Œå¹¶è¿›è¡Œæ­£ç¡®çš„NaNæ£€æŸ¥\"\"\"\n",
    "    \n",
    "    print(\"Loading data...\")\n",
    "    # åŠ è½½åŸå§‹æ•°æ®\n",
    "    raw_candle, raw_factor = load_all_data(c.candle_file, c.factor_file, c.factor2_file, c.factor, c.factor2)\n",
    "\n",
    "    # è®¡ç®—åˆ†å‰²ç‚¹\n",
    "    annualizer = annualizer_dict.get(c.interval, 365)\n",
    "    # train_split = annualizer * 3  # è®­ç»ƒä½¿ç”¨3å¹´æ•°æ®\n",
    "    \n",
    "    print(f\"Using {c.interval} data, annualizer: {annualizer}\") #, train_split points: {train_split}\n",
    "\n",
    "    # æŒ‰æ—¶é—´æˆ³æ’åº\n",
    "    raw_candle = raw_candle.sort_values('start_time').reset_index(drop=True)\n",
    "    raw_factor = raw_factor.sort_values('start_time').reset_index(drop=True)\n",
    "\n",
    "    # æ£€æŸ¥æ•°æ®é•¿åº¦\n",
    "    if len(raw_candle) <= 300:\n",
    "        print(f\"Error: Candle data ({len(raw_candle)} points) is insufficient for the specified train_split ({train_split} points)\")\n",
    "        return None\n",
    "    \n",
    "    if len(raw_factor) <= 300:\n",
    "        print(f\"Error: Factor data ({len(raw_factor)} points) is insufficient for the specified train_split ({train_split} points)\")\n",
    "        return None\n",
    "\n",
    "    # åˆ†å‰²æ•°æ®\n",
    "    split_result = split_data(raw_candle, raw_factor, years_for_training=3)\n",
    "    \n",
    "    # åˆ†å‰²dataframes\n",
    "    candle_train = split_result['train']['candle']\n",
    "    factor_train = split_result['train']['factor']\n",
    "    candle_test = split_result['test']['candle']\n",
    "    factor_test = split_result['test']['factor']\n",
    "    candle_full = split_result['full']['candle']\n",
    "    factor_full = split_result['full']['factor']\n",
    "\n",
    "    # è®°å½•åŸå§‹æ•°æ®é•¿åº¦\n",
    "    factor_train_length = len(factor_train)\n",
    "    factor_test_length = len(factor_test)\n",
    "    factor_full_length = len(factor_full)\n",
    "    \n",
    "    # ä¸ºæ‰€æœ‰æ•°æ®æ·»åŠ æ—¶é—´åˆ—\n",
    "    for df in [candle_train, factor_train, candle_test, factor_test, candle_full, factor_full]:\n",
    "        df['time'] = pd.to_datetime(df['start_time'], unit='ms')\n",
    "    \n",
    "    # åˆ›å»ºå®Œæ•´æ—¥æœŸèŒƒå›´\n",
    "    train_date_range = pd.date_range(\n",
    "        start=pd.to_datetime(max(candle_train['start_time'].min(), factor_train['start_time'].min()), unit='ms'),\n",
    "        end=pd.to_datetime(min(candle_train['start_time'].max(), factor_train['start_time'].max()), unit='ms'),\n",
    "        freq=c.interval\n",
    "    )\n",
    "    test_date_range = pd.date_range(\n",
    "        start=pd.to_datetime(max(candle_test['start_time'].min(), factor_test['start_time'].min()), unit='ms'),\n",
    "        end=pd.to_datetime(min(candle_test['start_time'].max(), factor_test['start_time'].max()), unit='ms'),\n",
    "        freq=c.interval\n",
    "    )\n",
    "    full_date_range = pd.date_range(\n",
    "        start=pd.to_datetime(max(candle_full['start_time'].min(), factor_full['start_time'].min()), unit='ms'),\n",
    "        end=pd.to_datetime(min(candle_full['start_time'].max(), factor_full['start_time'].max()), unit='ms'),\n",
    "        freq=c.interval\n",
    "    )\n",
    "\n",
    "    # åˆå§‹ä½¿ç”¨çš„å› å­åç§°\n",
    "    factor_name = c.factor\n",
    "\n",
    "    # 1. ç»„åˆå› å­å¤„ç† (å¦‚æœéœ€è¦)\n",
    "    if c.operation != 'none':\n",
    "        print(f\"\\nApplying operation '{c.operation}' to factors...\")\n",
    "        \n",
    "        # --- åˆ›å»ºæ•°æ®å‰¯æœ¬ç”¨äºNaNæ£€æµ‹ ---\n",
    "        factor_train_copy = factor_train.copy()\n",
    "        factor_test_copy = factor_test.copy()\n",
    "        factor_full_copy = factor_full.copy()\n",
    "        \n",
    "        # --- è®­ç»ƒé›†å¤„ç† ---\n",
    "        # åœ¨è®¡ç®—NaNæ¯”ä¾‹ä¹‹å‰å…ˆæ£€æŸ¥åŸå§‹æ•°æ®çš„NaNæ¯”ä¾‹\n",
    "        train_temp = factor_train_copy.set_index('time')\n",
    "        train_temp_reindexed = train_temp.reindex(train_date_range)\n",
    "        \n",
    "        # æ£€æŸ¥åŸå§‹æ•°æ®çš„NaNç™¾åˆ†æ¯”\n",
    "        nan_train_before = train_temp_reindexed[c.factor].isna().sum() / len(train_date_range)\n",
    "        nan_train_before2 = train_temp_reindexed[c.factor2].isna().sum() / len(train_date_range)\n",
    "        \n",
    "        if nan_train_before > c.nan_perc or nan_train_before2 > c.nan_perc:\n",
    "            print(f\"Error: Input factors already have too many NaNs before combination: {c.factor}={nan_train_before:.3f}, {c.factor2}={nan_train_before2:.3f}\")\n",
    "            return None\n",
    "        \n",
    "        # åˆå¹¶å› å­ (æ³¨æ„ï¼šcombines_dataç›´æ¥ä¿®æ”¹å¹¶è¿”å›æ•´ä¸ªDataFrame)\n",
    "        factor_train, merged_col_name = combines_data(factor_train, c.factor, c.factor2, c.operation)\n",
    "        factor_test, _ = combines_data(factor_test, c.factor, c.factor2, c.operation)\n",
    "        factor_full, _ = combines_data(factor_full, c.factor, c.factor2, c.operation)\n",
    "        \n",
    "        # æ›´æ–°å› å­åç§°\n",
    "        factor_name = merged_col_name\n",
    "        \n",
    "        # æ£€æŸ¥åˆå¹¶åçš„NaNæƒ…å†µ (ç”±äºcombines_dataå·²ç»åˆ é™¤äº†NaNï¼Œæ‰€ä»¥è¿™é‡Œä¸»è¦æ˜¯è®°å½•æœ‰å¤šå°‘æ•°æ®è¢«åˆ é™¤)\n",
    "        print(f\"Data remaining after combination: Train: {len(factor_train)}/{factor_train_length} ({len(factor_train)/factor_train_length:.1%})\")\n",
    "        print(f\"Data remaining after combination: Test: {len(factor_test)}/{factor_test_length} ({len(factor_test)/factor_test_length:.1%})\")\n",
    "        print(f\"Data remaining after combination: Full: {len(factor_full)}/{factor_full_length} ({len(factor_full)/factor_full_length:.1%})\")\n",
    "        \n",
    "        # å¦‚æœå‰©ä½™æ•°æ®å¤ªå°‘ï¼Œç»ˆæ­¢å›æµ‹    # é€™å€‹æ˜¯åœ¨ç”¨å·²ç¶“dropå®Œçš„factor_dfä¾†check, æ‰€ä»¥æ˜¯æª¢æŸ¥ (1-nan%) * åŸå§‹df length\n",
    "        if len(factor_train) < (1 - c.nan_perc) * factor_train_length:\n",
    "            print(\"Train data After droped more than 3% data. Skipping backtest.\")\n",
    "            return None\n",
    "        if len(factor_test) < (1 - c.nan_perc) * factor_test_length:\n",
    "            print(\"Test data After droped more than 3% data. Skipping backtest\")\n",
    "            return None\n",
    "        if len(factor_full) < (1 - c.nan_perc) * factor_full_length:\n",
    "            print(\"Full data After droped more than 3% data. Skipping backtest\")\n",
    "            return None\n",
    "            \n",
    "    # 2. å› å­é¢„å¤„ç†\n",
    "    if c.preprocess != \"direct\":\n",
    "        print(f\"\\nApplying preprocessing method '{c.preprocess}'...\")\n",
    "        \n",
    "        # åŸå§‹æ•°æ®é•¿åº¦æ›´æ–°\n",
    "        factor_train_length = len(factor_train)\n",
    "        factor_test_length = len(factor_test)\n",
    "        factor_full_length = len(factor_full)\n",
    "\n",
    "        print(f\"length of each split: Train:{factor_train_length}, Test:{factor_test_length}, Full:{factor_full_length}\")\n",
    "        \n",
    "        # --- è®­ç»ƒé›†é¢„å¤„ç† ---\n",
    "        factor_train, train_factor_name = data_processing(factor_train, c.preprocess, factor_name)\n",
    "        train_remaining = len(factor_train) / factor_train_length\n",
    "        \n",
    "        # --- æµ‹è¯•é›†é¢„å¤„ç† ---\n",
    "        factor_test, test_factor_name = data_processing(factor_test, c.preprocess, factor_name)\n",
    "        test_remaining = len(factor_test) / factor_test_length if factor_test_length > 0 else 1.0\n",
    "        \n",
    "        # --- å…¨é›†é¢„å¤„ç† ---\n",
    "        factor_full, full_factor_name = data_processing(factor_full, c.preprocess, factor_name)\n",
    "        full_remaining = len(factor_full) / factor_full_length\n",
    "        \n",
    "        # æ£€æŸ¥æ•°æ®ä¿ç•™æ¯”ä¾‹ï¼ˆè¶…è¿‡3%çš„NaNä¼šè¢«åˆ é™¤ï¼‰\n",
    "        nan_train = 1 - train_remaining\n",
    "        nan_test = 1 - test_remaining\n",
    "        nan_full = 1 - full_remaining\n",
    "        \n",
    "        print(f\"After {c.preprocess} preprocessing data retention: Train: {train_remaining:.3f}, Test: {test_remaining:.3f}, Full: {full_remaining:.3f}\")\n",
    "        print(f\"NaN percentages: Train: {nan_train:.3f}, Test: {nan_test:.3f}, Full: {nan_full:.3f}\")\n",
    "        \n",
    "        if nan_train > c.nan_perc:\n",
    "            print(f\"Train NaN% = {nan_train}, >3% after {c.preprocess} preprocessing. Skipping backtest.\")\n",
    "            return None\n",
    "        \n",
    "        if nan_test > c.nan_perc:\n",
    "            print(f\"Test NaN% = {nan_test}, >3% after {c.preprocess} preprocessing. Skipping backtest.\")\n",
    "            return None\n",
    "        \n",
    "        if nan_full > c.nan_perc:\n",
    "            print(f\"Full NaN% {nan_full}, >3% after {c.preprocess} preprocessing. Skipping backtest.\")\n",
    "            return None\n",
    "\n",
    "        # æ›´æ–°å› å­åç§°\n",
    "        factor_name = train_factor_name\n",
    "    \n",
    "    # 4. è®¾ç½®æœ€ç»ˆçš„æ—¶é—´ç´¢å¼•\n",
    "    candle_train.set_index('time', inplace=True)\n",
    "    factor_train.set_index('time', inplace=True)\n",
    "    candle_test.set_index('time', inplace=True)\n",
    "    factor_test.set_index('time', inplace=True)\n",
    "    candle_full.set_index('time', inplace=True)\n",
    "    factor_full.set_index('time', inplace=True)\n",
    "    \n",
    "    # åªä¿ç•™éœ€è¦çš„åˆ—\n",
    "    candle_train = candle_train[['start_time', 'Close']]\n",
    "    candle_test = candle_test[['start_time', 'Close']]\n",
    "    candle_full = candle_full[['start_time', 'Close']]\n",
    "    \n",
    "    factor_train = factor_train[['start_time', factor_name]]\n",
    "    factor_test = factor_test[['start_time', factor_name]]\n",
    "    factor_full = factor_full[['start_time', factor_name]]\n",
    "    \n",
    "    # 5. é‡å‘½åCloseåˆ—ä¸ºå°å†™close(å¦‚æœéœ€è¦)\n",
    "    candle_train.columns = ['start_time', 'close']\n",
    "    candle_test.columns = ['start_time', 'close']\n",
    "    candle_full.columns = ['start_time', 'close']\n",
    "    \n",
    "    # 6. æ‰“å°å¤„ç†åçš„æ•°æ®å¤§å°\n",
    "    print(f\"Final data sizes after processing:\")\n",
    "    print(f\"Train: candle={len(candle_train)}, factor={len(factor_train)}, original={factor_train_length}\")\n",
    "    print(f\"Test: candle={len(candle_test)}, factor={len(factor_test)}, original={factor_test_length}\")\n",
    "    print(f\"Full: candle={len(candle_full)}, factor={len(factor_full)}, original={factor_full_length}\")\n",
    "    \n",
    "    # è¿”å›æœ€ç»ˆç»“æœ\n",
    "    return {\n",
    "        'train': {'candle': candle_train, 'factor': factor_train, 'date_range': train_date_range},\n",
    "        'test': {'candle': candle_test, 'factor': factor_test, 'date_range': test_date_range},\n",
    "        'full': {'candle': candle_full, 'factor': factor_full, 'date_range': full_date_range},\n",
    "        'factor_name': factor_name\n",
    "    }\n",
    "\n",
    "def parse_manual_selection(filepath):\n",
    "    \"\"\"\n",
    "    è§£æmanual_selected.txtæ–‡ä»¶ï¼Œè¿”å›å‚æ•°å…ƒç»„åˆ—è¡¨[(model, entry, window, threshold)]\n",
    "    ä½¿ç”¨config.pyä¸­çš„modelsåˆ—è¡¨æ¥è¯†åˆ«æ¨¡å‹å’Œæ¡ç›®\n",
    "    å¦‚æœæ–‡ä»¶æ ¼å¼ä¸å®Œæ•´æˆ–ä¸å­˜åœ¨ï¼Œè¿”å›ç©ºåˆ—è¡¨\n",
    "    \"\"\"\n",
    "    if not os.path.exists(filepath):\n",
    "        print(f\"Warning: {filepath} not found\")\n",
    "        return []\n",
    "        \n",
    "    try:        \n",
    "        with open(filepath, \"r\") as file:\n",
    "            lines = [line.strip() for line in file.readlines() if line.strip()]\n",
    "        \n",
    "        params_list = []\n",
    "        current_model = None\n",
    "        \n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "                \n",
    "            # æ£€æŸ¥å½“å‰è¡Œæ˜¯å¦æ˜¯æ¨¡å‹åç§°\n",
    "            if line in c.models:\n",
    "                current_model = line\n",
    "                continue\n",
    "            \n",
    "            # å¦‚æœæ²¡æœ‰å½“å‰æ¨¡å‹ï¼Œåˆ™è·³è¿‡\n",
    "            if current_model is None:\n",
    "                print(f\"Warning: Entry '{line}' found without a model specified\")\n",
    "                continue\n",
    "                \n",
    "            # è§£ææ¡ç›®å’Œå‚æ•°\n",
    "            parts = line.split(maxsplit=1)\n",
    "            if len(parts) < 1:\n",
    "                continue\n",
    "                \n",
    "            entry = parts[0]\n",
    "            \n",
    "            if len(parts) > 1:\n",
    "                params_str = parts[1]\n",
    "                params_entries = params_str.split(\",\")\n",
    "                for param_entry in params_entries:\n",
    "                    param_entry = param_entry.strip()\n",
    "                    if param_entry:\n",
    "                        try:\n",
    "                            window_str, threshold_str = param_entry.split(\"/\")\n",
    "                            window = float(window_str) if \".\" in window_str else int(window_str)\n",
    "                            threshold = float(threshold_str)\n",
    "                            params_list.append((current_model, entry, window, threshold))\n",
    "                        except ValueError:\n",
    "                            print(f\"Warning: Could not parse parameter {param_entry}\")\n",
    "            else:\n",
    "                # æ²¡æœ‰å‚æ•°ï¼Œä½¿ç”¨é»˜è®¤å€¼\n",
    "                params_list.append((current_model, entry, c.window, c.threshold))\n",
    "    \n",
    "        return params_list\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing manual_selected.txt: {e}\")\n",
    "        return []\n",
    "\n",
    "def backtest_for_pnl(candle_df, factor_df, factor, factor2, interval, operation, preprocess, model,\n",
    "                  entry, window, threshold, backtest_style, date_range=None):\n",
    "    \"\"\"æ‰§è¡Œå›æµ‹å¹¶ç»˜åˆ¶ç»“æœå›¾è¡¨\"\"\"\n",
    "    print(f\"\\n=== Running {backtest_style} ===\")\n",
    "    print(f\"Using model: {model}, entry: {entry}, window: {window}, threshold: {threshold}\")\n",
    "    \n",
    "    # æ£€æŸ¥ç©ºæ•°æ®æ¡†\n",
    "    if candle_df.empty or factor_df.empty:\n",
    "        print(f\"Error: Empty dataframe in {backtest_style}\")\n",
    "        return None\n",
    "        \n",
    "    # æ£€æŸ¥æ•°æ®é•¿åº¦æ˜¯å¦è¶³å¤Ÿ\n",
    "    if len(candle_df) < window or len(factor_df) < window:\n",
    "        print(f\"Error: Insufficient data points (candle: {len(candle_df)}, factor: {len(factor_df)}) compared to window size ({window}) in {backtest_style}\")\n",
    "        return None\n",
    "    \n",
    "    candle_df['time'] = pd.to_datetime(candle_df['start_time'], unit='ms')\n",
    "\n",
    "    # è·å–annualizerå€¼\n",
    "    annualizer = annualizer_dict.get(interval, 365)\n",
    "    \n",
    "    # æ·»åŠ é¢å¤–çš„æŒ‡æ ‡\n",
    "    additional_metric = additional_metrics(c.alpha_id, c.symbol, factor, factor2,\n",
    "                                          operation, c.shift_candle_minite, backtest_style)\n",
    "    \n",
    "    try:\n",
    "        # è¿è¡Œå›æµ‹ - now passing date_range\n",
    "        backtest_result, df, log_msgs = backtest_cached(\n",
    "            candle_df, factor_df, window, threshold, preprocess, \n",
    "            entry, annualizer, model, factor, interval, date_range)\n",
    "        \n",
    "        # æ‰“å°æ—¥å¿—æ¶ˆæ¯\n",
    "        for msg in log_msgs:\n",
    "            print(msg)\n",
    "            \n",
    "        # æ£€æŸ¥dfæ˜¯å¦ä¸ºç©º\n",
    "        if df is None or df.empty:\n",
    "            print(f\"Warning: No results returned from backtest_cached for {backtest_style}\")\n",
    "            return None\n",
    "        \n",
    "        # # æ­£ç¡®å¤„ç†æ—¶é—´æˆ³\n",
    "        if isinstance(df.index, pd.DatetimeIndex):\n",
    "            # ç›´æ¥ä½¿ç”¨DatetimeIndex\n",
    "            start_date = df.index.min().strftime('%Y-%m-%d')\n",
    "            end_date = df.index.max().strftime('%Y-%m-%d')\n",
    "            start_time = df.index.min().strftime('%Y-%m-%d %H:%M:%S')\n",
    "            end_time = df.index.max().strftime('%Y-%m-%d %H:%M:%S')\n",
    "        elif 'start_time' in df.columns:\n",
    "            # ä½¿ç”¨start_timeåˆ—\n",
    "            start_date = pd.to_datetime(df['start_time'].min(), unit='ms').strftime('%Y-%m-%d')\n",
    "            end_date = pd.to_datetime(df['start_time'].max(), unit='ms').strftime('%Y-%m-%d')\n",
    "            start_time = pd.to_datetime(df['start_time'].min(), unit='ms').strftime('%Y-%m-%d %H:%M:%S')\n",
    "            end_time = pd.to_datetime(df['start_time'].max(), unit='ms').strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "        additional_metric.update({\"start_time\": start_time, \"end_time\": end_time})\n",
    "        combined_report = [{**additional_metric, **backtest_result}]\n",
    "\n",
    "        print(f\"{backtest_style} Report:\")\n",
    "        print(json.dumps(combined_report, indent=4))\n",
    "\n",
    "        # ç»˜åˆ¶ç»“æœ\n",
    "        fig, ax1 = plt.subplots(figsize=(15, 8))\n",
    "        ax1.plot(df.index, df['close'], label='Close Price', color='green', linewidth=2)\n",
    "        ax1.set_xlabel(\"Date\", fontsize=12)\n",
    "        ax1.set_ylabel(\"Close Price\", fontsize=12, color='green')\n",
    "        ax1.tick_params(axis='y', labelcolor='green')\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # åœ¨å³ä¾§yè½´ä¸Šç»˜åˆ¶ç´¯ç§¯PnL\n",
    "        ax2 = ax1.twinx()\n",
    "        ax2.plot(df.index, df['cumu_pnl'], label='Cumulative PnL', color='blue', linewidth=2)\n",
    "        ax2.set_ylabel(\"Cumulative PnL\", fontsize=12, color='blue')\n",
    "        ax2.tick_params(axis='y', labelcolor='blue')\n",
    "\n",
    "        plt.title(f\"Close Price and Cumulative PnL Plot (Split {backtest_style})-({start_date} ~ {end_date})\", fontsize=16)\n",
    "        fig.tight_layout()\n",
    "        # plt.show()\n",
    "\n",
    "        if c.save_plot:\n",
    "            plt.savefig(f\"{backtest_style}_Equity_Curve_{start_date}_{end_date}.png\", dpi=300, bbox_inches='tight')\n",
    "            print(f\"å·²å„²å­˜ {backtest_style}_Equity_Curve_{start_date}_{end_date}.png\")\n",
    "            output_backtest_data = {f\"{backtest_style}\": combined_report}\n",
    "            with open(f\"{c.alpha_id}_{backtest_style}.json\", \"w\") as json_file:\n",
    "                json.dump(output_backtest_data, json_file, indent=4)\n",
    "            df.to_csv(f\"{c.alpha_id}_{backtest_style}_df.csv\", index=True)\n",
    "            print(f\"å·²å„²å­˜ {c.alpha_id}_{backtest_style}_df.csv\")\n",
    "        plt.show()\n",
    "        return combined_report\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error in backtest_for_pnl for {backtest_style}: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "def run_backtest(data, model, entry, window, threshold):\n",
    "    \"\"\"è¿è¡Œå®Œæ•´çš„å›æµ‹æµç¨‹ï¼ˆå‰å‘æµ‹è¯•ã€å›æµ‹ã€å…¨æ—¶æ®µå›æµ‹ï¼‰\"\"\"\n",
    "    factor_name = data['factor_name']\n",
    "    \n",
    "    # è®¾ç½®SRé˜ˆå€¼\n",
    "    if \"short\" in entry.lower():\n",
    "        required_sr = 1.0\n",
    "    elif \"long\" in entry.lower():\n",
    "        required_sr = 1.7\n",
    "    else:\n",
    "        required_sr = 1.7\n",
    "    \n",
    "    print(f\"\\næ‰§è¡Œæµ‹è¯•: model = {model}, entry = {entry}, window = {window}, threshold = {threshold}\")\n",
    "    \n",
    "    # æ‰§è¡Œforward test\n",
    "    fwd_report = backtest_for_pnl(\n",
    "        data['test']['candle'],\n",
    "        data['test']['factor'],\n",
    "        factor_name,\n",
    "        c.factor2,\n",
    "        c.interval,\n",
    "        c.operation,\n",
    "        c.preprocess,\n",
    "        model,\n",
    "        entry,\n",
    "        window,\n",
    "        threshold,\n",
    "        \"forwardtest\",\n",
    "        data['test']['date_range'],\n",
    "    )\n",
    "    \n",
    "    # æ£€æŸ¥forward testç»“æœ\n",
    "    if fwd_report is None:\n",
    "        print(\"Warning: forward test failed, skipping this parameter set.\")\n",
    "        return\n",
    "        \n",
    "    # è·å–SRå€¼\n",
    "    fwd_sr = fwd_report[0].get(\"SR\", 0)\n",
    "    \n",
    "    # å¦‚æœSRæœªè¾¾åˆ°è¦æ±‚åˆ™è·³è¿‡åç»­æµ‹è¯•\n",
    "    if fwd_sr < required_sr:\n",
    "        print(f\"Forward test SR = {fwd_sr} æœªè¾¾åˆ°è¦æ±‚ (éœ€ > {required_sr})ï¼Œè·³è¿‡æ­¤ç»„å‚æ•°çš„åç»­æµ‹è¯•ã€‚\")\n",
    "        return\n",
    "        \n",
    "    # å¦‚æœç¬¦åˆSRè¦æ±‚åˆ™ç»§ç»­æ‰§è¡Œå…¶ä»–backtest\n",
    "    backtest_for_pnl(\n",
    "        data['train']['candle'],\n",
    "        data['train']['factor'],\n",
    "        factor_name,\n",
    "        c.factor2,\n",
    "        c.interval,\n",
    "        c.operation,\n",
    "        c.preprocess,\n",
    "        model,\n",
    "        entry,\n",
    "        window,\n",
    "        threshold,\n",
    "        \"backtest\",\n",
    "        data['train']['date_range'],\n",
    "    )\n",
    "    \n",
    "    backtest_for_pnl(\n",
    "        data['full']['candle'],\n",
    "        data['full']['factor'],\n",
    "        factor_name,\n",
    "        c.factor2,\n",
    "        c.interval,\n",
    "        c.operation,\n",
    "        c.preprocess,\n",
    "        model,\n",
    "        entry,\n",
    "        window,\n",
    "        threshold,\n",
    "        \"full_time_backtest\",\n",
    "        data['full']['date_range'],\n",
    "    )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # ä¸€æ¬¡æ€§åŠ è½½å’Œå¤„ç†æ•°æ®\n",
    "    data = load_and_prepare_data()\n",
    "    if data is None:\n",
    "        print(\"Error preparing data. Aborting backtest.\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    # å°è¯•ä»manual_selected.txtè¯»å–å‚æ•°åˆ—è¡¨\n",
    "    params_list = parse_manual_selection(\"manual_selected.txt\")\n",
    "    \n",
    "    if params_list:\n",
    "        print(f\"Found {len(params_list)} parameter sets in manual_selected.txt\")\n",
    "        # ä½¿ç”¨æ¯ç»„å‚æ•°è¿è¡Œå›æµ‹\n",
    "        for model, entry, window, threshold in params_list:\n",
    "            run_backtest(data, model, entry, window, threshold)\n",
    "    else:\n",
    "        # å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨æˆ–å‚æ•°ä¸é½å…¨ï¼Œä½¿ç”¨configä¸­çš„å‚æ•°\n",
    "        print(\"Using parameters from config.py\")\n",
    "        run_backtest(data, c.model, c.entry, c.window, c.threshold)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cybotrade",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
