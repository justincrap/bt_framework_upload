{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 每次运行时重新加载config.py\n",
    "from importlib import reload\n",
    "import config as c\n",
    "reload(c)\n",
    "\n",
    "print(f\"已加载配置：factor={c.factor}, factor2={c.factor2}, operation={c.operation}, preprocess={c.preprocess}, USE_ALL_MODELS={c.USE_ALL_MODELS}\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resampling 1min data to wanted timeframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import config as c\n",
    "\n",
    "def prepare_price_data(\n",
    "    csv_path: str,  # 輸入 CSV 檔案完整路徑\n",
    "    datasource: str = 'bybit_btcusdt',\n",
    "    factor: str = 'price',\n",
    "    timeframe: str = '1D', \n",
    "    delay_minutes: int = 0\n",
    "):\n",
    "    \"\"\"\n",
    "    讀取 1m 資料，轉成指定的時間週期 (timeframe)，\n",
    "    可選擇延遲(正值)或提前(負值)時間索引，並自動存檔到當前工作目錄。\n",
    "\n",
    "    參數：\n",
    "      csv_path      : 【完整路徑】輸入的 1m 級別 CSV 檔案\n",
    "      datasource    : 資料來源名稱 (如 bybit_btcusdt)\n",
    "      factor        : 影響因子名稱 (如 price)\n",
    "      timeframe     : 轉換後的時間週期，如 '1H'、'1D' 等 (預設 '1D')\n",
    "      delay_minutes : 時間平移的分鐘數 (正值 = 延後；負值 = 提前)\n",
    "\n",
    "    回傳：\n",
    "      pandas DataFrame (resampled 後的結果)，\n",
    "      並將結果輸出為 CSV，命名格式：\n",
    "      {datasource}_{factor}_{timeframe}_{start_time}_{end_time}.csv\n",
    "    \"\"\"\n",
    "    # 1. 讀取 CSV，解析時間\n",
    "    df = pd.read_csv(\n",
    "        csv_path, \n",
    "        parse_dates=['Time']  # pandas 會自動解析時間格式\n",
    "    )\n",
    "\n",
    "    # 2. 將 'Time' 欄設為索引\n",
    "    df.set_index('Time', inplace=True)\n",
    "\n",
    "    # 3. 時間平移 (延遲 / 提前)\n",
    "    if delay_minutes != 0:\n",
    "        df.index = df.index + pd.Timedelta(minutes=delay_minutes)\n",
    "\n",
    "    # 4. 定義 resample 聚合方式\n",
    "    if c.candle_exchange == 'bybit':\n",
    "        ohlc_dict = {\n",
    "            'Open': 'first',\n",
    "            'High': 'max',\n",
    "            'Low': 'min',\n",
    "            'Close': 'last',\n",
    "            'Volume': 'sum',\n",
    "            'Turnover': 'sum'\n",
    "        }\n",
    "    else:\n",
    "        ohlc_dict = {\n",
    "        'Open': 'first',\n",
    "        'High': 'max',\n",
    "        'Low': 'min',\n",
    "        'Close': 'last',\n",
    "        'Volume': 'sum',\n",
    "    }\n",
    "    \n",
    "    # 5. 進行 resample\n",
    "    df_resampled = df.resample(timeframe).agg(ohlc_dict).dropna(how='any')\n",
    "\n",
    "    # Use Time to create one more column named 'start_time' that is in unix timestamp\n",
    "    df_resampled['start_time'] = df_resampled.index.astype('int64') // 10**6\n",
    "    # df_resampled['start_time'] = df_resampled['start_time'].astype('float64')\n",
    "\n",
    "    # 6. 獲取開始與結束時間 (格式 YYYY-MM-DD)\n",
    "    if not df_resampled.empty:\n",
    "        start_time = df_resampled.index[0].strftime('%Y-%m-%d')\n",
    "        end_time = df_resampled.index[-1].strftime('%Y-%m-%d')\n",
    "\n",
    "        # 7. 構建輸出檔案名稱\n",
    "        output_filename = f\"./data/resample_{datasource}_{timeframe}_-{c.shift_candle_minite}m.csv\"\n",
    "        output_path = os.path.join(os.getcwd(), output_filename)  # 當前工作目錄\n",
    "\n",
    "        # 8. 輸出 CSV\n",
    "        df_resampled.to_csv(output_path)\n",
    "        print(f\"✅ 檔案已儲存：{output_path}\")\n",
    "    else:\n",
    "        print(\"⚠️ Resampled DataFrame 為空，未產生輸出檔案！\")\n",
    "\n",
    "    return df_resampled\n",
    "\n",
    "df_r = prepare_price_data(\n",
    "    csv_path=f\"./data/{c.candle_exchange}_{c.symbol.lower()}usdt_price_1m.csv\",\n",
    "    datasource=f'{c.candle_exchange}_{c.symbol.lower()}',\n",
    "    factor='price',\n",
    "    timeframe=c.interval,\n",
    "    delay_minutes=-c.shift_candle_minite\n",
    ")\n",
    "\n",
    "print(df_r.head()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilsnumpy import load_all_data, combines_data, data_processing\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import config as c\n",
    "from datetime import datetime\n",
    "\n",
    "def visualize_factors():\n",
    "    \"\"\"\n",
    "    加載並視覺化因子數據，支持因子組合和預處理\n",
    "    \n",
    "    Returns:\n",
    "        dict: 包含蠟燭圖數據、因子數據和因子名稱的字典\n",
    "    \"\"\"\n",
    "    print(\"Loading data...\")\n",
    "    # 加載原始數據\n",
    "    raw_candle, raw_factor = load_all_data(c.candle_file, c.factor_file, c.factor2_file, c.factor, c.factor2)\n",
    "    \n",
    "    # 處理時間格式以便於繪圖\n",
    "    raw_factor['time'] = pd.to_datetime(raw_factor['start_time'], unit='ms')\n",
    "    raw_candle['time'] = pd.to_datetime(raw_candle['start_time'], unit='ms')\n",
    "    \n",
    "    # 獲取時間範圍信息用於標題\n",
    "    start_date = raw_candle['time'].min().strftime('%Y-%m-%d')\n",
    "    end_date = raw_candle['time'].max().strftime('%Y-%m-%d')\n",
    "    date_range = f\"{start_date} to {end_date}\"\n",
    "    \n",
    "    # 初始化繪圖\n",
    "    fig, axes = plt.subplots(3, 1, figsize=(15, 18), sharex=True, dpi=100)\n",
    "    \n",
    "    # 繪製價格數據\n",
    "    axes[0].plot(raw_candle['time'], raw_candle['Close'], color='#1f77b4', linewidth=1.5)\n",
    "    axes[0].set_title(f'Price Data: {c.symbol.upper()}', fontsize=16, fontweight='bold')\n",
    "    axes[0].set_ylabel('Price (USD)', fontsize=14)\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 添加價格變化百分比\n",
    "    price_change = (raw_candle['Close'].iloc[-1] - raw_candle['Close'].iloc[0]) / raw_candle['Close'].iloc[0] * 100\n",
    "    price_text = f\"Price Change: {price_change:.2f}%\"\n",
    "    axes[0].text(0.99, 0.05, price_text, transform=axes[0].transAxes, \n",
    "                ha='right', fontsize=12, \n",
    "                bbox=dict(boxstyle='round,pad=0.5', facecolor='white', alpha=0.8))\n",
    "    \n",
    "    # 繪製原始因子數據\n",
    "    axes[1].plot(raw_factor['time'], raw_factor[c.factor], color='#2ca02c', linewidth=1.5, label=c.factor)\n",
    "    if c.factor2 in raw_factor.columns:\n",
    "        axes[1].plot(raw_factor['time'], raw_factor[c.factor2], color='#d62728', linewidth=1.5, label=c.factor2)\n",
    "    axes[1].set_title('Raw Factor Data', fontsize=16, fontweight='bold')\n",
    "    axes[1].set_ylabel('Factor Value', fontsize=14)\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    axes[1].legend(loc='best', frameon=True, framealpha=0.8)\n",
    "    \n",
    "    # 處理組合因子和預處理\n",
    "    factor_data = raw_factor.copy()\n",
    "    factor_to_display = c.factor\n",
    "    factor_label = c.factor\n",
    "    \n",
    "    # 如果需要組合因子\n",
    "    if c.operation != 'none' and c.factor2 in factor_data.columns:\n",
    "        print(f\"Combining factors with operation: {c.operation}\")\n",
    "        factor_data, combined_name = combines_data(\n",
    "            factor_data,\n",
    "            c.factor,\n",
    "            c.factor2,\n",
    "            c.operation\n",
    "        )\n",
    "        factor_to_display = combined_name\n",
    "        factor_label = f\"{c.factor} {c.operation} {c.factor2}\"\n",
    "    \n",
    "    # 如果需要預處理\n",
    "    if c.preprocess != \"direct\":\n",
    "        print(f\"Applying preprocessing: {c.preprocess}\")\n",
    "        # 注意：從paste.txt的代碼來看，這裡可能有一個問題\n",
    "        # data_processing在其他部分可能返回兩個值，但這裡只捕獲了一個\n",
    "        try:\n",
    "            # 嘗試新版API（返回DataFrame和名稱）\n",
    "            processed_df, new_factor_name = data_processing(factor_data, c.preprocess, factor_to_display)\n",
    "            factor_data = processed_df\n",
    "            factor_to_display = new_factor_name\n",
    "        except ValueError:\n",
    "            # 舊版API（直接返回處理後的值）\n",
    "            processed_values = data_processing(factor_data[factor_to_display], c.preprocess, factor_to_display)\n",
    "            factor_data[f\"{factor_to_display}_{c.preprocess}\"] = processed_values\n",
    "            factor_to_display = f\"{factor_to_display}_{c.preprocess}\"\n",
    "        \n",
    "        factor_label = f\"{factor_label} ({c.preprocess})\"\n",
    "    \n",
    "    # 繪製處理後的因子數據\n",
    "    axes[2].plot(factor_data['time'], factor_data[factor_to_display], color='#9467bd', linewidth=1.5)\n",
    "    axes[2].set_title('Processed Factor Data', fontsize=16, fontweight='bold')\n",
    "    axes[2].set_ylabel('Factor Value', fontsize=14)\n",
    "    axes[2].set_xlabel('Date', fontsize=14)\n",
    "    axes[2].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 在標題旁添加因子描述\n",
    "    axes[2].text(0.01, 0.95, f\"Factor: {factor_label}\", transform=axes[2].transAxes, \n",
    "                 fontsize=12, verticalalignment='top', \n",
    "                 bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "    \n",
    "    # 添加數據範圍\n",
    "    factor_data_description = f\"\"\"\n",
    "    Data points: {len(factor_data)}\n",
    "    Date range: {date_range}\n",
    "    \"\"\"\n",
    "    axes[2].text(0.99, 0.05, factor_data_description, transform=axes[2].transAxes,\n",
    "                ha='right', fontsize=10, va='bottom',\n",
    "                bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "    \n",
    "    # 格式化X軸日期\n",
    "    for ax in axes:\n",
    "        ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n",
    "        ax.xaxis.set_major_locator(mdates.MonthLocator(interval=3))\n",
    "        plt.setp(ax.xaxis.get_majorticklabels(), rotation=45)\n",
    "    \n",
    "    # 添加整體標題\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M\")\n",
    "    plt.suptitle(f'Factor Analysis - {c.symbol.upper()} ({timestamp})', fontsize=20, y=0.98)\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.97])\n",
    "    \n",
    "    # 保存圖像\n",
    "    if hasattr(c, 'save_plot') and c.save_plot:\n",
    "        filename = f\"factor_analysis_{c.symbol}_{c.factor}_{datetime.now().strftime('%Y%m%d')}.png\"\n",
    "        plt.savefig(filename, dpi=300, bbox_inches='tight')\n",
    "        print(f\"圖像已保存為: {filename}\")\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    # 返回處理後的數據，以便進一步分析\n",
    "    return {\n",
    "        'candle': raw_candle,\n",
    "        'factor': factor_data,\n",
    "        'factor_name': factor_to_display\n",
    "    }\n",
    "\n",
    "def visualize_factor_correlation(data, n_bins=20):\n",
    "    \"\"\"\n",
    "    視覺化因子與價格的相關性\n",
    "    \n",
    "    Args:\n",
    "        data: 由visualize_factors函數返回的數據字典\n",
    "        n_bins: 散點圖顏色分組數量，用於密度顯示\n",
    "    \"\"\"\n",
    "    if data is None:\n",
    "        print(\"沒有數據可用於相關性分析\")\n",
    "        return\n",
    "    \n",
    "    candle_df = data['candle']\n",
    "    factor_df = data['factor']\n",
    "    factor_name = data['factor_name']\n",
    "    \n",
    "    # 合併價格和因子數據\n",
    "    merged_df = pd.merge_asof(\n",
    "        candle_df.sort_values('time'), \n",
    "        factor_df[['time', factor_name]].sort_values('time'),\n",
    "        on='time',\n",
    "        direction='nearest'\n",
    "    )\n",
    "    \n",
    "    # 計算不同的價格變化率\n",
    "    merged_df['price_change_1d'] = merged_df['Close'].pct_change()\n",
    "    \n",
    "    # 如果時間間隔允許，也計算5日和10日變化率\n",
    "    if len(merged_df) > 10:\n",
    "        merged_df['price_change_5d'] = merged_df['Close'].pct_change(5)\n",
    "        merged_df['price_change_10d'] = merged_df['Close'].pct_change(10)\n",
    "    \n",
    "    # 去除NaN值\n",
    "    merged_df = merged_df.dropna()\n",
    "    \n",
    "    # 多個時間框架的相關係數\n",
    "    correlation_1d = merged_df['price_change_1d'].corr(merged_df[factor_name])\n",
    "    corr_results = [f\"1-day: {correlation_1d:.4f}\"]\n",
    "    \n",
    "    if 'price_change_5d' in merged_df.columns:\n",
    "        correlation_5d = merged_df['price_change_5d'].corr(merged_df[factor_name])\n",
    "        corr_results.append(f\"5-day: {correlation_5d:.4f}\")\n",
    "    \n",
    "    if 'price_change_10d' in merged_df.columns:\n",
    "        correlation_10d = merged_df['price_change_10d'].corr(merged_df[factor_name])\n",
    "        corr_results.append(f\"10-day: {correlation_10d:.4f}\")\n",
    "    \n",
    "    # 創建一個2x2網格圖表用於多個相關性視圖\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "    plt.subplots_adjust(hspace=0.3, wspace=0.3)\n",
    "    \n",
    "    # 基本散點圖（左上）\n",
    "    axes[0, 0].scatter(merged_df[factor_name], merged_df['price_change_1d'], \n",
    "                      alpha=0.6, c='#1f77b4', edgecolors='none')\n",
    "    axes[0, 0].axhline(y=0, color='r', linestyle='-', alpha=0.3)\n",
    "    axes[0, 0].axvline(x=0, color='r', linestyle='-', alpha=0.3)\n",
    "    \n",
    "    # 添加趨勢線\n",
    "    z = np.polyfit(merged_df[factor_name], merged_df['price_change_1d'], 1)\n",
    "    p = np.poly1d(z)\n",
    "    x_range = np.linspace(merged_df[factor_name].min(), merged_df[factor_name].max(), 100)\n",
    "    axes[0, 0].plot(x_range, p(x_range), \"r--\", alpha=0.8)\n",
    "    \n",
    "    axes[0, 0].set_title(f'Price Change vs {factor_name}\\nCorrelation: {correlation_1d:.4f}', fontsize=14)\n",
    "    axes[0, 0].set_xlabel(factor_name, fontsize=12)\n",
    "    axes[0, 0].set_ylabel('1-Day Price Change (%)', fontsize=12)\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 核密度散點圖（右上）- 使用顏色表示點的密度\n",
    "    from scipy.stats import gaussian_kde\n",
    "    \n",
    "    # 計算2D密度\n",
    "    xy = np.vstack([merged_df[factor_name], merged_df['price_change_1d']])\n",
    "    try:\n",
    "        z = gaussian_kde(xy)(xy)\n",
    "        # 根據密度排序點，這樣密度高的點會出現在頂部\n",
    "        idx = z.argsort()\n",
    "        x, y, z = merged_df[factor_name].iloc[idx], merged_df['price_change_1d'].iloc[idx], z[idx]\n",
    "        scatter = axes[0, 1].scatter(x, y, c=z, cmap='viridis', \n",
    "                                   edgecolor='none', alpha=0.8, s=30)\n",
    "        plt.colorbar(scatter, ax=axes[0, 1], label='Density')\n",
    "    except Exception as e:\n",
    "        print(f\"無法創建密度圖: {e}\")\n",
    "        axes[0, 1].scatter(merged_df[factor_name], merged_df['price_change_1d'], \n",
    "                          alpha=0.6, c='#2ca02c', edgecolors='none')\n",
    "    \n",
    "    axes[0, 1].axhline(y=0, color='r', linestyle='-', alpha=0.3)\n",
    "    axes[0, 1].axvline(x=0, color='r', linestyle='-', alpha=0.3)\n",
    "    axes[0, 1].set_title('Density Plot', fontsize=14)\n",
    "    axes[0, 1].set_xlabel(factor_name, fontsize=12)\n",
    "    axes[0, 1].set_ylabel('1-Day Price Change (%)', fontsize=12)\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 二維直方圖（左下）\n",
    "    h = axes[1, 0].hist2d(merged_df[factor_name], merged_df['price_change_1d'], \n",
    "                         bins=n_bins, cmap='Blues')\n",
    "    plt.colorbar(h[3], ax=axes[1, 0], label='Count')\n",
    "    axes[1, 0].set_title('2D Histogram', fontsize=14)\n",
    "    axes[1, 0].set_xlabel(factor_name, fontsize=12)\n",
    "    axes[1, 0].set_ylabel('1-Day Price Change (%)', fontsize=12)\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 多時間框架相關性（右下）\n",
    "    if 'price_change_5d' in merged_df.columns and 'price_change_10d' in merged_df.columns:\n",
    "        correlations = [\n",
    "            correlation_1d,\n",
    "            correlation_5d,\n",
    "            correlation_10d\n",
    "        ]\n",
    "        labels = ['1-Day', '5-Day', '10-Day']\n",
    "        colors = ['#1f77b4', '#ff7f0e', '#2ca02c']\n",
    "        \n",
    "        axes[1, 1].bar(labels, correlations, color=colors, alpha=0.7)\n",
    "        axes[1, 1].axhline(y=0, color='r', linestyle='-', alpha=0.3)\n",
    "        axes[1, 1].set_title('Correlation Across Timeframes', fontsize=14)\n",
    "        axes[1, 1].set_ylabel('Correlation Coefficient', fontsize=12)\n",
    "        axes[1, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # 添加相關性值作為標籤\n",
    "        for i, v in enumerate(correlations):\n",
    "            axes[1, 1].text(i, v + 0.02 if v >= 0 else v - 0.08,\n",
    "                          f\"{v:.4f}\", ha='center', fontsize=10)\n",
    "    else:\n",
    "        # 如果數據不足，只顯示因子分佈\n",
    "        axes[1, 1].hist(merged_df[factor_name], bins=30, alpha=0.7, color='#1f77b4')\n",
    "        axes[1, 1].set_title(f'Distribution of {factor_name}', fontsize=14)\n",
    "        axes[1, 1].set_xlabel(factor_name, fontsize=12)\n",
    "        axes[1, 1].set_ylabel('Frequency', fontsize=12)\n",
    "        axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 添加超級標題\n",
    "    plt.suptitle(f'Correlation Analysis: {c.symbol.upper()} vs {factor_name}', \n",
    "                fontsize=16, y=0.98, fontweight='bold')\n",
    "    \n",
    "    # 保存圖像\n",
    "    if hasattr(c, 'save_plot') and c.save_plot:\n",
    "        filename = f\"correlation_{c.symbol}_{factor_name}_{datetime.now().strftime('%Y%m%d')}.png\"\n",
    "        plt.savefig(filename, dpi=300, bbox_inches='tight')\n",
    "        print(f\"圖像已保存為: {filename}\")\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    # 返回相關係數結果\n",
    "    return {\n",
    "        'correlations': corr_results,\n",
    "        'merged_data': merged_df\n",
    "    }\n",
    "\n",
    "def visualize_factor_distribution(data):\n",
    "    \"\"\"\n",
    "    視覺化因子值的分佈\n",
    "    \n",
    "    Args:\n",
    "        data: 由visualize_factors函數返回的數據字典\n",
    "    \"\"\"\n",
    "    if data is None:\n",
    "        print(\"沒有數據可用於分佈分析\")\n",
    "        return\n",
    "    \n",
    "    factor_df = data['factor']\n",
    "    factor_name = data['factor_name']\n",
    "    \n",
    "    # 去除NaN值\n",
    "    factor_values = factor_df[factor_name].dropna().values\n",
    "    \n",
    "    # 創建2x2網格圖表用於多種分佈視圖\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "    plt.subplots_adjust(hspace=0.3, wspace=0.3)\n",
    "    \n",
    "    # 1. 基本直方圖（左上）\n",
    "    n, bins, patches = axes[0, 0].hist(factor_values, bins=50, alpha=0.7, color='#1f77b4', \n",
    "                                      edgecolor='black', linewidth=0.5)\n",
    "    \n",
    "    # 添加基本統計信息\n",
    "    mean_val = np.mean(factor_values)\n",
    "    median_val = np.median(factor_values)\n",
    "    std_val = np.std(factor_values)\n",
    "    \n",
    "    axes[0, 0].axvline(mean_val, color='r', linestyle='dashed', linewidth=1.5, label=f'Mean: {mean_val:.4f}')\n",
    "    axes[0, 0].axvline(median_val, color='g', linestyle='dashed', linewidth=1.5, label=f'Median: {median_val:.4f}')\n",
    "    \n",
    "    axes[0, 0].set_title('Histogram', fontsize=14)\n",
    "    axes[0, 0].set_xlabel(factor_name, fontsize=12)\n",
    "    axes[0, 0].set_ylabel('Frequency', fontsize=12)\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    axes[0, 0].legend()\n",
    "    \n",
    "    # 2. 核密度估計（右上）\n",
    "    try:\n",
    "        from scipy.stats import gaussian_kde\n",
    "        density = gaussian_kde(factor_values)\n",
    "        x = np.linspace(min(factor_values), max(factor_values), 1000)\n",
    "        axes[0, 1].plot(x, density(x), 'r-', linewidth=2)\n",
    "        axes[0, 1].fill_between(x, density(x), alpha=0.3, color='#ff7f0e')\n",
    "        axes[0, 1].set_title('Kernel Density Estimation', fontsize=14)\n",
    "    except Exception as e:\n",
    "        print(f\"無法創建密度圖: {e}\")\n",
    "        axes[0, 1].hist(factor_values, bins=50, alpha=0.7, color='#ff7f0e', \n",
    "                      density=True, edgecolor='black', linewidth=0.5)\n",
    "        axes[0, 1].set_title('Normalized Histogram', fontsize=14)\n",
    "    \n",
    "    axes[0, 1].axvline(mean_val, color='r', linestyle='dashed', linewidth=1.5)\n",
    "    axes[0, 1].axvline(median_val, color='g', linestyle='dashed', linewidth=1.5)\n",
    "    axes[0, 1].set_xlabel(factor_name, fontsize=12)\n",
    "    axes[0, 1].set_ylabel('Density', fontsize=12)\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. 箱型圖（左下）\n",
    "    axes[1, 0].boxplot(factor_values, vert=False, showmeans=True, \n",
    "                      meanprops={'marker':'o', 'markerfacecolor':'red', 'markeredgecolor':'red'},\n",
    "                      flierprops={'marker':'x', 'markerfacecolor':'red', 'markeredgecolor':'red'})\n",
    "    axes[1, 0].set_title('Boxplot', fontsize=14)\n",
    "    axes[1, 0].set_xlabel(factor_name, fontsize=12)\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. ECDF（右下）- 經驗累積分佈函數\n",
    "    sorted_data = np.sort(factor_values)\n",
    "    ecdf = np.arange(1, len(sorted_data) + 1) / len(sorted_data)\n",
    "    axes[1, 1].step(sorted_data, ecdf, linewidth=2, color='#2ca02c')\n",
    "    \n",
    "    # 繪製關鍵百分位數的垂直線\n",
    "    percentiles = [25, 50, 75]\n",
    "    colors = ['#ff9896', '#9467bd', '#8c564b']\n",
    "    \n",
    "    for p, color in zip(percentiles, colors):\n",
    "        percentile_val = np.percentile(factor_values, p)\n",
    "        axes[1, 1].axvline(percentile_val, color=color, linestyle='--', \n",
    "                          label=f'{p}th percentile: {percentile_val:.4f}')\n",
    "    \n",
    "    axes[1, 1].set_title('Empirical Cumulative Distribution', fontsize=14)\n",
    "    axes[1, 1].set_xlabel(factor_name, fontsize=12)\n",
    "    axes[1, 1].set_ylabel('Cumulative Probability', fontsize=12)\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    axes[1, 1].legend(loc='lower right')\n",
    "    \n",
    "    # 添加文本框顯示統計信息\n",
    "    stats_text = f\"\"\"\n",
    "    Mean: {mean_val:.4f}\n",
    "    Median: {median_val:.4f}\n",
    "    Std Dev: {std_val:.4f}\n",
    "    Skewness: {np.percentile(factor_values, 75) - 2*np.percentile(factor_values, 50) + np.percentile(factor_values, 25):.4f}\n",
    "    Min: {np.min(factor_values):.4f}\n",
    "    Max: {np.max(factor_values):.4f}\n",
    "    Count: {len(factor_values)}\n",
    "    \"\"\"\n",
    "    \n",
    "    # 放在箱型圖上方\n",
    "    axes[1, 0].text(0.05, 0.95, stats_text, transform=axes[1, 0].transAxes, \n",
    "                  fontsize=10, verticalalignment='top', \n",
    "                  bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "    \n",
    "    # 添加超級標題\n",
    "    plt.suptitle(f'Distribution Analysis: {factor_name}', \n",
    "                fontsize=16, y=0.98, fontweight='bold')\n",
    "    \n",
    "    # 保存圖像\n",
    "    if hasattr(c, 'save_plot') and c.save_plot:\n",
    "        filename = f\"distribution_{c.symbol}_{factor_name}_{datetime.now().strftime('%Y%m%d')}.png\"\n",
    "        plt.savefig(filename, dpi=300, bbox_inches='tight')\n",
    "        print(f\"圖像已保存為: {filename}\")\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    # 計算並返回更多統計信息\n",
    "    from scipy import stats\n",
    "    \n",
    "    try:\n",
    "        # 嘗試計算高級統計量\n",
    "        factor_skewness = stats.skew(factor_values)\n",
    "        factor_kurtosis = stats.kurtosis(factor_values)\n",
    "        shapiro_test = stats.shapiro(factor_values)\n",
    "        ks_normal_test = stats.kstest(factor_values, 'norm', args=(mean_val, std_val))\n",
    "        \n",
    "        print(f\"分佈統計資訊：\")\n",
    "        print(f\"  偏度: {factor_skewness:.4f} ({'正偏' if factor_skewness > 0 else '負偏'})\")\n",
    "        print(f\"  峰度: {factor_kurtosis:.4f} ({'尖峰' if factor_kurtosis > 0 else '平峰'})\")\n",
    "        print(f\"  Shapiro-Wilk正態性檢驗 p值: {shapiro_test.pvalue:.6f} ({'可能是正態分佈' if shapiro_test.pvalue > 0.05 else '非正態分佈'})\")\n",
    "        print(f\"  KS正態性檢驗 p值: {ks_normal_test.pvalue:.6f} ({'可能是正態分佈' if ks_normal_test.pvalue > 0.05 else '非正態分佈'})\")\n",
    "    except Exception as e:\n",
    "        print(f\"計算高級統計量時發生錯誤: {e}\")\n",
    "    \n",
    "    # 返回基本統計信息\n",
    "    return {\n",
    "        \"mean\": mean_val,\n",
    "        \"median\": median_val,\n",
    "        \"std\": std_val,\n",
    "        \"min\": np.min(factor_values),\n",
    "        \"max\": np.max(factor_values),\n",
    "        \"count\": len(factor_values),\n",
    "        \"quartiles\": [np.percentile(factor_values, p) for p in [25, 50, 75]]\n",
    "    }\n",
    "\n",
    "def visualize_factor_time_series(data):\n",
    "    \"\"\"\n",
    "    進階時間序列分析視圖\n",
    "    \n",
    "    Args:\n",
    "        data: 由visualize_factors函數返回的數據字典\n",
    "    \"\"\"\n",
    "    if data is None:\n",
    "        print(\"沒有數據可用於時間序列分析\")\n",
    "        return\n",
    "    \n",
    "    candle_df = data['candle']\n",
    "    factor_df = data['factor']\n",
    "    factor_name = data['factor_name']\n",
    "    \n",
    "    # 確保時間列格式正確\n",
    "    factor_df['time'] = pd.to_datetime(factor_df['time'])\n",
    "    candle_df['time'] = pd.to_datetime(candle_df['time'])\n",
    "    \n",
    "    # 合併數據\n",
    "    merged_df = pd.merge_asof(\n",
    "        candle_df.sort_values('time'), \n",
    "        factor_df[['time', factor_name]].sort_values('time'),\n",
    "        on='time',\n",
    "        direction='nearest'\n",
    "    )\n",
    "    \n",
    "    # 設置時間索引\n",
    "    merged_df.set_index('time', inplace=True)\n",
    "    \n",
    "    # 如果數據點足夠，計算移動平均\n",
    "    has_ma = len(merged_df) >= 30\n",
    "    if has_ma:\n",
    "        merged_df[f'{factor_name}_MA30'] = merged_df[factor_name].rolling(window=30).mean()\n",
    "        merged_df[f'{factor_name}_MA60'] = merged_df[factor_name].rolling(window=60).mean()\n",
    "    \n",
    "    # 創建2x2網格圖表\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "    plt.subplots_adjust(hspace=0.3, wspace=0.3)\n",
    "    \n",
    "    # 1. 主要時間序列圖（左上）\n",
    "    axes[0, 0].plot(merged_df.index, merged_df[factor_name], color='#1f77b4', linewidth=1.5, label=factor_name)\n",
    "    \n",
    "    if has_ma:\n",
    "        axes[0, 0].plot(merged_df.index, merged_df[f'{factor_name}_MA30'], \n",
    "                       color='#ff7f0e', linewidth=1.5, label='30-Period MA')\n",
    "        axes[0, 0].plot(merged_df.index, merged_df[f'{factor_name}_MA60'], \n",
    "                       color='#2ca02c', linewidth=1.5, label='60-Period MA')\n",
    "    \n",
    "    axes[0, 0].set_title(f'{factor_name} Time Series', fontsize=14)\n",
    "    axes[0, 0].set_ylabel('Value', fontsize=12)\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    axes[0, 0].legend(loc='best')\n",
    "    axes[0, 0].xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n",
    "    axes[0, 0].xaxis.set_major_locator(mdates.MonthLocator(interval=3))\n",
    "    plt.setp(axes[0, 0].xaxis.get_majorticklabels(), rotation=45)\n",
    "    \n",
    "    # 2. 因子變化率（右上）\n",
    "    merged_df[f'{factor_name}_chg'] = merged_df[factor_name].pct_change()\n",
    "    axes[0, 1].plot(merged_df.index, merged_df[f'{factor_name}_chg'], \n",
    "                   color='#d62728', linewidth=1.5)\n",
    "    axes[0, 1].set_title(f'{factor_name} Percent Change', fontsize=14)\n",
    "    axes[0, 1].set_ylabel('% Change', fontsize=12)\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    axes[0, 1].xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n",
    "    axes[0, 1].xaxis.set_major_locator(mdates.MonthLocator(interval=3))\n",
    "    plt.setp(axes[0, 1].xaxis.get_majorticklabels(), rotation=45)\n",
    "    \n",
    "    # 3. 累積分佈隨時間變化（左下）\n",
    "    try:\n",
    "        # 創建一個colormap，用於區分時間\n",
    "        import matplotlib.cm as cm\n",
    "        import matplotlib.colors as mcolors\n",
    "        from matplotlib.collections import LineCollection\n",
    "        \n",
    "        # 將時間序列分為若干段\n",
    "        n_segments = 4\n",
    "        segment_size = len(merged_df) // n_segments\n",
    "        \n",
    "        # 依時間順序為每個段落設置不同顏色\n",
    "        colors = plt.cm.viridis(np.linspace(0, 1, n_segments))\n",
    "        \n",
    "        # 繪製分段的ECDF\n",
    "        for i in range(n_segments):\n",
    "            start_idx = i * segment_size\n",
    "            end_idx = start_idx + segment_size if i < n_segments - 1 else len(merged_df)\n",
    "            \n",
    "            segment_data = merged_df[factor_name].iloc[start_idx:end_idx].dropna().values\n",
    "            if len(segment_data) > 0:\n",
    "                sorted_data = np.sort(segment_data)\n",
    "                ecdf = np.arange(1, len(sorted_data) + 1) / len(sorted_data)\n",
    "                \n",
    "                # 獲取該段的時間標籤\n",
    "                time_label = merged_df.index[start_idx].strftime('%Y-%m-%d')\n",
    "                \n",
    "                axes[1, 0].step(sorted_data, ecdf, linewidth=2, color=colors[i], \n",
    "                              label=f'Period {i+1}: from {time_label}')\n",
    "        \n",
    "        axes[1, 0].set_title('ECDF Evolution Over Time', fontsize=14)\n",
    "        axes[1, 0].set_xlabel(factor_name, fontsize=12)\n",
    "        axes[1, 0].set_ylabel('Cumulative Probability', fontsize=12)\n",
    "        axes[1, 0].grid(True, alpha=0.3)\n",
    "        axes[1, 0].legend(loc='best')\n",
    "    except Exception as e:\n",
    "        print(f\"無法創建累積分佈變化圖: {e}\")\n",
    "        # 備用圖：使用最簡單的滾動統計\n",
    "        if has_ma:\n",
    "            axes[1, 0].plot(merged_df.index, merged_df[f'{factor_name}'].rolling(window=30).std(), \n",
    "                         color='#9467bd', linewidth=1.5, label='30-Period Std')\n",
    "            axes[1, 0].set_title(f'{factor_name} Rolling Volatility', fontsize=14)\n",
    "            axes[1, 0].set_ylabel('Standard Deviation', fontsize=12)\n",
    "            axes[1, 0].grid(True, alpha=0.3)\n",
    "            axes[1, 0].legend(loc='best')\n",
    "            axes[1, 0].xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n",
    "            axes[1, 0].xaxis.set_major_locator(mdates.MonthLocator(interval=3))\n",
    "            plt.setp(axes[1, 0].xaxis.get_majorticklabels(), rotation=45)\n",
    "        else:\n",
    "            axes[1, 0].text(0.5, 0.5, 'Insufficient data for rolling metrics', \n",
    "                          transform=axes[1, 0].transAxes, ha='center', va='center',\n",
    "                          fontsize=12)\n",
    "    \n",
    "    # 4. 價格與因子疊加圖（右下）\n",
    "    ax1 = axes[1, 1]\n",
    "    ax2 = ax1.twinx()\n",
    "    \n",
    "    # 繪製價格線\n",
    "    price_line, = ax1.plot(merged_df.index, merged_df['Close'], color='#1f77b4', linewidth=1.5, label='Price')\n",
    "    ax1.set_ylabel('Price', fontsize=12, color='#1f77b4')\n",
    "    ax1.tick_params(axis='y', colors='#1f77b4')\n",
    "    \n",
    "    # 繪製因子線\n",
    "    factor_line, = ax2.plot(merged_df.index, merged_df[factor_name], color='#d62728', linewidth=1.5, label=factor_name)\n",
    "    ax2.set_ylabel(factor_name, fontsize=12, color='#d62728')\n",
    "    ax2.tick_params(axis='y', colors='#d62728')\n",
    "    \n",
    "    # 組合兩個圖例\n",
    "    lines = [price_line, factor_line]\n",
    "    ax1.legend(lines, [line.get_label() for line in lines], loc='upper left')\n",
    "    \n",
    "    ax1.set_title('Price vs Factor Overlay', fontsize=14)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n",
    "    ax1.xaxis.set_major_locator(mdates.MonthLocator(interval=3))\n",
    "    plt.setp(ax1.xaxis.get_majorticklabels(), rotation=45)\n",
    "    \n",
    "    # 添加超級標題\n",
    "    plt.suptitle(f'Advanced Time Series Analysis: {factor_name}', \n",
    "                fontsize=16, y=0.98, fontweight='bold')\n",
    "    \n",
    "    # 保存圖像\n",
    "    if hasattr(c, 'save_plot') and c.save_plot:\n",
    "        filename = f\"time_series_{c.symbol}_{factor_name}_{datetime.now().strftime('%Y%m%d')}.png\"\n",
    "        plt.savefig(filename, dpi=300, bbox_inches='tight')\n",
    "        print(f\"圖像已保存為: {filename}\")\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    return merged_df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(f\"正在執行 {c.symbol} 的因子分析，使用因子: {c.factor}\" + \n",
    "          (f\" 和 {c.factor2}\" if c.factor2 and c.operation != 'none' else \"\"))\n",
    "    \n",
    "    # 執行因子視覺化\n",
    "    data = visualize_factors()\n",
    "    \n",
    "    # 可視化因子與價格的相關性\n",
    "    correlation_results = visualize_factor_correlation(data)\n",
    "    \n",
    "    # 可視化因子分佈\n",
    "    stats = visualize_factor_distribution(data)\n",
    "    \n",
    "    # 額外執行時間序列分析\n",
    "    try:\n",
    "        print(\"\\n執行時間序列分析...\")\n",
    "        merged_data = visualize_factor_time_series(data)\n",
    "        print(\"分析完成！\")\n",
    "    except Exception as e:\n",
    "        print(f\"時間序列分析時發生錯誤: {e}\")\n",
    "        \n",
    "    print(\"\\n分析總結:\")\n",
    "    print(f\"- 因子: {data['factor_name']}\")\n",
    "    if 'correlation_results' in locals() and correlation_results and 'correlations' in correlation_results:\n",
    "        print(f\"- 相關性: {', '.join(correlation_results['correlations'])}\")\n",
    "    if 'stats' in locals() and stats:\n",
    "        print(f\"- 平均值: {stats['mean']:.4f}, 中位數: {stats['median']:.4f}, 標準差: {stats['std']:.4f}\")\n",
    "        print(f\"- 範圍: {stats['min']:.4f} 到 {stats['max']:.4f}, 樣本數: {stats['count']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Split Loop + Heatmap (Inclding looping Preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilsnumpy import nan_count, load_all_data, combines_data, data_processing, precompute_rolling_stats, backtest_cached\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "import config as c\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import sys\n",
    "from dask import delayed, compute\n",
    "from dask.diagnostics import ProgressBar\n",
    "\n",
    "def parse_manual_selection(filepath, all_models):\n",
    "    \"\"\"\n",
    "    解析手動選擇的模型與進出場方式。\n",
    "    \"\"\"\n",
    "    with open(filepath, \"r\") as file:\n",
    "        lines = [line.strip() for line in file.readlines() if line.strip()]\n",
    "    models_entrys = {}\n",
    "    current_model = None\n",
    "    for line in lines:\n",
    "        if line in all_models:\n",
    "            current_model = line\n",
    "            models_entrys[current_model] = []\n",
    "        elif current_model:\n",
    "            models_entrys[current_model].append(line)\n",
    "    return models_entrys\n",
    "\n",
    "def plot_heatmaps(sr_threshold=1.5, preprocess_method=None, highlight_x=0.5, highlight_y=50):\n",
    "    for model, entry, backtest_df in plot_data:\n",
    "        # ✅ Dynamically set SR threshold based on entry name\n",
    "        sr_threshold = 1.2 if entry.startswith('S') else sr_threshold  # Use 1.2 for entries starting with 'S', otherwise 1.8\n",
    "        sr_threshold = sr_threshold if entry.startswith('L') else sr_threshold # Changing threshold backto 1.5 if startwith 'L'\n",
    "        # ✅ Optimized pivot using groupby instead of pivot\n",
    "        sr_pivot_data = backtest_df.groupby(['rolling_window', 'threshold'])['SR'].mean().unstack()\n",
    "\n",
    "        # ✅ Check if the entire heatmap is NaN\n",
    "        if sr_pivot_data.isna().all().all():\n",
    "            print(f\"⚠️ Skipping {model}_{entry} heatmap: All SR values are NaN.\")\n",
    "            continue  # Skip plotting\n",
    "\n",
    "        # ✅ Check if there is at least one SR > threshold\n",
    "        if not np.any(sr_pivot_data.to_numpy() > sr_threshold):\n",
    "            print(f\"⚠️ Skipping {model}_{entry} heatmap: No SR value exceeds {sr_threshold}.\")\n",
    "            continue  # Skip plotting\n",
    "\n",
    "        plt.figure(figsize=(20, 16))  # ✅ Reduced figure size for faster rendering\n",
    "        \n",
    "        sr_pivot_data.columns = sr_pivot_data.columns.round(2)\n",
    "\n",
    "        ax = sns.heatmap(sr_pivot_data, annot=True, fmt=\".2f\", cmap=\"RdYlGn\", linewidths=0.3, cbar_kws={'label': 'Sharpe Ratio'})           \n",
    "        plt.xticks(np.arange(len(sr_pivot_data.columns)) + 0.5, [f\"{col:.2f}\" for col in sr_pivot_data.columns], rotation=45)\n",
    "        plt.yticks(np.arange(len(sr_pivot_data.index)) + 0.5, [f\"{row:.2f}\" for row in sr_pivot_data.index], rotation=0)\n",
    "        # plt.grid(visible=True, linestyle='--', linewidth=0.5)  # 顯示格線\n",
    "        \n",
    "        # 如果你想要高亮某個 (highlight_x, highlight_y) 對應的 pivot cell：\n",
    "        if highlight_x is not None and highlight_y is not None:\n",
    "            # 找出 x, y 的實際索引位置\n",
    "            # highlight_x 對應 columns (threshold)， highlight_y 對應 index (rolling_window)\n",
    "            try:\n",
    "                col_idx = sr_pivot_data.columns.get_loc(highlight_x)\n",
    "                row_idx = sr_pivot_data.index.get_loc(highlight_y)\n",
    "                # 設定 3x3 區域，並讓 highlight cell 為中央\n",
    "                top_left_col = col_idx - 1 if col_idx > 0 else col_idx\n",
    "                top_left_row = row_idx - 1 if row_idx > 0 else row_idx\n",
    "                # 根據資料表大小，計算寬與高（避免超出邊界）\n",
    "                max_cols = len(sr_pivot_data.columns)\n",
    "                max_rows = len(sr_pivot_data.index)\n",
    "                width = 3 if top_left_col + 3 <= max_cols else max_cols - top_left_col\n",
    "                height = 3 if top_left_row + 3 <= max_rows else max_rows - top_left_row\n",
    "                # 畫出 3x3 的框框\n",
    "                ax.add_patch(Rectangle(\n",
    "                    (top_left_col, top_left_row),  # 起始位置\n",
    "                    width, height,\n",
    "                    fill=False,          # 只框線，不填滿\n",
    "                    edgecolor='red',\n",
    "                    linewidth=2\n",
    "                ))\n",
    "                print(f\"✨ Highlighted 3x3 block centered at x={highlight_x}, y={highlight_y}\")\n",
    "            except KeyError:\n",
    "                print(f\"⚠️ Cannot highlight ({highlight_x}, {highlight_y}): value not found in pivot.\")\n",
    "        # ============== 關鍵加亮部分結束 ==============\n",
    "        \n",
    "        plt.title(f\"{model}_{preprocess_method}_{entry} Train Period BackTest SR Heatmap\", fontsize=14)\n",
    "        # ✅ Save the heatmap in the current working directory\n",
    "        if c.save_plot:\n",
    "            save_path = f\"{model}_{entry}_heatmap.png\"\n",
    "            plt.savefig(save_path, dpi=300, bbox_inches='tight', pad_inches=0.1)\n",
    "            print(f\"📁 Heatmap saved to: {save_path}\")\n",
    "\n",
    "        plt.show()  # ✅ Display the plot\n",
    "        plt.close()  # ✅ Free memory after each plot\n",
    "\n",
    "def process_and_validate(factor_series, method, factor_name):\n",
    "    \"\"\"\n",
    "    使用指定的 preprocess 方法處理資料並檢查 NaN 百分比。\n",
    "    \n",
    "    Parameters:\n",
    "        factor_series (pd.Series): 原始的因子資料。\n",
    "        method (str): 要套用的預處理方法。\n",
    "        factor_name (str): 因子名稱，用於 debug 訊息。\n",
    "        \n",
    "    Returns:\n",
    "        pd.Series: 若處理後 NaN 低於 3%，則回傳處理後的資料；否則回傳 None 表示跳過此方法。\n",
    "    \"\"\"\n",
    "    if method != \"direct\":\n",
    "        # Update to handle the return values from data_processing\n",
    "        factor_df, new_factor_name = data_processing(pd.DataFrame({factor_name: factor_series}), method, factor_name)\n",
    "        processed = factor_df[new_factor_name]\n",
    "    else:\n",
    "        processed = factor_series.copy()\n",
    "    if processed.isna().sum() / len(processed) > c.nan_perc:\n",
    "        preprocess_nan_count = nan_count(processed)\n",
    "        print(f\"nan count After {method} Preprocessing: {preprocess_nan_count}\")\n",
    "        print(f\"{factor_name} after {method} transformation exceed 3% NaN. Skipping this preprocess method.\")\n",
    "        return None\n",
    "    else:\n",
    "        print(f\"NaN% after preprocess: {processed.isna().sum()/len(processed)}%.\")\n",
    "        processed.dropna(inplace=True)\n",
    "    return processed\n",
    "\n",
    "def main(candle_data, factor_data, factor, interval, operation, model, entry,\n",
    "         window_start, window_end, window_step, threshold_start, threshold_end,\n",
    "         threshold_step, rolling_stats, preprocess_method, date_range):\n",
    "    \"\"\"\n",
    "    回測主程式，根據參數進行多參數回測。\n",
    "    \"\"\"\n",
    "    candle_df_copy = candle_data[['start_time', 'Close']].copy()\n",
    "    candle_df_copy.columns = ['start_time', 'close']\n",
    "    factor_df_copy = factor_data[['start_time', factor]].copy()\n",
    "\n",
    "    candle_df_copy['time'] = pd.to_datetime(candle_df_copy['start_time'], unit='ms')\n",
    "    \n",
    "    annualizer = annualizer_dict.get(interval, None)\n",
    "    backtest_report = []\n",
    "    for rolling_window in range(window_start, window_end, window_step):\n",
    "        for threshold in np.arange(threshold_start, threshold_end, threshold_step):\n",
    "            result, _, log_msgs = backtest_cached(candle_df_copy, factor_df_copy, rolling_window, threshold, \n",
    "                                          preprocess_method, entry, annualizer, model, factor, interval, date_range,\n",
    "                                          rolling_stats)\n",
    "            backtest_report.append(result)\n",
    "    \n",
    "    backtest_df = pd.DataFrame(backtest_report)\n",
    "    return (model, entry, backtest_df, log_msgs)\n",
    "\n",
    "# 定義 annualizer 字典\n",
    "annualizer_dict = {\n",
    "    '1m': 525600, '5m': 105120, '15m': 35040,\n",
    "    '30m': 17520, '1h': 8760, '4h': 2190,\n",
    "    '1d': 365, '1w': 52, '1M': 12\n",
    "}\n",
    "\n",
    "# 載入原始資料\n",
    "raw_candle, raw_factor = load_all_data(c.candle_file, c.factor_file, c.factor2_file, c.factor, c.factor2)\n",
    "train_split = annualizer_dict.get(c.interval, None) * 3\n",
    "\n",
    "candle_train = raw_candle[:train_split].reset_index(drop=True).copy()\n",
    "factor_train_original = raw_factor[:train_split].reset_index(drop=True).copy()\n",
    "\n",
    "# Start and end Time and date_range\n",
    "start_time = max(candle_train['start_time'].min(), factor_train_original['start_time'].min())\n",
    "end_time = min(candle_train['start_time'].max(), factor_train_original['start_time'].max())\n",
    "date_range = pd.date_range(start=pd.to_datetime(start_time, unit='ms'),\n",
    "                           end=pd.to_datetime(end_time, unit='ms'),\n",
    "                           freq=c.interval)\n",
    "candle_train['time'] = pd.to_datetime(candle_train['start_time'], unit='ms')\n",
    "factor_train_original['time'] = pd.to_datetime(factor_train_original['start_time'], unit='ms')\n",
    "candle_train.set_index('time', inplace=True)\n",
    "factor_train_original.set_index('time', inplace=True)\n",
    "\n",
    "# 若 operation 不是 'none'，則合併兩個因子\n",
    "if c.operation != 'none':\n",
    "    # 直接傳入 DataFrame 進行處理\n",
    "    factor_train_original, merged_col_name = combines_data(\n",
    "        factor_train_original,\n",
    "        c.factor,\n",
    "        c.factor2,\n",
    "        c.operation\n",
    "    )\n",
    "    factor_used = merged_col_name\n",
    "else:\n",
    "    factor_used = c.factor\n",
    "\n",
    "################################################\n",
    "# Step 0: 判斷 c.preprocess 是單一字串，還是串列\n",
    "################################################\n",
    "if isinstance(c.preprocess, list):\n",
    "    all_preprocess_methods = c.preprocess\n",
    "else:\n",
    "    all_preprocess_methods = [c.preprocess]\n",
    "\n",
    "# 對每個 preprocess 方法進行迴圈\n",
    "for current_preprocess in all_preprocess_methods:\n",
    "    print(f\"\\n===== Processing with preprocess method: {current_preprocess} =====\")\n",
    "    # 從原始資料複製一份\n",
    "    factor_train = factor_train_original.copy()\n",
    "    # 對指定因子進行預處理\n",
    "    processed_factor = process_and_validate(factor_train[factor_used], current_preprocess, factor_used)\n",
    "    if processed_factor is None:\n",
    "        continue  # 如果驗證不通過，則跳到下一個 preprocess 方法\n",
    "    factor_train[factor_used] = processed_factor\n",
    "\n",
    "    # 模型與進出場設定\n",
    "    if c.USE_ALL_MODELS:\n",
    "        models = c.models\n",
    "        entry_map = {model: c.entrys for model in c.models}\n",
    "        # window_step = 20\n",
    "        # threshold_step = 0.2\n",
    "    else:\n",
    "        entry_map = parse_manual_selection(\"manual_selected.txt\", c.models)\n",
    "        models = list(entry_map.keys())\n",
    "        # window_step = 10\n",
    "        # threshold_step = 0.1\n",
    "\n",
    "    # 預先計算滾動統計值\n",
    "    windows = list(range(5, c.window_end, c.window_step))\n",
    "    rolling_stats_dict = precompute_rolling_stats(factor_train[factor_used], windows)\n",
    "\n",
    "    # 重置 plot_data 以儲存當前 preprocess 方法的回測結果\n",
    "    plot_data = []\n",
    "    tasks = []\n",
    "    for model in models:\n",
    "        for entry in entry_map[model]:\n",
    "            task = delayed(main)(\n",
    "                candle_train,\n",
    "                factor_train,\n",
    "                factor_used,\n",
    "                c.interval,\n",
    "                c.operation,\n",
    "                model,\n",
    "                entry,\n",
    "                window_start=5,\n",
    "                window_end=c.window_end,\n",
    "                window_step=c.window_step,\n",
    "                threshold_start=0,\n",
    "                threshold_end=c.threshold_end,\n",
    "                threshold_step=c.threshold_step,\n",
    "                rolling_stats=rolling_stats_dict,\n",
    "                preprocess_method=current_preprocess,\n",
    "                date_range=date_range\n",
    "            )\n",
    "            tasks.append(task)\n",
    "\n",
    "    with ProgressBar():\n",
    "        results = compute(*tasks, scheduler='processes')\n",
    "    \n",
    "    for res in results:\n",
    "        if res is not None:\n",
    "            m, e, backtest_df, log_msgs = res\n",
    "            for msg in log_msgs:\n",
    "                print(msg)\n",
    "            plot_data.append((m, e, backtest_df))\n",
    "\n",
    "    # 繪製當前 preprocess 方法的熱力圖\n",
    "    plot_heatmaps(1.65, preprocess_method=current_preprocess, highlight_x=c.highlight_threshold, highlight_y=c.highlight_window)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split forward, Split Backtest, full_length_backtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilsnumpy import split_data, load_all_data, combines_data, data_processing, backtest_cached, additional_metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import config as c\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import sys\n",
    "import os\n",
    "\n",
    "annualizer_dict = {\n",
    "    '1m': 525600, '5m': 105120, '15m': 35040,\n",
    "    '30m': 17520, '1h': 8760, '4h': 2190,\n",
    "    '1d': 365, '1w': 52, '1M': 12\n",
    "}\n",
    "\n",
    "def load_and_prepare_data():\n",
    "    \"\"\"加载和准备回测数据，包括分割、预处理，并进行正确的NaN检查\"\"\"\n",
    "    \n",
    "    print(\"Loading data...\")\n",
    "    # 加载原始数据\n",
    "    raw_candle, raw_factor = load_all_data(c.candle_file, c.factor_file, c.factor2_file, c.factor, c.factor2)\n",
    "\n",
    "    # 计算分割点\n",
    "    annualizer = annualizer_dict.get(c.interval, 365)\n",
    "    # train_split = annualizer * 3  # 训练使用3年数据\n",
    "    \n",
    "    print(f\"Using {c.interval} data, annualizer: {annualizer}\") #, train_split points: {train_split}\n",
    "\n",
    "    # 按时间戳排序\n",
    "    raw_candle = raw_candle.sort_values('start_time').reset_index(drop=True)\n",
    "    raw_factor = raw_factor.sort_values('start_time').reset_index(drop=True)\n",
    "\n",
    "    # 检查数据长度\n",
    "    if len(raw_candle) <= 300:\n",
    "        print(f\"Error: Candle data ({len(raw_candle)} points) is insufficient for the specified train_split ({train_split} points)\")\n",
    "        return None\n",
    "    \n",
    "    if len(raw_factor) <= 300:\n",
    "        print(f\"Error: Factor data ({len(raw_factor)} points) is insufficient for the specified train_split ({train_split} points)\")\n",
    "        return None\n",
    "\n",
    "    # 分割数据\n",
    "    split_result = split_data(raw_candle, raw_factor, years_for_training=3)\n",
    "    \n",
    "    # 分割dataframes\n",
    "    candle_train = split_result['train']['candle']\n",
    "    factor_train = split_result['train']['factor']\n",
    "    candle_test = split_result['test']['candle']\n",
    "    factor_test = split_result['test']['factor']\n",
    "    candle_full = split_result['full']['candle']\n",
    "    factor_full = split_result['full']['factor']\n",
    "\n",
    "    # 记录原始数据长度\n",
    "    factor_train_length = len(factor_train)\n",
    "    factor_test_length = len(factor_test)\n",
    "    factor_full_length = len(factor_full)\n",
    "    \n",
    "    # 为所有数据添加时间列\n",
    "    for df in [candle_train, factor_train, candle_test, factor_test, candle_full, factor_full]:\n",
    "        df['time'] = pd.to_datetime(df['start_time'], unit='ms')\n",
    "    \n",
    "    # 创建完整日期范围\n",
    "    train_date_range = pd.date_range(\n",
    "        start=pd.to_datetime(max(candle_train['start_time'].min(), factor_train['start_time'].min()), unit='ms'),\n",
    "        end=pd.to_datetime(min(candle_train['start_time'].max(), factor_train['start_time'].max()), unit='ms'),\n",
    "        freq=c.interval\n",
    "    )\n",
    "    test_date_range = pd.date_range(\n",
    "        start=pd.to_datetime(max(candle_test['start_time'].min(), factor_test['start_time'].min()), unit='ms'),\n",
    "        end=pd.to_datetime(min(candle_test['start_time'].max(), factor_test['start_time'].max()), unit='ms'),\n",
    "        freq=c.interval\n",
    "    )\n",
    "    full_date_range = pd.date_range(\n",
    "        start=pd.to_datetime(max(candle_full['start_time'].min(), factor_full['start_time'].min()), unit='ms'),\n",
    "        end=pd.to_datetime(min(candle_full['start_time'].max(), factor_full['start_time'].max()), unit='ms'),\n",
    "        freq=c.interval\n",
    "    )\n",
    "\n",
    "    # 初始使用的因子名称\n",
    "    factor_name = c.factor\n",
    "\n",
    "    # 1. 组合因子处理 (如果需要)\n",
    "    if c.operation != 'none':\n",
    "        print(f\"\\nApplying operation '{c.operation}' to factors...\")\n",
    "        \n",
    "        # --- 创建数据副本用于NaN检测 ---\n",
    "        factor_train_copy = factor_train.copy()\n",
    "        factor_test_copy = factor_test.copy()\n",
    "        factor_full_copy = factor_full.copy()\n",
    "        \n",
    "        # --- 训练集处理 ---\n",
    "        # 在计算NaN比例之前先检查原始数据的NaN比例\n",
    "        train_temp = factor_train_copy.set_index('time')\n",
    "        train_temp_reindexed = train_temp.reindex(train_date_range)\n",
    "        \n",
    "        # 检查原始数据的NaN百分比\n",
    "        nan_train_before = train_temp_reindexed[c.factor].isna().sum() / len(train_date_range)\n",
    "        nan_train_before2 = train_temp_reindexed[c.factor2].isna().sum() / len(train_date_range)\n",
    "        \n",
    "        if nan_train_before > c.nan_perc or nan_train_before2 > c.nan_perc:\n",
    "            print(f\"Error: Input factors already have too many NaNs before combination: {c.factor}={nan_train_before:.3f}, {c.factor2}={nan_train_before2:.3f}\")\n",
    "            return None\n",
    "        \n",
    "        # 合并因子 (注意：combines_data直接修改并返回整个DataFrame)\n",
    "        factor_train, merged_col_name = combines_data(factor_train, c.factor, c.factor2, c.operation)\n",
    "        factor_test, _ = combines_data(factor_test, c.factor, c.factor2, c.operation)\n",
    "        factor_full, _ = combines_data(factor_full, c.factor, c.factor2, c.operation)\n",
    "        \n",
    "        # 更新因子名称\n",
    "        factor_name = merged_col_name\n",
    "        \n",
    "        # 检查合并后的NaN情况 (由于combines_data已经删除了NaN，所以这里主要是记录有多少数据被删除)\n",
    "        print(f\"Data remaining after combination: Train: {len(factor_train)}/{factor_train_length} ({len(factor_train)/factor_train_length:.1%})\")\n",
    "        print(f\"Data remaining after combination: Test: {len(factor_test)}/{factor_test_length} ({len(factor_test)/factor_test_length:.1%})\")\n",
    "        print(f\"Data remaining after combination: Full: {len(factor_full)}/{factor_full_length} ({len(factor_full)/factor_full_length:.1%})\")\n",
    "        \n",
    "        # 如果剩余数据太少，终止回测    # 這個是在用已經drop完的factor_df來check, 所以是檢查 (1-nan%) * 原始df length\n",
    "        if len(factor_train) < (1 - c.nan_perc) * factor_train_length:\n",
    "            print(\"Train data After droped more than 3% data. Skipping backtest.\")\n",
    "            return None\n",
    "        if len(factor_test) < (1 - c.nan_perc) * factor_test_length:\n",
    "            print(\"Test data After droped more than 3% data. Skipping backtest\")\n",
    "            return None\n",
    "        if len(factor_full) < (1 - c.nan_perc) * factor_full_length:\n",
    "            print(\"Full data After droped more than 3% data. Skipping backtest\")\n",
    "            return None\n",
    "            \n",
    "    # 2. 因子预处理\n",
    "    if c.preprocess != \"direct\":\n",
    "        print(f\"\\nApplying preprocessing method '{c.preprocess}'...\")\n",
    "        \n",
    "        # 原始数据长度更新\n",
    "        factor_train_length = len(factor_train)\n",
    "        factor_test_length = len(factor_test)\n",
    "        factor_full_length = len(factor_full)\n",
    "\n",
    "        print(f\"length of each split: Train:{factor_train_length}, Test:{factor_test_length}, Full:{factor_full_length}\")\n",
    "        \n",
    "        # --- 训练集预处理 ---\n",
    "        factor_train, train_factor_name = data_processing(factor_train, c.preprocess, factor_name)\n",
    "        train_remaining = len(factor_train) / factor_train_length\n",
    "        \n",
    "        # --- 测试集预处理 ---\n",
    "        factor_test, test_factor_name = data_processing(factor_test, c.preprocess, factor_name)\n",
    "        test_remaining = len(factor_test) / factor_test_length if factor_test_length > 0 else 1.0\n",
    "        \n",
    "        # --- 全集预处理 ---\n",
    "        factor_full, full_factor_name = data_processing(factor_full, c.preprocess, factor_name)\n",
    "        full_remaining = len(factor_full) / factor_full_length\n",
    "        \n",
    "        # 检查数据保留比例（超过3%的NaN会被删除）\n",
    "        nan_train = 1 - train_remaining\n",
    "        nan_test = 1 - test_remaining\n",
    "        nan_full = 1 - full_remaining\n",
    "        \n",
    "        print(f\"After {c.preprocess} preprocessing data retention: Train: {train_remaining:.3f}, Test: {test_remaining:.3f}, Full: {full_remaining:.3f}\")\n",
    "        print(f\"NaN percentages: Train: {nan_train:.3f}, Test: {nan_test:.3f}, Full: {nan_full:.3f}\")\n",
    "        \n",
    "        if nan_train > c.nan_perc:\n",
    "            print(f\"Train NaN% = {nan_train}, >3% after {c.preprocess} preprocessing. Skipping backtest.\")\n",
    "            return None\n",
    "        \n",
    "        if nan_test > c.nan_perc:\n",
    "            print(f\"Test NaN% = {nan_test}, >3% after {c.preprocess} preprocessing. Skipping backtest.\")\n",
    "            return None\n",
    "        \n",
    "        if nan_full > c.nan_perc:\n",
    "            print(f\"Full NaN% {nan_full}, >3% after {c.preprocess} preprocessing. Skipping backtest.\")\n",
    "            return None\n",
    "\n",
    "        # 更新因子名称\n",
    "        factor_name = train_factor_name\n",
    "    \n",
    "    # 4. 设置最终的时间索引\n",
    "    candle_train.set_index('time', inplace=True)\n",
    "    factor_train.set_index('time', inplace=True)\n",
    "    candle_test.set_index('time', inplace=True)\n",
    "    factor_test.set_index('time', inplace=True)\n",
    "    candle_full.set_index('time', inplace=True)\n",
    "    factor_full.set_index('time', inplace=True)\n",
    "    \n",
    "    # 只保留需要的列\n",
    "    candle_train = candle_train[['start_time', 'Close']]\n",
    "    candle_test = candle_test[['start_time', 'Close']]\n",
    "    candle_full = candle_full[['start_time', 'Close']]\n",
    "    \n",
    "    factor_train = factor_train[['start_time', factor_name]]\n",
    "    factor_test = factor_test[['start_time', factor_name]]\n",
    "    factor_full = factor_full[['start_time', factor_name]]\n",
    "    \n",
    "    # 5. 重命名Close列为小写close(如果需要)\n",
    "    candle_train.columns = ['start_time', 'close']\n",
    "    candle_test.columns = ['start_time', 'close']\n",
    "    candle_full.columns = ['start_time', 'close']\n",
    "    \n",
    "    # 6. 打印处理后的数据大小\n",
    "    print(f\"Final data sizes after processing:\")\n",
    "    print(f\"Train: candle={len(candle_train)}, factor={len(factor_train)}, original={factor_train_length}\")\n",
    "    print(f\"Test: candle={len(candle_test)}, factor={len(factor_test)}, original={factor_test_length}\")\n",
    "    print(f\"Full: candle={len(candle_full)}, factor={len(factor_full)}, original={factor_full_length}\")\n",
    "    \n",
    "    # 返回最终结果\n",
    "    return {\n",
    "        'train': {'candle': candle_train, 'factor': factor_train, 'date_range': train_date_range},\n",
    "        'test': {'candle': candle_test, 'factor': factor_test, 'date_range': test_date_range},\n",
    "        'full': {'candle': candle_full, 'factor': factor_full, 'date_range': full_date_range},\n",
    "        'factor_name': factor_name\n",
    "    }\n",
    "\n",
    "def parse_manual_selection(filepath):\n",
    "    \"\"\"\n",
    "    解析manual_selected.txt文件，返回参数元组列表[(model, entry, window, threshold)]\n",
    "    使用config.py中的models列表来识别模型和条目\n",
    "    如果文件格式不完整或不存在，返回空列表\n",
    "    \"\"\"\n",
    "    if not os.path.exists(filepath):\n",
    "        print(f\"Warning: {filepath} not found\")\n",
    "        return []\n",
    "        \n",
    "    try:        \n",
    "        with open(filepath, \"r\") as file:\n",
    "            lines = [line.strip() for line in file.readlines() if line.strip()]\n",
    "        \n",
    "        params_list = []\n",
    "        current_model = None\n",
    "        \n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "                \n",
    "            # 检查当前行是否是模型名称\n",
    "            if line in c.models:\n",
    "                current_model = line\n",
    "                continue\n",
    "            \n",
    "            # 如果没有当前模型，则跳过\n",
    "            if current_model is None:\n",
    "                print(f\"Warning: Entry '{line}' found without a model specified\")\n",
    "                continue\n",
    "                \n",
    "            # 解析条目和参数\n",
    "            parts = line.split(maxsplit=1)\n",
    "            if len(parts) < 1:\n",
    "                continue\n",
    "                \n",
    "            entry = parts[0]\n",
    "            \n",
    "            if len(parts) > 1:\n",
    "                params_str = parts[1]\n",
    "                params_entries = params_str.split(\",\")\n",
    "                for param_entry in params_entries:\n",
    "                    param_entry = param_entry.strip()\n",
    "                    if param_entry:\n",
    "                        try:\n",
    "                            window_str, threshold_str = param_entry.split(\"/\")\n",
    "                            window = float(window_str) if \".\" in window_str else int(window_str)\n",
    "                            threshold = float(threshold_str)\n",
    "                            params_list.append((current_model, entry, window, threshold))\n",
    "                        except ValueError:\n",
    "                            print(f\"Warning: Could not parse parameter {param_entry}\")\n",
    "            else:\n",
    "                # 没有参数，使用默认值\n",
    "                params_list.append((current_model, entry, c.window, c.threshold))\n",
    "    \n",
    "        return params_list\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing manual_selected.txt: {e}\")\n",
    "        return []\n",
    "\n",
    "def backtest_for_pnl(candle_df, factor_df, factor, factor2, interval, operation, preprocess, model,\n",
    "                  entry, window, threshold, backtest_style, date_range=None):\n",
    "    \"\"\"执行回测并绘制结果图表\"\"\"\n",
    "    print(f\"\\n=== Running {backtest_style} ===\")\n",
    "    print(f\"Using model: {model}, entry: {entry}, window: {window}, threshold: {threshold}\")\n",
    "    \n",
    "    # 检查空数据框\n",
    "    if candle_df.empty or factor_df.empty:\n",
    "        print(f\"Error: Empty dataframe in {backtest_style}\")\n",
    "        return None\n",
    "        \n",
    "    # 检查数据长度是否足够\n",
    "    if len(candle_df) < window or len(factor_df) < window:\n",
    "        print(f\"Error: Insufficient data points (candle: {len(candle_df)}, factor: {len(factor_df)}) compared to window size ({window}) in {backtest_style}\")\n",
    "        return None\n",
    "    \n",
    "    candle_df['time'] = pd.to_datetime(candle_df['start_time'], unit='ms')\n",
    "\n",
    "    # 获取annualizer值\n",
    "    annualizer = annualizer_dict.get(interval, 365)\n",
    "    \n",
    "    # 添加额外的指标\n",
    "    additional_metric = additional_metrics(c.alpha_id, c.symbol, factor, factor2,\n",
    "                                          operation, c.shift_candle_minite, backtest_style)\n",
    "    \n",
    "    try:\n",
    "        # 运行回测 - now passing date_range\n",
    "        backtest_result, df, log_msgs = backtest_cached(\n",
    "            candle_df, factor_df, window, threshold, preprocess, \n",
    "            entry, annualizer, model, factor, interval, date_range)\n",
    "        \n",
    "        # 打印日志消息\n",
    "        for msg in log_msgs:\n",
    "            print(msg)\n",
    "            \n",
    "        # 检查df是否为空\n",
    "        if df is None or df.empty:\n",
    "            print(f\"Warning: No results returned from backtest_cached for {backtest_style}\")\n",
    "            return None\n",
    "        \n",
    "        # # 正确处理时间戳\n",
    "        if isinstance(df.index, pd.DatetimeIndex):\n",
    "            # 直接使用DatetimeIndex\n",
    "            start_date = df.index.min().strftime('%Y-%m-%d')\n",
    "            end_date = df.index.max().strftime('%Y-%m-%d')\n",
    "            start_time = df.index.min().strftime('%Y-%m-%d %H:%M:%S')\n",
    "            end_time = df.index.max().strftime('%Y-%m-%d %H:%M:%S')\n",
    "        elif 'start_time' in df.columns:\n",
    "            # 使用start_time列\n",
    "            start_date = pd.to_datetime(df['start_time'].min(), unit='ms').strftime('%Y-%m-%d')\n",
    "            end_date = pd.to_datetime(df['start_time'].max(), unit='ms').strftime('%Y-%m-%d')\n",
    "            start_time = pd.to_datetime(df['start_time'].min(), unit='ms').strftime('%Y-%m-%d %H:%M:%S')\n",
    "            end_time = pd.to_datetime(df['start_time'].max(), unit='ms').strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "        additional_metric.update({\"start_time\": start_time, \"end_time\": end_time})\n",
    "        combined_report = [{**additional_metric, **backtest_result}]\n",
    "\n",
    "        print(f\"{backtest_style} Report:\")\n",
    "        print(json.dumps(combined_report, indent=4))\n",
    "\n",
    "        # 绘制结果\n",
    "        fig, ax1 = plt.subplots(figsize=(15, 8))\n",
    "        ax1.plot(df.index, df['close'], label='Close Price', color='green', linewidth=2)\n",
    "        ax1.set_xlabel(\"Date\", fontsize=12)\n",
    "        ax1.set_ylabel(\"Close Price\", fontsize=12, color='green')\n",
    "        ax1.tick_params(axis='y', labelcolor='green')\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 在右侧y轴上绘制累积PnL\n",
    "        ax2 = ax1.twinx()\n",
    "        ax2.plot(df.index, df['cumu_pnl'], label='Cumulative PnL', color='blue', linewidth=2)\n",
    "        ax2.set_ylabel(\"Cumulative PnL\", fontsize=12, color='blue')\n",
    "        ax2.tick_params(axis='y', labelcolor='blue')\n",
    "\n",
    "        plt.title(f\"Close Price and Cumulative PnL Plot (Split {backtest_style})-({start_date} ~ {end_date})\", fontsize=16)\n",
    "        fig.tight_layout()\n",
    "        # plt.show()\n",
    "\n",
    "        if c.save_plot:\n",
    "            plt.savefig(f\"{backtest_style}_Equity_Curve_{start_date}_{end_date}.png\", dpi=300, bbox_inches='tight')\n",
    "            print(f\"已儲存 {backtest_style}_Equity_Curve_{start_date}_{end_date}.png\")\n",
    "            output_backtest_data = {f\"{backtest_style}\": combined_report}\n",
    "            with open(f\"{c.alpha_id}_{backtest_style}.json\", \"w\") as json_file:\n",
    "                json.dump(output_backtest_data, json_file, indent=4)\n",
    "            df.to_csv(f\"{c.alpha_id}_{backtest_style}_df.csv\", index=True)\n",
    "            print(f\"已儲存 {c.alpha_id}_{backtest_style}_df.csv\")\n",
    "        plt.show()\n",
    "        return combined_report\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error in backtest_for_pnl for {backtest_style}: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "def run_backtest(data, model, entry, window, threshold):\n",
    "    \"\"\"运行完整的回测流程（前向测试、回测、全时段回测）\"\"\"\n",
    "    factor_name = data['factor_name']\n",
    "    \n",
    "    # 设置SR阈值\n",
    "    if \"short\" in entry.lower():\n",
    "        required_sr = 1.0\n",
    "    elif \"long\" in entry.lower():\n",
    "        required_sr = 1.7\n",
    "    else:\n",
    "        required_sr = 1.7\n",
    "    \n",
    "    print(f\"\\n执行测试: model = {model}, entry = {entry}, window = {window}, threshold = {threshold}\")\n",
    "    \n",
    "    # 执行forward test\n",
    "    fwd_report = backtest_for_pnl(\n",
    "        data['test']['candle'],\n",
    "        data['test']['factor'],\n",
    "        factor_name,\n",
    "        c.factor2,\n",
    "        c.interval,\n",
    "        c.operation,\n",
    "        c.preprocess,\n",
    "        model,\n",
    "        entry,\n",
    "        window,\n",
    "        threshold,\n",
    "        \"forwardtest\",\n",
    "        data['test']['date_range'],\n",
    "    )\n",
    "    \n",
    "    # 检查forward test结果\n",
    "    if fwd_report is None:\n",
    "        print(\"Warning: forward test failed, skipping this parameter set.\")\n",
    "        return\n",
    "        \n",
    "    # 获取SR值\n",
    "    fwd_sr = fwd_report[0].get(\"SR\", 0)\n",
    "    \n",
    "    # 如果SR未达到要求则跳过后续测试\n",
    "    if fwd_sr < required_sr:\n",
    "        print(f\"Forward test SR = {fwd_sr} 未达到要求 (需 > {required_sr})，跳过此组参数的后续测试。\")\n",
    "        return\n",
    "        \n",
    "    # 如果符合SR要求则继续执行其他backtest\n",
    "    backtest_for_pnl(\n",
    "        data['train']['candle'],\n",
    "        data['train']['factor'],\n",
    "        factor_name,\n",
    "        c.factor2,\n",
    "        c.interval,\n",
    "        c.operation,\n",
    "        c.preprocess,\n",
    "        model,\n",
    "        entry,\n",
    "        window,\n",
    "        threshold,\n",
    "        \"backtest\",\n",
    "        data['train']['date_range'],\n",
    "    )\n",
    "    \n",
    "    backtest_for_pnl(\n",
    "        data['full']['candle'],\n",
    "        data['full']['factor'],\n",
    "        factor_name,\n",
    "        c.factor2,\n",
    "        c.interval,\n",
    "        c.operation,\n",
    "        c.preprocess,\n",
    "        model,\n",
    "        entry,\n",
    "        window,\n",
    "        threshold,\n",
    "        \"full_time_backtest\",\n",
    "        data['full']['date_range'],\n",
    "    )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 一次性加载和处理数据\n",
    "    data = load_and_prepare_data()\n",
    "    if data is None:\n",
    "        print(\"Error preparing data. Aborting backtest.\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    # 尝试从manual_selected.txt读取参数列表\n",
    "    params_list = parse_manual_selection(\"manual_selected.txt\")\n",
    "    \n",
    "    if params_list:\n",
    "        print(f\"Found {len(params_list)} parameter sets in manual_selected.txt\")\n",
    "        # 使用每组参数运行回测\n",
    "        for model, entry, window, threshold in params_list:\n",
    "            run_backtest(data, model, entry, window, threshold)\n",
    "    else:\n",
    "        # 如果文件不存在或参数不齐全，使用config中的参数\n",
    "        print(\"Using parameters from config.py\")\n",
    "        run_backtest(data, c.model, c.entry, c.window, c.threshold)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cybotrade",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
