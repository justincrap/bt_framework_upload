{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已加载配置：factor=long_liquidations, factor2=long_liquidations, operation=none, preprocess=diff\n"
     ]
    }
   ],
   "source": [
    "# 每次运行时重新加载config.py\n",
    "from importlib import reload\n",
    "import config as c\n",
    "reload(c)\n",
    "\n",
    "print(f\"已加载配置：factor={c.factor}, factor2={c.factor2}, operation={c.operation}, preprocess={c.preprocess}, USE_ALL_MODELS={c.USE_ALL_MODELS}\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resampling 1min data to wanted timeframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import config as c\n",
    "\n",
    "def prepare_price_data(\n",
    "    csv_path: str,  # 輸入 CSV 檔案完整路徑\n",
    "    datasource: str = 'bybit_btcusdt',\n",
    "    factor: str = 'price',\n",
    "    timeframe: str = '1D', \n",
    "    delay_minutes: int = 0\n",
    "):\n",
    "    \"\"\"\n",
    "    讀取 1m 資料，轉成指定的時間週期 (timeframe)，\n",
    "    可選擇延遲(正值)或提前(負值)時間索引，並自動存檔到當前工作目錄。\n",
    "\n",
    "    參數：\n",
    "      csv_path      : 【完整路徑】輸入的 1m 級別 CSV 檔案\n",
    "      datasource    : 資料來源名稱 (如 bybit_btcusdt)\n",
    "      factor        : 影響因子名稱 (如 price)\n",
    "      timeframe     : 轉換後的時間週期，如 '1H'、'1D' 等 (預設 '1D')\n",
    "      delay_minutes : 時間平移的分鐘數 (正值 = 延後；負值 = 提前)\n",
    "\n",
    "    回傳：\n",
    "      pandas DataFrame (resampled 後的結果)，\n",
    "      並將結果輸出為 CSV，命名格式：\n",
    "      {datasource}_{factor}_{timeframe}_{start_time}_{end_time}.csv\n",
    "    \"\"\"\n",
    "    # 1. 讀取 CSV，解析時間\n",
    "    df = pd.read_csv(\n",
    "        csv_path, \n",
    "        parse_dates=['Time']  # pandas 會自動解析時間格式\n",
    "    )\n",
    "\n",
    "    # 2. 將 'Time' 欄設為索引\n",
    "    df.set_index('Time', inplace=True)\n",
    "\n",
    "    # 3. 時間平移 (延遲 / 提前)\n",
    "    if delay_minutes != 0:\n",
    "        df.index = df.index + pd.Timedelta(minutes=delay_minutes)\n",
    "\n",
    "    # 4. 定義 resample 聚合方式\n",
    "    if c.exchange_name == 'bybit':\n",
    "        ohlc_dict = {\n",
    "            'Open': 'first',\n",
    "            'High': 'max',\n",
    "            'Low': 'min',\n",
    "            'Close': 'last',\n",
    "            'Volume': 'sum',\n",
    "            'Turnover': 'sum'\n",
    "        }\n",
    "    else:\n",
    "        ohlc_dict = {\n",
    "        'Open': 'first',\n",
    "        'High': 'max',\n",
    "        'Low': 'min',\n",
    "        'Close': 'last',\n",
    "        'Volume': 'sum',\n",
    "    }\n",
    "    \n",
    "    # 5. 進行 resample\n",
    "    df_resampled = df.resample(timeframe).agg(ohlc_dict).dropna(how='any')\n",
    "\n",
    "    # Use Time to create one more column named 'start_time' that is in unix timestamp\n",
    "    df_resampled['start_time'] = df_resampled.index.astype('int64') // 10**6\n",
    "    # df_resampled['start_time'] = df_resampled['start_time'].astype('float64')\n",
    "\n",
    "    # 6. 獲取開始與結束時間 (格式 YYYY-MM-DD)\n",
    "    if not df_resampled.empty:\n",
    "        start_time = df_resampled.index[0].strftime('%Y-%m-%d')\n",
    "        end_time = df_resampled.index[-1].strftime('%Y-%m-%d')\n",
    "\n",
    "        # 7. 構建輸出檔案名稱\n",
    "        output_filename = f\"./data/resample_{datasource}_{timeframe}_-{c.candle_delay}m.csv\"\n",
    "        output_path = os.path.join(os.getcwd(), output_filename)  # 當前工作目錄\n",
    "\n",
    "        # 8. 輸出 CSV\n",
    "        df_resampled.to_csv(output_path)\n",
    "        print(f\"✅ 檔案已儲存：{output_path}\")\n",
    "    else:\n",
    "        print(\"⚠️ Resampled DataFrame 為空，未產生輸出檔案！\")\n",
    "\n",
    "    return df_resampled\n",
    "\n",
    "df_r = prepare_price_data(\n",
    "    csv_path=f\"./data/{c.exchange_name}_{c.coin}usdt_price_1m.csv\",\n",
    "    datasource=f'{c.exchange_name}_{c.coin}',\n",
    "    factor='price',\n",
    "    timeframe=c.candle_timeframe,\n",
    "    delay_minutes=-c.candle_delay\n",
    ")\n",
    "\n",
    "print(df_r.head()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './data/resample_bybit_btc_1h.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 215\u001b[0m\n\u001b[0;32m    211\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;66;03m# 执行因子可视化\u001b[39;00m\n\u001b[1;32m--> 215\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mvisualize_factors\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# 可视化因子与价格的相关性\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     visualize_factor_correlation(data)\n",
      "Cell \u001b[1;32mIn[2], line 14\u001b[0m, in \u001b[0;36mvisualize_factors\u001b[1;34m()\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading data...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# 加载原始数据\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m raw_candle, raw_factor \u001b[38;5;241m=\u001b[39m \u001b[43mload_all_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcandle_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfactor_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfactor2_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfactor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfactor2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# 处理时间格式以便于绘图\u001b[39;00m\n\u001b[0;32m     17\u001b[0m raw_factor[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(raw_factor[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstart_time\u001b[39m\u001b[38;5;124m'\u001b[39m], unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mms\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mf:\\bq\\phase 2 - PythonBacktest+ExecuteWithCybotrade\\bt_framework_upload\\utilsnumpy.py:35\u001b[0m, in \u001b[0;36mload_all_data\u001b[1;34m(candle_file, factor_file, factor2_file, factor, factor2)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_all_data\u001b[39m(candle_file, factor_file, factor2_file, factor, factor2):\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;66;03m# Load data and factor\u001b[39;00m\n\u001b[1;32m---> 35\u001b[0m     candle_data \u001b[38;5;241m=\u001b[39m \u001b[43mload_single_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcandle_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mClose\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     36\u001b[0m     factor_data \u001b[38;5;241m=\u001b[39m load_single_data(factor_file, factor)\n\u001b[0;32m     37\u001b[0m     factor2_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mf:\\bq\\phase 2 - PythonBacktest+ExecuteWithCybotrade\\bt_framework_upload\\utilsnumpy.py:15\u001b[0m, in \u001b[0;36mload_single_data\u001b[1;34m(filename, factor)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_single_data\u001b[39m(filename, factor)\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39mpd\u001b[38;5;241m.\u001b[39mDataFrame:\n\u001b[1;32m---> 15\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;66;03m# get only start_time and factor\u001b[39;00m\n\u001b[0;32m     17\u001b[0m     df \u001b[38;5;241m=\u001b[39m df[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstart_time\u001b[39m\u001b[38;5;124m'\u001b[39m, factor]]\n",
      "File \u001b[1;32mf:\\anaconda3\\envs\\cybotrade\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mf:\\anaconda3\\envs\\cybotrade\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mf:\\anaconda3\\envs\\cybotrade\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mf:\\anaconda3\\envs\\cybotrade\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mf:\\anaconda3\\envs\\cybotrade\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/resample_bybit_btc_1h.csv'"
     ]
    }
   ],
   "source": [
    "from utilsnumpy import load_all_data, combines_data, data_processing\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import config as c\n",
    "\n",
    "def visualize_factors():\n",
    "    \"\"\"\n",
    "    加载并可视化因子数据，支持因子组合和预处理\n",
    "    \"\"\"\n",
    "    print(\"Loading data...\")\n",
    "    # 加载原始数据\n",
    "    raw_candle, raw_factor = load_all_data(c.candle_file, c.factor_file, c.factor2_file, c.factor, c.factor2)\n",
    "    \n",
    "    # 处理时间格式以便于绘图\n",
    "    raw_factor['time'] = pd.to_datetime(raw_factor['start_time'], unit='ms')\n",
    "    raw_candle['time'] = pd.to_datetime(raw_candle['start_time'], unit='ms')\n",
    "    \n",
    "    # 初始化绘图\n",
    "    fig, axes = plt.subplots(3, 1, figsize=(15, 18), sharex=True)\n",
    "    \n",
    "    # 绘制价格数据\n",
    "    axes[0].plot(raw_candle['time'], raw_candle['Close'], color='blue', linewidth=1.5)\n",
    "    axes[0].set_title(f'Price Data: {c.symbol}', fontsize=16)\n",
    "    axes[0].set_ylabel('Price', fontsize=14)\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 绘制原始因子数据\n",
    "    axes[1].plot(raw_factor['time'], raw_factor[c.factor], color='green', linewidth=1.5, label=c.factor)\n",
    "    if c.factor2 in raw_factor.columns:\n",
    "        axes[1].plot(raw_factor['time'], raw_factor[c.factor2], color='red', linewidth=1.5, label=c.factor2)\n",
    "    axes[1].set_title('Raw Factor Data', fontsize=16)\n",
    "    axes[1].set_ylabel('Factor Value', fontsize=14)\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    axes[1].legend()\n",
    "    \n",
    "    # 处理组合因子和预处理\n",
    "    factor_data = raw_factor.copy()\n",
    "    factor_to_display = c.factor\n",
    "    factor_label = c.factor\n",
    "    \n",
    "    # 如果需要组合因子\n",
    "    if c.operation != 'none' and c.factor2 in factor_data.columns:\n",
    "        print(f\"Combining factors with operation: {c.operation}\")\n",
    "        combined_values, combined_name = combines_data(\n",
    "            factor_data[c.factor].values, \n",
    "            factor_data[c.factor2].values, \n",
    "            c.operation, \n",
    "            c.factor, \n",
    "            c.factor2\n",
    "        )\n",
    "        factor_data[combined_name] = combined_values\n",
    "        factor_to_display = combined_name\n",
    "        factor_label = f\"{c.factor} {c.operation} {c.factor2}\"\n",
    "    \n",
    "    # 如果需要预处理\n",
    "    if c.preprocess != \"direct\":\n",
    "        print(f\"Applying preprocessing: {c.preprocess}\")\n",
    "        processed_values = data_processing(factor_data[factor_to_display], c.preprocess, factor_to_display)\n",
    "        factor_data[f\"{factor_to_display}_{c.preprocess}\"] = processed_values\n",
    "        factor_to_display = f\"{factor_to_display}_{c.preprocess}\"\n",
    "        factor_label = f\"{factor_label} ({c.preprocess})\"\n",
    "    \n",
    "    # 绘制处理后的因子数据\n",
    "    axes[2].plot(factor_data['time'], factor_data[factor_to_display], color='purple', linewidth=1.5)\n",
    "    axes[2].set_title('Processed Factor Data', fontsize=16)\n",
    "    axes[2].set_ylabel('Factor Value', fontsize=14)\n",
    "    axes[2].set_xlabel('Date', fontsize=14)\n",
    "    axes[2].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 在标题旁添加因子描述\n",
    "    axes[2].text(0.01, 0.95, f\"Factor: {factor_label}\", transform=axes[2].transAxes, \n",
    "                 fontsize=12, verticalalignment='top', bbox=dict(boxstyle='round', facecolor='white', alpha=0.5))\n",
    "    \n",
    "    # 格式化X轴日期\n",
    "    for ax in axes:\n",
    "        ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n",
    "        ax.xaxis.set_major_locator(mdates.MonthLocator(interval=3))\n",
    "        plt.setp(ax.xaxis.get_majorticklabels(), rotation=45)\n",
    "    \n",
    "    # 添加整体标题\n",
    "    plt.suptitle(f'Factor Analysis - {c.symbol}', fontsize=20, y=0.98)\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.97])\n",
    "    \n",
    "    # 保存图像\n",
    "    if hasattr(c, 'save_plot') and c.save_plot:\n",
    "        # plt.savefig(f\"factor_analysis_{c.symbol}_{c.factor}.png\", dpi=300, bbox_inches='tight')\n",
    "        # print(f\"图像已保存为: factor_analysis_{c.symbol}_{c.factor}.png\")\n",
    "        pass\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    # 返回处理后的数据，以便进一步分析\n",
    "    return {\n",
    "        'candle': raw_candle,\n",
    "        'factor': factor_data,\n",
    "        'factor_name': factor_to_display\n",
    "    }\n",
    "\n",
    "def visualize_factor_correlation(data):\n",
    "    \"\"\"\n",
    "    可视化因子与价格的相关性\n",
    "    \n",
    "    参数:\n",
    "    data: 由visualize_factors函数返回的数据字典\n",
    "    \"\"\"\n",
    "    if data is None:\n",
    "        print(\"没有数据可用于相关性分析\")\n",
    "        return\n",
    "    \n",
    "    candle_df = data['candle']\n",
    "    factor_df = data['factor']\n",
    "    factor_name = data['factor_name']\n",
    "    \n",
    "    # 合并价格和因子数据\n",
    "    merged_df = pd.merge_asof(\n",
    "        candle_df.sort_values('time'), \n",
    "        factor_df[['time', factor_name]].sort_values('time'),\n",
    "        on='time',\n",
    "        direction='nearest'\n",
    "    )\n",
    "    \n",
    "    # 计算价格变化\n",
    "    merged_df['price_change'] = merged_df['Close'].pct_change()\n",
    "    \n",
    "    # 去除NaN值\n",
    "    merged_df = merged_df.dropna()\n",
    "    \n",
    "    # 计算相关系数\n",
    "    correlation = merged_df['price_change'].corr(merged_df[factor_name])\n",
    "    \n",
    "    # 绘制散点图\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.scatter(merged_df[factor_name], merged_df['price_change'], alpha=0.5)\n",
    "    plt.axhline(y=0, color='r', linestyle='-', alpha=0.3)\n",
    "    plt.axvline(x=0, color='r', linestyle='-', alpha=0.3)\n",
    "    \n",
    "    # 添加趋势线\n",
    "    z = np.polyfit(merged_df[factor_name], merged_df['price_change'], 1)\n",
    "    p = np.poly1d(z)\n",
    "    plt.plot(merged_df[factor_name], p(merged_df[factor_name]), \"r--\", alpha=0.8)\n",
    "    \n",
    "    plt.title(f'Correlation between Price Change and {factor_name}: {correlation:.4f}', fontsize=16)\n",
    "    plt.xlabel(factor_name, fontsize=14)\n",
    "    plt.ylabel('Price Change (%)', fontsize=14)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 保存图像\n",
    "    if hasattr(c, 'save_plot') and c.save_plot:\n",
    "        # plt.savefig(f\"correlation_{c.symbol}_{factor_name}.png\", dpi=300, bbox_inches='tight')\n",
    "        # print(f\"图像已保存为: correlation_{c.symbol}_{factor_name}.png\")\n",
    "        pass\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "def visualize_factor_distribution(data):\n",
    "    \"\"\"\n",
    "    可视化因子值的分布\n",
    "    \n",
    "    参数:\n",
    "    data: 由visualize_factors函数返回的数据字典\n",
    "    \"\"\"\n",
    "    if data is None:\n",
    "        print(\"没有数据可用于分布分析\")\n",
    "        return\n",
    "    \n",
    "    factor_df = data['factor']\n",
    "    factor_name = data['factor_name']\n",
    "    \n",
    "    # 去除NaN值\n",
    "    factor_values = factor_df[factor_name].dropna().values\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # 绘制直方图\n",
    "    plt.hist(factor_values, bins=50, alpha=0.7, color='blue')\n",
    "    \n",
    "    # 添加基本统计信息\n",
    "    mean_val = np.mean(factor_values)\n",
    "    median_val = np.median(factor_values)\n",
    "    std_val = np.std(factor_values)\n",
    "    \n",
    "    plt.axvline(mean_val, color='r', linestyle='dashed', linewidth=1, label=f'Mean: {mean_val:.4f}')\n",
    "    plt.axvline(median_val, color='g', linestyle='dashed', linewidth=1, label=f'Median: {median_val:.4f}')\n",
    "    \n",
    "    plt.title(f'Distribution of {factor_name}', fontsize=16)\n",
    "    plt.xlabel('Value', fontsize=14)\n",
    "    plt.ylabel('Frequency', fontsize=14)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend()\n",
    "    \n",
    "    # 文本框显示统计信息\n",
    "    stats_text = f\"\"\"\n",
    "    Mean: {mean_val:.4f}\n",
    "    Median: {median_val:.4f}\n",
    "    Std Dev: {std_val:.4f}\n",
    "    Min: {np.min(factor_values):.4f}\n",
    "    Max: {np.max(factor_values):.4f}\n",
    "    \"\"\"\n",
    "    plt.text(0.01, 0.95, stats_text, transform=plt.gca().transAxes, \n",
    "             fontsize=12, verticalalignment='top', \n",
    "             bbox=dict(boxstyle='round', facecolor='white', alpha=0.5))\n",
    "    \n",
    "    # 保存图像\n",
    "    if hasattr(c, 'save_plot') and c.save_plot:\n",
    "        # plt.savefig(f\"distribution_{c.symbol}_{factor_name}.png\", dpi=300, bbox_inches='tight')\n",
    "        # print(f\"图像已保存为: distribution_{c.symbol}_{factor_name}.png\")\n",
    "        pass\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 执行因子可视化\n",
    "    data = visualize_factors()\n",
    "    \n",
    "    # 可视化因子与价格的相关性\n",
    "    visualize_factor_correlation(data)\n",
    "    \n",
    "    # 可视化因子分布\n",
    "    visualize_factor_distribution(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Split Loop + Heatmap (Inclding looping Preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilsnumpy import nan_count, load_all_data, combines_data, data_processing, precompute_rolling_stats, backtest_cached\n",
    "import matplotlib.pyplot as plt\n",
    "import config as c\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import sys\n",
    "from dask import delayed, compute\n",
    "from dask.diagnostics import ProgressBar\n",
    "\n",
    "def parse_manual_selection(filepath, all_models):\n",
    "    \"\"\"\n",
    "    解析手動選擇的模型與進出場方式。\n",
    "    \"\"\"\n",
    "    with open(filepath, \"r\") as file:\n",
    "        lines = [line.strip() for line in file.readlines() if line.strip()]\n",
    "    models_entrys = {}\n",
    "    current_model = None\n",
    "    for line in lines:\n",
    "        if line in all_models:\n",
    "            current_model = line\n",
    "            models_entrys[current_model] = []\n",
    "        elif current_model:\n",
    "            models_entrys[current_model].append(line)\n",
    "    return models_entrys\n",
    "\n",
    "def plot_heatmaps(sr_threshold=1.5, preprocess_method=\"direct\"):\n",
    "    \"\"\"\n",
    "    依據回測結果繪製 SR 熱力圖，\n",
    "    並在標題中標示當前使用的 preprocess_method。\n",
    "    \"\"\"\n",
    "    for model, entry, backtest_df in plot_data:\n",
    "        if 'short' in entry:\n",
    "            srthreshold = 1.2\n",
    "        elif 'long' in entry:\n",
    "            srthreshold = sr_threshold\n",
    "        else:\n",
    "            srthreshold = sr_threshold\n",
    "        sr_pivot_data = backtest_df.groupby(['rolling_window', 'threshold'])['SR'].mean().unstack()\n",
    "        sr_pivot_data.columns = sr_pivot_data.columns.round(2)\n",
    "        if sr_pivot_data.isna().all().all():\n",
    "            print(f\"⚠️ Skipping {model}_{entry} heatmap: All SR values are NaN.\")\n",
    "            continue\n",
    "        if not np.any(sr_pivot_data.to_numpy() > srthreshold):\n",
    "            print(f\"⚠️ Skipping {model}_{entry} heatmap: No SR value exceeds {srthreshold}.\")\n",
    "            continue\n",
    "        plt.figure(figsize=(18, 14))\n",
    "        sns.heatmap(sr_pivot_data, annot=True, fmt=\".2f\", cmap=\"RdYlGn\", linewidths=0.3,\n",
    "                    cbar_kws={'label': 'Sharpe Ratio'})\n",
    "        plt.xticks(ticks=range(len(sr_pivot_data.columns)),\n",
    "                   labels=[f\"{col:.2f}\" for col in sr_pivot_data.columns], rotation=45)\n",
    "        plt.yticks(ticks=range(len(sr_pivot_data.index)),\n",
    "                   labels=[f\"{row:.2f}\" for row in sr_pivot_data.index], rotation=0)\n",
    "        plt.title(f\"{model}_{preprocess_method}_{entry} Train Period BackTest SR Heatmap\", fontsize=14)\n",
    "        plt.show()\n",
    "        if c.save_plot:\n",
    "            plt.savefig(f\"{model}_{entry}_heatmap\", dpi=300, bbox_inches='tight')\n",
    "            print(f\"已儲存 {model}_{entry}_heatmap.png\")\n",
    "        plt.close()\n",
    "\n",
    "def process_and_validate(factor_series, method, factor_name):\n",
    "    \"\"\"\n",
    "    使用指定的 preprocess 方法處理資料並檢查 NaN 百分比。\n",
    "    \n",
    "    Parameters:\n",
    "        factor_series (pd.Series): 原始的因子資料。\n",
    "        method (str): 要套用的預處理方法。\n",
    "        factor_name (str): 因子名稱，用於 debug 訊息。\n",
    "        \n",
    "    Returns:\n",
    "        pd.Series: 若處理後 NaN 低於 3%，則回傳處理後的資料；否則回傳 None 表示跳過此方法。\n",
    "    \"\"\"\n",
    "    if method != \"direct\":\n",
    "        processed = data_processing(factor_series, method, factor_name)\n",
    "    else:\n",
    "        processed = factor_series.copy()\n",
    "    if processed.isna().sum() / len(processed) > 0.03:\n",
    "        preprocess_nan_count = nan_count(processed)\n",
    "        print(f\"nan count After {method} Preprocessing: {preprocess_nan_count}\")\n",
    "        print(f\"{factor_name} after {method} transformation exceed 3% NaN. Skipping this preprocess method.\")\n",
    "        return None\n",
    "    return processed\n",
    "\n",
    "def main(candle_data, factor_data, factor, interval, operation, model, entry,\n",
    "         window_start, window_end, window_step, threshold_start, threshold_end,\n",
    "         threshold_step, rolling_stats, preprocess_method):\n",
    "    \"\"\"\n",
    "    回測主程式，根據參數進行多參數回測。\n",
    "    \"\"\"\n",
    "    candle_df_copy = candle_data[['start_time', 'Close']].copy()\n",
    "    candle_df_copy.columns = ['start_time', 'close']\n",
    "    factor_df_copy = factor_data[['start_time', factor]].copy()\n",
    "\n",
    "    annualizer = annualizer_dict.get(interval, None)\n",
    "    backtest_report = []\n",
    "    for rolling_window in range(window_start, window_end, window_step):\n",
    "        for threshold in np.arange(threshold_start, threshold_end, threshold_step):\n",
    "            result, _, log_msgs = backtest_cached(candle_df_copy, factor_df_copy, rolling_window, threshold, \n",
    "                                          preprocess_method, entry, annualizer, model, factor, interval, \n",
    "                                          rolling_stats)\n",
    "            backtest_report.append(result)\n",
    "    \n",
    "    backtest_df = pd.DataFrame(backtest_report)\n",
    "    return (model, entry, backtest_df, log_msgs)\n",
    "\n",
    "# 定義 annualizer 字典\n",
    "annualizer_dict = {\n",
    "    '1m': 525600, '5m': 105120, '15m': 35040,\n",
    "    '30m': 17520, '1h': 8760, '4h': 2190,\n",
    "    '1d': 365, '1w': 52, '1M': 12\n",
    "}\n",
    "\n",
    "# 載入原始資料\n",
    "raw_candle, raw_factor = load_all_data(c.candle_file, c.factor_file, c.factor2_file, c.factor, c.factor2)\n",
    "train_split = annualizer_dict.get(c.interval, None) * 3\n",
    "\n",
    "candle_train = raw_candle[:train_split].reset_index(drop=True).copy()\n",
    "factor_train_original = raw_factor[:train_split].reset_index(drop=True).copy()\n",
    "\n",
    "# 若 operation 不是 'none'，則合併兩個因子\n",
    "if c.operation != 'none':\n",
    "    combined_data, merged_col_name = combines_data(factor_train_original[c.factor].values, \n",
    "                                                     factor_train_original[c.factor2].values, \n",
    "                                                     c.operation, c.factor, c.factor2)\n",
    "    factor_train_original[merged_col_name] = combined_data\n",
    "    factor_used = merged_col_name\n",
    "else:\n",
    "    factor_used = c.factor\n",
    "\n",
    "################################################\n",
    "# Step 0: 判斷 c.preprocess 是單一字串，還是串列\n",
    "################################################\n",
    "if isinstance(c.preprocess, list):\n",
    "    all_preprocess_methods = c.preprocess\n",
    "else:\n",
    "    all_preprocess_methods = [c.preprocess]\n",
    "\n",
    "# 對每個 preprocess 方法進行迴圈\n",
    "for current_preprocess in all_preprocess_methods:\n",
    "    print(f\"\\n===== Processing with preprocess method: {current_preprocess} =====\")\n",
    "    # 從原始資料複製一份\n",
    "    factor_train = factor_train_original.copy()\n",
    "    # 對指定因子進行預處理\n",
    "    processed_factor = process_and_validate(factor_train[factor_used], current_preprocess, factor_used)\n",
    "    if processed_factor is None:\n",
    "        continue  # 如果驗證不通過，則跳到下一個 preprocess 方法\n",
    "    factor_train[factor_used] = processed_factor\n",
    "\n",
    "    # 模型與進出場設定\n",
    "    if c.USE_ALL_MODELS:\n",
    "        models = c.ALL_MODELS\n",
    "        entry_map = {model: c.ALL_ENTRYS for model in c.ALL_MODELS}\n",
    "        window_step = 20\n",
    "        threshold_step = 0.2\n",
    "    else:\n",
    "        entry_map = parse_manual_selection(\"manual_selected.txt\", c.ALL_MODELS)\n",
    "        models = list(entry_map.keys())\n",
    "        window_step = 10\n",
    "        threshold_step = 0.1\n",
    "\n",
    "    # 預先計算滾動統計值\n",
    "    windows = list(range(5, 351, window_step))\n",
    "    rolling_stats_dict = precompute_rolling_stats(factor_train[factor_used], windows)\n",
    "\n",
    "    # 重置 plot_data 以儲存當前 preprocess 方法的回測結果\n",
    "    plot_data = []\n",
    "    tasks = []\n",
    "    for model in models:\n",
    "        for entry in entry_map[model]:\n",
    "            task = delayed(main)(\n",
    "                candle_train,\n",
    "                factor_train,\n",
    "                factor_used,\n",
    "                c.interval,\n",
    "                c.operation,\n",
    "                model,\n",
    "                entry,\n",
    "                window_start=5,\n",
    "                window_end=351,\n",
    "                window_step=window_step,\n",
    "                threshold_start=0,\n",
    "                threshold_end=4.01,\n",
    "                threshold_step=threshold_step,\n",
    "                rolling_stats=rolling_stats_dict,\n",
    "                preprocess_method=current_preprocess\n",
    "            )\n",
    "            tasks.append(task)\n",
    "\n",
    "    with ProgressBar():\n",
    "        results = compute(*tasks, scheduler='processes')\n",
    "    \n",
    "    for res in results:\n",
    "        if res is not None:\n",
    "            m, e, backtest_df, log_msgs = res\n",
    "            for msg in log_msgs:\n",
    "                print(msg)\n",
    "            plot_data.append((m, e, backtest_df))\n",
    "    \n",
    "    # 繪製當前 preprocess 方法的熱力圖\n",
    "    plot_heatmaps(1.65, preprocess_method=current_preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split forward, Split Backtest, full_length_backtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilsnumpy import nan_count, load_all_data, combines_data, data_processing, backtest_cached, additional_metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import config as c\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import sys\n",
    "import os\n",
    "\n",
    "annualizer_dict = {\n",
    "    '1m': 525600, '5m': 105120, '15m': 35040,\n",
    "    '30m': 17520, '1h': 8760, '4h': 2190,\n",
    "    '1d': 365, '1w': 52, '1M': 12\n",
    "}\n",
    "\n",
    "def load_and_prepare_data():\n",
    "    \"\"\"加载和准备回测数据，包括分割和预处理\"\"\"\n",
    "    print(\"Loading data...\")\n",
    "    raw_candle, raw_factor = load_all_data(c.candle_file, c.factor_file, c.factor2_file, c.factor, c.factor2)\n",
    "\n",
    "    # 计算分割点\n",
    "    annualizer = annualizer_dict.get(c.interval, 365)\n",
    "    train_split = annualizer * 3  # 训练使用3年数据\n",
    "    \n",
    "    print(f\"Using {c.interval} data, annualizer: {annualizer}, train_split points: {train_split}\")\n",
    "\n",
    "    # 按时间戳排序\n",
    "    raw_candle = raw_candle.sort_values('start_time').reset_index(drop=True)\n",
    "    raw_factor = raw_factor.sort_values('start_time').reset_index(drop=True)\n",
    "\n",
    "    # 检查数据长度\n",
    "    if len(raw_candle) <= train_split:\n",
    "        print(f\"Error: Candle data ({len(raw_candle)} points) is insufficient for the specified train_split ({train_split} points)\")\n",
    "        return None\n",
    "    \n",
    "    if len(raw_factor) <= train_split:\n",
    "        print(f\"Error: Factor data ({len(raw_factor)} points) is insufficient for the specified train_split ({train_split} points)\")\n",
    "        return None\n",
    "\n",
    "    # 分割数据\n",
    "    split_time_candle = raw_candle.iloc[train_split]['start_time']\n",
    "    print(f\"Split timestamp: {pd.to_datetime(split_time_candle, unit='ms')}\")\n",
    "    \n",
    "    candle_train = raw_candle.iloc[:train_split].copy()\n",
    "    factor_train = raw_factor.iloc[:train_split].copy()\n",
    "\n",
    "    candle_test = raw_candle.iloc[train_split:].copy()\n",
    "    factor_test = raw_factor.iloc[train_split:].copy()\n",
    "\n",
    "    candle_full = raw_candle.copy()\n",
    "    factor_full = raw_factor.copy()\n",
    "\n",
    "    # 应用因子操作和预处理\n",
    "    factor_name = c.factor\n",
    "    if c.operation != 'none':\n",
    "        print(f\"\\nApplying operation '{c.operation}' to factors...\")\n",
    "        \n",
    "        # 处理训练数据\n",
    "        combined_train, merged_col_name = combines_data(factor_train[c.factor].values, \n",
    "                                                     factor_train[c.factor2].values, \n",
    "                                                     c.operation, c.factor, c.factor2)\n",
    "        factor_train[merged_col_name] = combined_train\n",
    "        \n",
    "        # 处理测试数据\n",
    "        combined_test, _ = combines_data(factor_test[c.factor].values, \n",
    "                                       factor_test[c.factor2].values, \n",
    "                                       c.operation, c.factor, c.factor2)\n",
    "        factor_test[merged_col_name] = combined_test\n",
    "        \n",
    "        # 处理完整数据\n",
    "        combined_full, _ = combines_data(factor_full[c.factor].values, \n",
    "                                       factor_full[c.factor2].values, \n",
    "                                       c.operation, c.factor, c.factor2)\n",
    "        factor_full[merged_col_name] = combined_full\n",
    "        \n",
    "        factor_name = merged_col_name\n",
    "        print(f\"Created merged factor: {factor_name}\")\n",
    "\n",
    "    # 应用预处理\n",
    "    if c.preprocess != \"direct\":\n",
    "        print(f\"\\nApplying preprocessing method '{c.preprocess}'...\")\n",
    "        \n",
    "        # 处理训练数据\n",
    "        processed_train = data_processing(factor_train[factor_name], c.preprocess, factor_name)\n",
    "        factor_train[factor_name] = processed_train\n",
    "        \n",
    "        # 处理测试数据\n",
    "        processed_test = data_processing(factor_test[factor_name], c.preprocess, factor_name)\n",
    "        factor_test[factor_name] = processed_test\n",
    "        \n",
    "        # 处理完整数据\n",
    "        processed_full = data_processing(factor_full[factor_name], c.preprocess, factor_name)\n",
    "        factor_full[factor_name] = processed_full\n",
    "\n",
    "    # 检查NaN百分比\n",
    "    nan_train = factor_train[factor_name].isna().sum() / len(factor_train[factor_name]) if len(factor_train) > 0 else 0\n",
    "    nan_test = factor_test[factor_name].isna().sum() / len(factor_test[factor_name]) if len(factor_test) > 0 else 0\n",
    "    print(f\"Train data NaN %: {nan_train:.3f}, Test data NaN %: {nan_test:.3f}\")\n",
    "\n",
    "    if nan_train > 0.03:\n",
    "        print(f\"Warning: Train data {factor_name} after {c.preprocess} transformation exceeds 3% NaN.\")\n",
    "    if nan_test > 0.03:\n",
    "        print(f\"Warning: Test data {factor_name} after {c.preprocess} transformation exceeds 3% NaN.\")\n",
    "        \n",
    "    return {\n",
    "        'train': {'candle': candle_train, 'factor': factor_train},\n",
    "        'test': {'candle': candle_test, 'factor': factor_test},\n",
    "        'full': {'candle': candle_full, 'factor': factor_full},\n",
    "        'factor_name': factor_name\n",
    "    }\n",
    "\n",
    "def parse_manual_selection(filepath):\n",
    "    \"\"\"\n",
    "    解析manual_selected.txt文件，返回参数元组列表[(model, entry, window, threshold)]\n",
    "    使用config.py中的ALL_MODELS列表来识别模型和条目\n",
    "    如果文件格式不完整或不存在，返回空列表\n",
    "    \"\"\"\n",
    "    if not os.path.exists(filepath):\n",
    "        print(f\"Warning: {filepath} not found\")\n",
    "        return []\n",
    "        \n",
    "    try:        \n",
    "        with open(filepath, \"r\") as file:\n",
    "            lines = [line.strip() for line in file.readlines() if line.strip()]\n",
    "        \n",
    "        params_list = []\n",
    "        current_model = None\n",
    "        \n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "                \n",
    "            # 检查当前行是否是模型名称\n",
    "            if line in c.ALL_MODELS:\n",
    "                current_model = line\n",
    "                continue\n",
    "            \n",
    "            # 如果没有当前模型，则跳过\n",
    "            if current_model is None:\n",
    "                print(f\"Warning: Entry '{line}' found without a model specified\")\n",
    "                continue\n",
    "                \n",
    "            # 解析条目和参数\n",
    "            parts = line.split(maxsplit=1)\n",
    "            if len(parts) < 1:\n",
    "                continue\n",
    "                \n",
    "            entry = parts[0]\n",
    "            \n",
    "            if len(parts) > 1:\n",
    "                params_str = parts[1]\n",
    "                params_entries = params_str.split(\",\")\n",
    "                for param_entry in params_entries:\n",
    "                    param_entry = param_entry.strip()\n",
    "                    if param_entry:\n",
    "                        try:\n",
    "                            window_str, threshold_str = param_entry.split(\"/\")\n",
    "                            window = float(window_str) if \".\" in window_str else int(window_str)\n",
    "                            threshold = float(threshold_str)\n",
    "                            params_list.append((current_model, entry, window, threshold))\n",
    "                        except ValueError:\n",
    "                            print(f\"Warning: Could not parse parameter {param_entry}\")\n",
    "            else:\n",
    "                # 没有参数，使用默认值\n",
    "                params_list.append((current_model, entry, c.window, c.threshold))\n",
    "    \n",
    "        return params_list\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing manual_selected.txt: {e}\")\n",
    "        return []\n",
    "\n",
    "def backtest_for_pnl(candle_df, factor_df, factor, factor2, interval, operation, preprocess, model,\n",
    "                  entry, window, threshold, backtest_style):\n",
    "    \"\"\"执行回测并绘制结果图表\"\"\"\n",
    "    print(f\"\\n=== Running {backtest_style} ===\")\n",
    "    print(f\"Using model: {model}, entry: {entry}, window: {window}, threshold: {threshold}\")\n",
    "    \n",
    "    # 日志信息\n",
    "    # print(f\"Candle data shape: {candle_df.shape}, Factor data shape: {factor_df.shape}\")\n",
    "    # print(f\"Candle data date range: {pd.to_datetime(candle_df['start_time'].min(), unit='ms')} to {pd.to_datetime(candle_df['start_time'].max(), unit='ms')}\")\n",
    "    # print(f\"Factor data date range: {pd.to_datetime(factor_df['start_time'].min(), unit='ms')} to {pd.to_datetime(factor_df['start_time'].max(), unit='ms')}\")\n",
    "\n",
    "    # 检查空数据框\n",
    "    if candle_df.empty or factor_df.empty:\n",
    "        print(f\"Error: Empty dataframe in {backtest_style}\")\n",
    "        return None\n",
    "        \n",
    "    # 检查数据长度是否足够\n",
    "    if len(candle_df) < window or len(factor_df) < window:\n",
    "        print(f\"Error: Insufficient data points (candle: {len(candle_df)}, factor: {len(factor_df)}) compared to window size ({window}) in {backtest_style}\")\n",
    "        return None\n",
    "        \n",
    "    # 创建副本避免修改原始数据\n",
    "    candle_df_copy = candle_df[['start_time', 'Close']].copy()\n",
    "    candle_df_copy.columns = ['start_time', 'close']\n",
    "    factor_df_copy = factor_df[['start_time', factor]].copy()\n",
    "    \n",
    "    # 检查重复项并排序\n",
    "    candle_df_copy = candle_df_copy.drop_duplicates(subset=['start_time'], keep='first')\n",
    "    factor_df_copy = factor_df_copy.drop_duplicates(subset=['start_time'], keep='first')\n",
    "    candle_df_copy = candle_df_copy.sort_values('start_time').reset_index(drop=True)\n",
    "    factor_df_copy = factor_df_copy.sort_values('start_time').reset_index(drop=True)\n",
    "    \n",
    "    # 筛选共同日期范围\n",
    "    common_start = max(candle_df_copy['start_time'].min(), factor_df_copy['start_time'].min())\n",
    "    common_end = min(candle_df_copy['start_time'].max(), factor_df_copy['start_time'].max())\n",
    "    \n",
    "    if common_start >= common_end:\n",
    "        print(f\"Error: No overlapping data between candle and factor datasets in {backtest_style}\")\n",
    "        return None\n",
    "    \n",
    "    candle_df_copy = candle_df_copy[(candle_df_copy['start_time'] >= common_start) & \n",
    "                                   (candle_df_copy['start_time'] <= common_end)]\n",
    "    factor_df_copy = factor_df_copy[(factor_df_copy['start_time'] >= common_start) & \n",
    "                                   (factor_df_copy['start_time'] <= common_end)]\n",
    "    \n",
    "    # 再次检查数据长度\n",
    "    if len(factor_df_copy) <= window:\n",
    "        print(f\"Error: After alignment, not enough data points ({len(factor_df_copy)}) compared to window size ({window}) in {backtest_style}\")\n",
    "        return None\n",
    "    \n",
    "    # 获取annualizer值\n",
    "    annualizer = annualizer_dict.get(interval, 365)\n",
    "    \n",
    "    # 添加额外的指标\n",
    "    additional_metric = additional_metrics(c.alpha_id, c.symbol, factor, factor2,\n",
    "                                           operation, c.candle_delay, backtest_style)\n",
    "    \n",
    "    try:\n",
    "        # 运行回测\n",
    "        backtest_result, df, log_msgs = backtest_cached(candle_df_copy, factor_df_copy, window, threshold, preprocess, \n",
    "                                                      entry, annualizer, model, factor, interval)\n",
    "        \n",
    "        # 打印日志消息\n",
    "        for msg in log_msgs:\n",
    "            print(msg)\n",
    "            \n",
    "        # 检查df是否为空\n",
    "        if df is None or df.empty:\n",
    "            print(f\"Warning: No results returned from backtest_cached for {backtest_style}\")\n",
    "            return None\n",
    "        \n",
    "        # 正确处理时间戳\n",
    "        if 'time' in df.columns:\n",
    "            start_date = pd.to_datetime(df['time'].min()).strftime('%Y-%m-%d')\n",
    "            end_date = pd.to_datetime(df['time'].max()).strftime('%Y-%m-%d')\n",
    "            start_time = pd.to_datetime(df['time'].min()).strftime('%Y-%m-%d %H:%M:%S')\n",
    "            end_time = pd.to_datetime(df['time'].max()).strftime('%Y-%m-%d %H:%M:%S')\n",
    "        else:\n",
    "            # 如果缺少time列，使用计算的共同范围\n",
    "            start_date = pd.to_datetime(common_start, unit='ms').strftime('%Y-%m-%d')\n",
    "            end_date = pd.to_datetime(common_end, unit='ms').strftime('%Y-%m-%d')\n",
    "            start_time = pd.to_datetime(common_start, unit='ms').strftime('%Y-%m-%d %H:%M:%S')\n",
    "            end_time = pd.to_datetime(common_end, unit='ms').strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "        additional_metric.update({\"start_time\": start_time, \"end_time\": end_time})\n",
    "        combined_report = [{**additional_metric, **backtest_result}]\n",
    "\n",
    "        print(f\"{backtest_style} Report:\")\n",
    "        print(json.dumps(combined_report, indent=4))\n",
    "\n",
    "        # 绘制结果\n",
    "        fig, ax1 = plt.subplots(figsize=(15, 8))\n",
    "        ax1.plot(df['time'], df['close'], label='Close Price', color='green', linewidth=2)\n",
    "        ax1.set_xlabel(\"Date\", fontsize=12)\n",
    "        ax1.set_ylabel(\"Close Price\", fontsize=12, color='green')\n",
    "        ax1.tick_params(axis='y', labelcolor='green')\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 在右侧y轴上绘制累积PnL\n",
    "        ax2 = ax1.twinx()\n",
    "        ax2.plot(df['time'], df['cumu_pnl'], label='Cumulative PnL', color='blue', linewidth=2)\n",
    "        ax2.set_ylabel(\"Cumulative PnL\", fontsize=12, color='blue')\n",
    "        ax2.tick_params(axis='y', labelcolor='blue')\n",
    "\n",
    "        plt.title(f\"Close Price and Cumulative PnL Plot (Split {backtest_style})-({start_date} ~ {end_date})\", fontsize=16)\n",
    "        fig.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        if c.save_plot:\n",
    "            plt.savefig(f\"{backtest_style}_Equity_Curve_{start_date}_{end_date}.png\", dpi=300, bbox_inches='tight')\n",
    "            print(f\"已儲存 {backtest_style}_Equity_Curve_{start_date}_{end_date}.png\")\n",
    "            output_backtest_data = {f\"{backtest_style}\": combined_report}\n",
    "            with open(f\"{c.alpha_id}_{backtest_style}.json\", \"w\") as json_file:\n",
    "                json.dump(output_backtest_data, json_file, indent=4)\n",
    "            df.to_csv(f\"{c.alpha_id}_{backtest_style}_df.csv\", index=False)\n",
    "            print(f\"已儲存 {c.alpha_id}_{backtest_style}_df.csv\")\n",
    "        \n",
    "        return combined_report\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error in backtest_for_pnl for {backtest_style}: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "def run_backtest(data, model, entry, window, threshold):\n",
    "    \"\"\"运行完整的回测流程（前向测试、回测、全时段回测）\"\"\"\n",
    "    factor_name = data['factor_name']\n",
    "    \n",
    "    # 设置SR阈值\n",
    "    if \"short\" in entry.lower():\n",
    "        required_sr = 1.0\n",
    "    elif \"long\" in entry.lower():\n",
    "        required_sr = 1.7\n",
    "    else:\n",
    "        required_sr = 1.7\n",
    "    \n",
    "    print(f\"\\n执行测试: model = {model}, entry = {entry}, window = {window}, threshold = {threshold}\")\n",
    "    \n",
    "    # 执行forward test\n",
    "    fwd_report = backtest_for_pnl(\n",
    "        data['test']['candle'],\n",
    "        data['test']['factor'],\n",
    "        factor_name,\n",
    "        c.factor2,\n",
    "        c.interval,\n",
    "        c.operation,\n",
    "        c.preprocess,\n",
    "        model,\n",
    "        entry,\n",
    "        window,\n",
    "        threshold,\n",
    "        \"forwardtest\"\n",
    "    )\n",
    "    \n",
    "    # 检查forward test结果\n",
    "    if fwd_report is None:\n",
    "        print(\"Warning: forward test failed, skipping this parameter set.\")\n",
    "        return\n",
    "        \n",
    "    # 获取SR值\n",
    "    fwd_sr = fwd_report[0].get(\"SR\", 0)\n",
    "    \n",
    "    # 如果SR未达到要求则跳过后续测试\n",
    "    if fwd_sr < required_sr:\n",
    "        print(f\"Forward test SR = {fwd_sr} 未达到要求 (需 > {required_sr})，跳过此组参数的后续测试。\")\n",
    "        return\n",
    "        \n",
    "    # 如果符合SR要求则继续执行其他backtest\n",
    "    backtest_for_pnl(\n",
    "        data['train']['candle'],\n",
    "        data['train']['factor'],\n",
    "        factor_name,\n",
    "        c.factor2,\n",
    "        c.interval,\n",
    "        c.operation,\n",
    "        c.preprocess,\n",
    "        model,\n",
    "        entry,\n",
    "        window,\n",
    "        threshold,\n",
    "        \"backtest\"\n",
    "    )\n",
    "    \n",
    "    backtest_for_pnl(\n",
    "        data['full']['candle'],\n",
    "        data['full']['factor'],\n",
    "        factor_name,\n",
    "        c.factor2,\n",
    "        c.interval,\n",
    "        c.operation,\n",
    "        c.preprocess,\n",
    "        model,\n",
    "        entry,\n",
    "        window,\n",
    "        threshold,\n",
    "        \"full_time_backtest\"\n",
    "    )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 一次性加载和处理数据\n",
    "    data = load_and_prepare_data()\n",
    "    if data is None:\n",
    "        print(\"Error preparing data. Aborting backtest.\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    # 尝试从manual_selected.txt读取参数列表\n",
    "    params_list = parse_manual_selection(\"manual_selected.txt\")\n",
    "    \n",
    "    if params_list:\n",
    "        print(f\"Found {len(params_list)} parameter sets in manual_selected.txt\")\n",
    "        # 使用每组参数运行回测\n",
    "        for model, entry, window, threshold in params_list:\n",
    "            run_backtest(data, model, entry, window, threshold)\n",
    "    else:\n",
    "        # 如果文件不存在或参数不齐全，使用config中的参数\n",
    "        print(\"Using parameters from config.py\")\n",
    "        run_backtest(data, c.model, c.entry, c.window, c.threshold)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cybotrade",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
